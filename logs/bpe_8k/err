2025-05-26 19:11:49,081 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2025-05-26 19:11:49,081 - INFO - joeynmt.helpers -                           cfg.name : bpe_8k
2025-05-26 19:11:49,081 - INFO - joeynmt.helpers -                cfg.joeynmt_version : 2.0.0
2025-05-26 19:11:49,081 - INFO - joeynmt.helpers -                     cfg.data.train : sampled_data/train.bpe8000
2025-05-26 19:11:49,081 - INFO - joeynmt.helpers -                       cfg.data.dev : sampled_data/dev.bpe8000
2025-05-26 19:11:49,081 - INFO - joeynmt.helpers -                      cfg.data.test : sampled_data/test.bpe8000
2025-05-26 19:11:49,081 - INFO - joeynmt.helpers -              cfg.data.dataset_type : plain
2025-05-26 19:11:49,081 - INFO - joeynmt.helpers -                  cfg.data.src.lang : en
2025-05-26 19:11:49,081 - INFO - joeynmt.helpers -                 cfg.data.src.level : bpe
2025-05-26 19:11:49,081 - INFO - joeynmt.helpers -              cfg.data.src.voc_file : sampled_data/vocab8000.joint
2025-05-26 19:11:49,081 - INFO - joeynmt.helpers -        cfg.data.src.tokenizer_type : subword-nmt
2025-05-26 19:11:49,081 - INFO - joeynmt.helpers -   cfg.data.src.tokenizer_cfg.codes : sampled_data/bpe8000.codes
2025-05-26 19:11:49,081 - INFO - joeynmt.helpers -                  cfg.data.trg.lang : it
2025-05-26 19:11:49,081 - INFO - joeynmt.helpers -                 cfg.data.trg.level : bpe
2025-05-26 19:11:49,081 - INFO - joeynmt.helpers -              cfg.data.trg.voc_file : sampled_data/vocab8000.joint
2025-05-26 19:11:49,081 - INFO - joeynmt.helpers -        cfg.data.trg.tokenizer_type : subword-nmt
2025-05-26 19:11:49,081 - INFO - joeynmt.helpers -   cfg.data.trg.tokenizer_cfg.codes : sampled_data/bpe8000.codes
2025-05-26 19:11:49,081 - INFO - joeynmt.helpers -              cfg.testing.beam_size : 5
2025-05-26 19:11:49,081 - INFO - joeynmt.helpers -             cfg.testing.beam_alpha : 1.0
2025-05-26 19:11:49,081 - INFO - joeynmt.helpers -           cfg.training.random_seed : 42
2025-05-26 19:11:49,081 - INFO - joeynmt.helpers -             cfg.training.optimizer : adam
2025-05-26 19:11:49,081 - INFO - joeynmt.helpers -         cfg.training.normalization : tokens
2025-05-26 19:11:49,081 - INFO - joeynmt.helpers -         cfg.training.learning_rate : 0.0003
2025-05-26 19:11:49,081 - INFO - joeynmt.helpers -            cfg.training.batch_size : 2048
2025-05-26 19:11:49,081 - INFO - joeynmt.helpers -            cfg.training.batch_type : token
2025-05-26 19:11:49,082 - INFO - joeynmt.helpers -       cfg.training.eval_batch_size : 1024
2025-05-26 19:11:49,082 - INFO - joeynmt.helpers -       cfg.training.eval_batch_type : token
2025-05-26 19:11:49,082 - INFO - joeynmt.helpers -            cfg.training.scheduling : none
2025-05-26 19:11:49,082 - INFO - joeynmt.helpers -              cfg.training.patience : 8
2025-05-26 19:11:49,082 - INFO - joeynmt.helpers -          cfg.training.weight_decay : 0.0
2025-05-26 19:11:49,082 - INFO - joeynmt.helpers -           cfg.training.eval_metric : ppl
2025-05-26 19:11:49,082 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl
2025-05-26 19:11:49,082 - INFO - joeynmt.helpers -                cfg.training.epochs : 10
2025-05-26 19:11:49,082 - INFO - joeynmt.helpers -       cfg.training.validation_freq : 500
2025-05-26 19:11:49,082 - INFO - joeynmt.helpers -          cfg.training.logging_freq : 100
2025-05-26 19:11:49,082 - INFO - joeynmt.helpers -          cfg.training.eval_metrics : ['ppl', 'loss', 'bleu']
2025-05-26 19:11:49,082 - INFO - joeynmt.helpers -             cfg.training.model_dir : models/bpe_8k
2025-05-26 19:11:49,082 - INFO - joeynmt.helpers -             cfg.training.overwrite : True
2025-05-26 19:11:49,082 - INFO - joeynmt.helpers -               cfg.training.shuffle : True
2025-05-26 19:11:49,082 - INFO - joeynmt.helpers -              cfg.training.use_cuda : False
2025-05-26 19:11:49,082 - INFO - joeynmt.helpers -     cfg.training.max_output_length : 100
2025-05-26 19:11:49,082 - INFO - joeynmt.helpers -     cfg.training.print_valid_sents : [0, 1, 2, 3, 4]
2025-05-26 19:11:49,082 - INFO - joeynmt.helpers -       cfg.training.label_smoothing : 0.3
2025-05-26 19:11:49,082 - INFO - joeynmt.helpers -              cfg.model.initializer : xavier_uniform
2025-05-26 19:11:49,082 - INFO - joeynmt.helpers -         cfg.model.bias_initializer : zeros
2025-05-26 19:11:49,082 - INFO - joeynmt.helpers -                cfg.model.init_gain : 1.0
2025-05-26 19:11:49,082 - INFO - joeynmt.helpers -        cfg.model.embed_initializer : xavier_uniform
2025-05-26 19:11:49,082 - INFO - joeynmt.helpers -          cfg.model.embed_init_gain : 1.0
2025-05-26 19:11:49,082 - INFO - joeynmt.helpers -          cfg.model.tied_embeddings : True
2025-05-26 19:11:49,082 - INFO - joeynmt.helpers -             cfg.model.tied_softmax : True
2025-05-26 19:11:49,082 - INFO - joeynmt.helpers -             cfg.model.encoder.type : transformer
2025-05-26 19:11:49,082 - INFO - joeynmt.helpers -       cfg.model.encoder.num_layers : 4
2025-05-26 19:11:49,082 - INFO - joeynmt.helpers -        cfg.model.encoder.num_heads : 2
2025-05-26 19:11:49,082 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256
2025-05-26 19:11:49,082 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True
2025-05-26 19:11:49,082 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.2
2025-05-26 19:11:49,082 - INFO - joeynmt.helpers -      cfg.model.encoder.hidden_size : 256
2025-05-26 19:11:49,082 - INFO - joeynmt.helpers -          cfg.model.encoder.ff_size : 512
2025-05-26 19:11:49,082 - INFO - joeynmt.helpers -          cfg.model.encoder.dropout : 0.1
2025-05-26 19:11:49,082 - INFO - joeynmt.helpers -             cfg.model.decoder.type : transformer
2025-05-26 19:11:49,082 - INFO - joeynmt.helpers -       cfg.model.decoder.num_layers : 1
2025-05-26 19:11:49,082 - INFO - joeynmt.helpers -        cfg.model.decoder.num_heads : 2
2025-05-26 19:11:49,082 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256
2025-05-26 19:11:49,082 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True
2025-05-26 19:11:49,082 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.2
2025-05-26 19:11:49,083 - INFO - joeynmt.helpers -      cfg.model.decoder.hidden_size : 256
2025-05-26 19:11:49,083 - INFO - joeynmt.helpers -          cfg.model.decoder.ff_size : 512
2025-05-26 19:11:49,083 - INFO - joeynmt.helpers -          cfg.model.decoder.dropout : 0.1
2025-05-26 19:11:49,086 - INFO - joeynmt.data - Building tokenizer...
2025-05-26 19:11:49,097 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-05-26 19:11:49,097 - INFO - joeynmt.tokenizers - it tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-05-26 19:11:49,097 - INFO - joeynmt.data - Loading train set...
2025-05-26 19:11:49,186 - INFO - joeynmt.data - Building vocabulary...
2025-05-26 19:11:49,630 - INFO - joeynmt.data - Loading dev set...
2025-05-26 19:11:49,631 - INFO - joeynmt.data - Loading test set...
2025-05-26 19:11:49,632 - INFO - joeynmt.data - Data loaded.
2025-05-26 19:11:49,632 - INFO - joeynmt.data - Train dataset: PlaintextDataset(split=train, len=100000, src_lang=en, trg_lang=it, has_trg=True, random_subset=-1)
2025-05-26 19:11:49,632 - INFO - joeynmt.data - Valid dataset: PlaintextDataset(split=dev, len=929, src_lang=en, trg_lang=it, has_trg=True, random_subset=-1)
2025-05-26 19:11:49,632 - INFO - joeynmt.data -  Test dataset: PlaintextDataset(split=test, len=1566, src_lang=en, trg_lang=it, has_trg=True, random_subset=-1)
2025-05-26 19:11:49,632 - INFO - joeynmt.data - First training example:
	[SRC] I don 't know .
	[TRG] Non lo so .
2025-05-26 19:11:49,632 - INFO - joeynmt.data - First 10 Src tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4)  (5)  (6)  (7)  (8) ! (9) «
2025-05-26 19:11:49,632 - INFO - joeynmt.data - First 10 Trg tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4)  (5)  (6)  (7)  (8) ! (9) «
2025-05-26 19:11:49,632 - INFO - joeynmt.data - Number of unique Src tokens (vocab_size): 7973
2025-05-26 19:11:49,632 - INFO - joeynmt.data - Number of unique Trg tokens (vocab_size): 7973
2025-05-26 19:11:49,634 - INFO - joeynmt.model - Building an encoder-decoder model...
2025-05-26 19:11:49,689 - INFO - joeynmt.model - Enc-dec model built.
2025-05-26 19:11:49,691 - INFO - joeynmt.model - Total params: 4940288
2025-05-26 19:11:49,691 - DEBUG - joeynmt.model - Trainable parameters: ['decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'src_embed.lut.weight']
2025-05-26 19:11:49,691 - INFO - joeynmt.training - Model(
	encoder=TransformerEncoder(num_layers=4, num_heads=2, alpha=1.0, layer_norm="pre", activation=ReLU()),
	decoder=TransformerDecoder(num_layers=1, num_heads=2, alpha=1.0, layer_norm="post", activation=ReLU()),
	src_embed=Embeddings(embedding_dim=256, vocab_size=7973),
	trg_embed=Embeddings(embedding_dim=256, vocab_size=7973),
	loss_function=XentLoss(criterion=KLDivLoss(), smoothing=0.3))
2025-05-26 19:11:50,330 - INFO - joeynmt.builders - Adam(lr=0.0003, weight_decay=0.0, betas=(0.9, 0.999))
2025-05-26 19:11:50,330 - INFO - joeynmt.builders - NoneType()
2025-05-26 19:11:50,330 - INFO - joeynmt.training - Train stats:
	device: cpu
	n_gpu: 0
	16-bits training: False
	gradient accumulation: 1
	batch size per device: 2048
	effective batch size (w. parallel & accumulation): 2048
2025-05-26 19:11:50,330 - INFO - joeynmt.training - EPOCH 1
2025-05-26 19:12:18,767 - INFO - joeynmt.training - Epoch   1, Step:      100, Batch Loss:     3.429843, Batch Acc: 0.185037, Tokens per Sec:     2494, Lr: 0.000300
2025-05-26 19:12:48,096 - INFO - joeynmt.training - Epoch   1, Step:      200, Batch Loss:     3.230262, Batch Acc: 0.302798, Tokens per Sec:     2504, Lr: 0.000300
2025-05-26 19:13:16,527 - INFO - joeynmt.training - Epoch   1, Step:      300, Batch Loss:     3.166579, Batch Acc: 0.308754, Tokens per Sec:     2597, Lr: 0.000300
2025-05-26 19:13:44,590 - INFO - joeynmt.training - Epoch   1, Step:      400, Batch Loss:     3.102111, Batch Acc: 0.325248, Tokens per Sec:     2668, Lr: 0.000300
2025-05-26 19:14:12,303 - INFO - joeynmt.training - Epoch   1, Step:      500, Batch Loss:     3.150774, Batch Acc: 0.341163, Tokens per Sec:     2675, Lr: 0.000300
2025-05-26 19:14:12,303 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-26 19:17:58,755 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.98, ppl:  19.76, acc:   0.36, generation: 226.4388[sec], evaluation: 0.0000[sec]
2025-05-26 19:17:58,758 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-26 19:17:58,896 - INFO - joeynmt.training - Example #0
2025-05-26 19:17:58,896 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-26 19:17:58,896 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-26 19:17:58,896 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '@', 'are', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un']
2025-05-26 19:17:58,896 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-26 19:17:58,897 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-26 19:17:58,897 - INFO - joeynmt.training - 	Hypothesis: <unk> @ are , , , , , , , , , , , , , , , , , , , , , , , un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un
2025-05-26 19:17:58,897 - INFO - joeynmt.training - Example #1
2025-05-26 19:17:58,897 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-26 19:17:58,897 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-26 19:17:58,897 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '@', 'are', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', '.', '</s>']
2025-05-26 19:17:58,897 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-26 19:17:58,897 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-26 19:17:58,897 - INFO - joeynmt.training - 	Hypothesis: <unk> @ are , , , , , , , , , , , , , , un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un .
2025-05-26 19:17:58,897 - INFO - joeynmt.training - Example #2
2025-05-26 19:17:58,897 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-26 19:17:58,897 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-26 19:17:58,897 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '@', 'are', ',', ',', ',', ',', ',', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', '.', '</s>']
2025-05-26 19:17:58,897 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-26 19:17:58,897 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-26 19:17:58,897 - INFO - joeynmt.training - 	Hypothesis: <unk> @ are , , , , , un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un .
2025-05-26 19:17:58,897 - INFO - joeynmt.training - Example #3
2025-05-26 19:17:58,897 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-26 19:17:58,897 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-26 19:17:58,897 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['.', '</s>']
2025-05-26 19:17:58,897 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-26 19:17:58,897 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-26 19:17:58,897 - INFO - joeynmt.training - 	Hypothesis: .
2025-05-26 19:17:58,897 - INFO - joeynmt.training - Example #4
2025-05-26 19:17:58,898 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-26 19:17:58,898 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-26 19:17:58,898 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', '@', 'are', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', '.', '</s>']
2025-05-26 19:17:58,898 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-26 19:17:58,898 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-26 19:17:58,898 - INFO - joeynmt.training - 	Hypothesis: <unk> @ are , , , , , , , , , , , , , , , , , , , , , , un un un un un un un un un un un un un un un un un un un .
2025-05-26 19:18:27,440 - INFO - joeynmt.training - Epoch   1, Step:      600, Batch Loss:     3.002288, Batch Acc: 0.349494, Tokens per Sec:     2535, Lr: 0.000300
2025-05-26 19:18:55,314 - INFO - joeynmt.training - Epoch   1, Step:      700, Batch Loss:     2.997701, Batch Acc: 0.362500, Tokens per Sec:     2727, Lr: 0.000300
2025-05-26 19:19:22,922 - INFO - joeynmt.training - Epoch   1, Step:      800, Batch Loss:     2.913601, Batch Acc: 0.369899, Tokens per Sec:     2578, Lr: 0.000300
2025-05-26 19:19:51,115 - INFO - joeynmt.training - Epoch   1, Step:      900, Batch Loss:     2.783917, Batch Acc: 0.377169, Tokens per Sec:     2612, Lr: 0.000300
2025-05-26 19:20:18,321 - INFO - joeynmt.training - Epoch   1, Step:     1000, Batch Loss:     2.929214, Batch Acc: 0.374281, Tokens per Sec:     2760, Lr: 0.000300
2025-05-26 19:20:18,321 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-26 19:23:56,616 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.76, ppl:  15.88, acc:   0.39, generation: 218.2859[sec], evaluation: 0.0000[sec]
2025-05-26 19:23:56,618 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-26 19:23:56,821 - INFO - joeynmt.training - Example #0
2025-05-26 19:23:56,822 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-26 19:23:56,822 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-26 19:23:56,822 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'che', 'un', 'mondo', ',', 'e', 'un', 'mondo', ',', 'e', 'la', 'mondo', ',', 'e', 'la', 'mondo', ',', 'e', 'la', 'mondo', ',', 'e', 'la', 'mondo', ',', 'e', 'la', 'mondo', ',', 'e', 'la', 'mondo', ',', 'e', 'la', 'mondo', ',', 'e', 'la', 'mondo', ',', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'la', 'mondo', ',', 'e', 'e', 'e', 'la', 'mondo', ',', 'e', 'il', 'mondo', ',', 'e', 'e', 'e', 'e', 'il', 'mondo', ',', 'e', 'il', 'mondo', ',', 'e', 'la', 'mondo', ',', 'e', 'il', 'mondo', ',', 'e', 'il', 'mondo', ',', 'e', 'il', 'mondo', ',', 'e', 'e', 'il', 'mondo', ',', 'e', 'un', 'mondo', ',', 'e', 'e', 'e', 'e', 'e', 'la', 'mondo', ',', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'il', 'mondo', ',', 'e', 'il', 'mondo', ',', 'e', 'e', 'e', 'e']
2025-05-26 19:23:56,822 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-26 19:23:56,822 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-26 19:23:56,822 - INFO - joeynmt.training - 	Hypothesis: E che un mondo , e un mondo , e la mondo , e la mondo , e la mondo , e la mondo , e la mondo , e la mondo , e la mondo , e la mondo , e e e e e e e la mondo , e e e la mondo , e il mondo , e e e e il mondo , e il mondo , e la mondo , e il mondo , e il mondo , e il mondo , e e il mondo , e un mondo , e e e e e la mondo , e e e e e e e il mondo , e il mondo , e e e e
2025-05-26 19:23:56,822 - INFO - joeynmt.training - Example #1
2025-05-26 19:23:56,822 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-26 19:23:56,822 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-26 19:23:56,822 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'un', 'mondo', '.', '</s>']
2025-05-26 19:23:56,822 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-26 19:23:56,822 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-26 19:23:56,822 - INFO - joeynmt.training - 	Hypothesis: E è è è è è è è è è è è è è è un mondo .
2025-05-26 19:23:56,822 - INFO - joeynmt.training - Example #2
2025-05-26 19:23:56,822 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-26 19:23:56,822 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-26 19:23:56,822 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', ',', 'non', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', ',', 'non', 'non', 'è', ',', ',', 'non', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', ',', ',', ',', ',', 'non']
2025-05-26 19:23:56,822 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-26 19:23:56,823 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-26 19:23:56,823 - INFO - joeynmt.training - 	Hypothesis: E è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è , non è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è , non non è , , non è è è è è è è è è è è è è è è è è è è è è è , , , , non
2025-05-26 19:23:56,823 - INFO - joeynmt.training - Example #3
2025-05-26 19:23:56,823 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-26 19:23:56,823 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-26 19:23:56,823 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'è', 'è', 'è', 'è', 'è', 'un', 'mondo', '.', '</s>']
2025-05-26 19:23:56,823 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-26 19:23:56,823 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-26 19:23:56,823 - INFO - joeynmt.training - 	Hypothesis: E è è è è è un mondo .
2025-05-26 19:23:56,823 - INFO - joeynmt.training - Example #4
2025-05-26 19:23:56,823 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-26 19:23:56,823 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-26 19:23:56,823 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'la', 'mondo', ',', 'la', 'mondo', '.', '</s>']
2025-05-26 19:23:56,823 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-26 19:23:56,823 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-26 19:23:56,823 - INFO - joeynmt.training - 	Hypothesis: E la mondo , la mondo .
2025-05-26 19:24:25,711 - INFO - joeynmt.training - Epoch   1, Step:     1100, Batch Loss:     2.800076, Batch Acc: 0.379214, Tokens per Sec:     2558, Lr: 0.000300
2025-05-26 19:40:20,505 - INFO - joeynmt.training - Epoch   1, Step:     1200, Batch Loss:     2.730513, Batch Acc: 0.377233, Tokens per Sec:       78, Lr: 0.000300
2025-05-26 19:40:48,249 - INFO - joeynmt.training - Epoch   1, Step:     1300, Batch Loss:     2.653145, Batch Acc: 0.380587, Tokens per Sec:     2691, Lr: 0.000300
2025-05-26 19:57:53,391 - INFO - joeynmt.training - Epoch   1, Step:     1400, Batch Loss:     2.863061, Batch Acc: 0.383408, Tokens per Sec:       71, Lr: 0.000300
2025-05-26 19:58:21,547 - INFO - joeynmt.training - Epoch   1, Step:     1500, Batch Loss:     2.728305, Batch Acc: 0.385022, Tokens per Sec:     2614, Lr: 0.000300
2025-05-26 19:58:21,547 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-26 20:05:40,871 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.70, ppl:  14.88, acc:   0.40, generation: 439.3169[sec], evaluation: 0.0000[sec]
2025-05-26 20:05:40,875 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-26 20:05:41,063 - INFO - joeynmt.training - Example #0
2025-05-26 20:05:41,063 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-26 20:05:41,063 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-26 20:05:41,063 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'è', 'è', 'è', 'è', 'la', 'mio', 'mondo', ',', 'e', 'la', 'mondo', ',', 'e', 'la', 'mondo', ',', 'e', 'la', 'mondo', ',', 'e', 'il', 'mondo', ',', 'e', 'la', 'mondo', ',', 'e', 'la', 'mondo', ',', 'e', 'la', 'mondo', ',', 'e', 'la', 'mondo', ',', 'e', 'la', 'mondo', '.', '</s>']
2025-05-26 20:05:41,063 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-26 20:05:41,063 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-26 20:05:41,063 - INFO - joeynmt.training - 	Hypothesis: E è è è è la mio mondo , e la mondo , e la mondo , e la mondo , e il mondo , e la mondo , e la mondo , e la mondo , e la mondo , e la mondo .
2025-05-26 20:05:41,063 - INFO - joeynmt.training - Example #1
2025-05-26 20:05:41,063 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-26 20:05:41,063 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-26 20:05:41,063 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'è', 'è', 'è', 'è', 'è', 'è', 'la', 'mondo', '.', '</s>']
2025-05-26 20:05:41,064 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-26 20:05:41,064 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-26 20:05:41,064 - INFO - joeynmt.training - 	Hypothesis: E è è è è è è la mondo .
2025-05-26 20:05:41,064 - INFO - joeynmt.training - Example #2
2025-05-26 20:05:41,064 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-26 20:05:41,064 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-26 20:05:41,064 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'è', 'è', 'è', 'è', 'la', 'mondo', ',', 'e', 'la', 'mondo', ',', 'e', 'la', 'mondo', ',', 'e', 'la', 'mondo', '.', '</s>']
2025-05-26 20:05:41,064 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-26 20:05:41,064 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-26 20:05:41,064 - INFO - joeynmt.training - 	Hypothesis: E è è è è la mondo , e la mondo , e la mondo , e la mondo .
2025-05-26 20:05:41,064 - INFO - joeynmt.training - Example #3
2025-05-26 20:05:41,064 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-26 20:05:41,064 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-26 20:05:41,064 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'è', 'è', 'è', 'è', 'è', 'è', 'il', 'mondo', '.', '</s>']
2025-05-26 20:05:41,064 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-26 20:05:41,064 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-26 20:05:41,064 - INFO - joeynmt.training - 	Hypothesis: E è è è è è è il mondo .
2025-05-26 20:05:41,064 - INFO - joeynmt.training - Example #4
2025-05-26 20:05:41,064 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-26 20:05:41,064 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-26 20:05:41,064 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'è', 'è', 'è', 'è', 'la', 'mondo', 'di', 'cui', 'la', 'mondo', '.', '</s>']
2025-05-26 20:05:41,064 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-26 20:05:41,064 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-26 20:05:41,065 - INFO - joeynmt.training - 	Hypothesis: E è è è è la mondo di cui la mondo .
2025-05-26 20:06:09,864 - INFO - joeynmt.training - Epoch   1, Step:     1600, Batch Loss:     2.705547, Batch Acc: 0.385358, Tokens per Sec:     2613, Lr: 0.000300
2025-05-26 20:06:38,800 - INFO - joeynmt.training - Epoch   1, Step:     1700, Batch Loss:     2.750883, Batch Acc: 0.387852, Tokens per Sec:     2583, Lr: 0.000300
2025-05-26 20:07:07,404 - INFO - joeynmt.training - Epoch   1, Step:     1800, Batch Loss:     2.659415, Batch Acc: 0.388625, Tokens per Sec:     2556, Lr: 0.000300
2025-05-26 20:07:36,066 - INFO - joeynmt.training - Epoch   1, Step:     1900, Batch Loss:     2.509442, Batch Acc: 0.388982, Tokens per Sec:     2567, Lr: 0.000300
2025-05-26 20:08:05,297 - INFO - joeynmt.training - Epoch   1, Step:     2000, Batch Loss:     2.737777, Batch Acc: 0.394220, Tokens per Sec:     2553, Lr: 0.000300
2025-05-26 20:08:05,298 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-26 20:11:49,274 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.65, ppl:  14.16, acc:   0.41, generation: 223.9672[sec], evaluation: 0.0000[sec]
2025-05-26 20:11:49,277 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-26 20:11:49,457 - INFO - joeynmt.training - Example #0
2025-05-26 20:11:49,457 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-26 20:11:49,457 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-26 20:11:49,457 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'mio', 'mio', 'mio', 'mio', 'mio', 'mio', 'mio', 'mio', 'vita', 'di', 'un', 'mio', 'vita', ',', 'e', 'il', 'mio', 'vita', ',', 'e', 'il', 'mio', 'vita', 'di', 'persone', ',', 'e', 'il', 'mio', 'vita', 'di', 'persone', 'che', 'il', 'mio', 'vita', '.', '</s>']
2025-05-26 20:11:49,457 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-26 20:11:49,457 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-26 20:11:49,457 - INFO - joeynmt.training - 	Hypothesis: Il mio mio mio mio mio mio mio mio vita di un mio vita , e il mio vita , e il mio vita di persone , e il mio vita di persone che il mio vita .
2025-05-26 20:11:49,457 - INFO - joeynmt.training - Example #1
2025-05-26 20:11:49,457 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-26 20:11:49,457 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-26 20:11:49,457 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'è', 'un', 'cosa', 'è', 'un', 'mio', 'cosa', 'che', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non']
2025-05-26 20:11:49,457 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-26 20:11:49,457 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-26 20:11:49,458 - INFO - joeynmt.training - 	Hypothesis: Ma è un cosa è un mio cosa che non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non
2025-05-26 20:11:49,458 - INFO - joeynmt.training - Example #2
2025-05-26 20:11:49,458 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-26 20:11:49,458 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-26 20:11:49,458 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'è', 'un', 'mio', 'mio', 'mio', 'mio', 'mio', 'mio', 'mio', 'mio', 'mio', 'mio', 'vita', ',', 'e', 'il', 'mio', 'vita', '.', '</s>']
2025-05-26 20:11:49,458 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-26 20:11:49,458 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-26 20:11:49,458 - INFO - joeynmt.training - 	Hypothesis: La è un mio mio mio mio mio mio mio mio mio mio vita , e il mio vita .
2025-05-26 20:11:49,458 - INFO - joeynmt.training - Example #3
2025-05-26 20:11:49,458 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-26 20:11:49,458 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-26 20:11:49,458 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'è', 'un', 'modo', 'di', 'un', 'vita', '.', '</s>']
2025-05-26 20:11:49,458 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-26 20:11:49,458 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-26 20:11:49,458 - INFO - joeynmt.training - 	Hypothesis: E è un modo di un vita .
2025-05-26 20:11:49,458 - INFO - joeynmt.training - Example #4
2025-05-26 20:11:49,458 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-26 20:11:49,458 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-26 20:11:49,458 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'mio', 'mio', 'mio', 'mio', 'mio', 'mio', 'mio', 'mio', 'vita', 'di', 'un', 'vita', 'di', 'persone', 'che', 'il', 'vita', '.', '</s>']
2025-05-26 20:11:49,458 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-26 20:11:49,458 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-26 20:11:49,458 - INFO - joeynmt.training - 	Hypothesis: La mio mio mio mio mio mio mio mio vita di un vita di persone che il vita .
2025-05-26 20:12:18,674 - INFO - joeynmt.training - Epoch   1, Step:     2100, Batch Loss:     2.730132, Batch Acc: 0.393102, Tokens per Sec:     2521, Lr: 0.000300
2025-05-26 20:12:47,744 - INFO - joeynmt.training - Epoch   1, Step:     2200, Batch Loss:     2.696193, Batch Acc: 0.394791, Tokens per Sec:     2482, Lr: 0.000300
2025-05-26 20:13:16,269 - INFO - joeynmt.training - Epoch   1, Step:     2300, Batch Loss:     2.759553, Batch Acc: 0.398144, Tokens per Sec:     2659, Lr: 0.000300
2025-05-26 20:13:45,265 - INFO - joeynmt.training - Epoch   1, Step:     2400, Batch Loss:     2.504512, Batch Acc: 0.403446, Tokens per Sec:     2548, Lr: 0.000300
2025-05-26 20:14:13,885 - INFO - joeynmt.training - Epoch   1, Step:     2500, Batch Loss:     2.572783, Batch Acc: 0.402821, Tokens per Sec:     2554, Lr: 0.000300
2025-05-26 20:14:13,885 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-26 20:18:04,764 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.60, ppl:  13.41, acc:   0.41, generation: 230.8677[sec], evaluation: 0.0000[sec]
2025-05-26 20:18:04,767 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-26 20:18:04,978 - INFO - joeynmt.training - Example #0
2025-05-26 20:18:04,978 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-26 20:18:04,978 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-26 20:18:04,978 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'mio', 'mio', 'mio', 'parte', 'di', 'persone', 'che', 'la', 'parte', 'di', 'persone', 'che', 'la', 'parte', 'di', 'persone', 'che', 'la', 'parte', 'di', 'persone', 'che', 'la', 'parte', 'di', 'persone', 'che', 'la', 'parte', 'di', 'persone', 'che', 'la', 'parte', 'di', 'persone', 'che', 'la', 'parte', 'di', 'persone', 'che', 'la', 'parte', 'di', 'persone', 'che', 'la', 'parte', 'di', 'cui', 'la', 'loro', 'parte', 'di', 'persone', 'che', 'la', 'loro', '.', '</s>']
2025-05-26 20:18:04,978 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-26 20:18:04,978 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-26 20:18:04,978 - INFO - joeynmt.training - 	Hypothesis: Il mio mio mio parte di persone che la parte di persone che la parte di persone che la parte di persone che la parte di persone che la parte di persone che la parte di persone che la parte di persone che la parte di persone che la parte di cui la loro parte di persone che la loro .
2025-05-26 20:18:04,978 - INFO - joeynmt.training - Example #1
2025-05-26 20:18:04,978 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-26 20:18:04,978 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-26 20:18:04,978 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non']
2025-05-26 20:18:04,978 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-26 20:18:04,978 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-26 20:18:04,978 - INFO - joeynmt.training - 	Hypothesis: Ma non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non
2025-05-26 20:18:04,978 - INFO - joeynmt.training - Example #2
2025-05-26 20:18:04,978 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-26 20:18:04,978 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-26 20:18:04,978 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'mondo', 'è', 'una', 'è', 'una', 'è', 'una', 'è', 'una', 'è', 'una', 'parte', 'di', 'una', 'parte', 'di', 'una', 'parte', 'di', 'un', 'mondo', '.', '</s>']
2025-05-26 20:18:04,979 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-26 20:18:04,979 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-26 20:18:04,979 - INFO - joeynmt.training - 	Hypothesis: Il mondo è una è una è una è una è una parte di una parte di una parte di un mondo .
2025-05-26 20:18:04,979 - INFO - joeynmt.training - Example #3
2025-05-26 20:18:04,979 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-26 20:18:04,979 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-26 20:18:04,979 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'la', 'parte', 'di', 'un', 'mondo', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'la', 'sua', 'parte', 'di', 'un', 'mondo', '.', '</s>']
2025-05-26 20:18:04,979 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-26 20:18:04,979 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-26 20:18:04,979 - INFO - joeynmt.training - 	Hypothesis: E la parte di un mondo e e e e e e e e e e e e la sua parte di un mondo .
2025-05-26 20:18:04,979 - INFO - joeynmt.training - Example #4
2025-05-26 20:18:04,979 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-26 20:18:04,979 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-26 20:18:04,979 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'mio', 'mio', 'mio', 'mio', 'mio', 'mio', 'mio', 'mio', 'mio', 'mio', 'mio', 'mio', 'mondo', 'che', 'è', 'che', 'è', 'che', 'è', 'che', 'è', 'stato', 'stato', 'stato', 'stato', 'stato', 'stato', 'stato', 'stato', 'stato', 'stato', 'stato', 'stato', 'stato', 'stato', 'stato', 'stato', '.', '</s>']
2025-05-26 20:18:04,979 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-26 20:18:04,979 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-26 20:18:04,979 - INFO - joeynmt.training - 	Hypothesis: Il mio mio mio mio mio mio mio mio mio mio mio mio mondo che è che è che è che è stato stato stato stato stato stato stato stato stato stato stato stato stato stato stato stato .
2025-05-26 20:18:34,765 - INFO - joeynmt.training - Epoch   1, Step:     2600, Batch Loss:     2.494792, Batch Acc: 0.401609, Tokens per Sec:     2416, Lr: 0.000300
2025-05-26 20:19:04,246 - INFO - joeynmt.training - Epoch   1, Step:     2700, Batch Loss:     2.765563, Batch Acc: 0.407949, Tokens per Sec:     2558, Lr: 0.000300
2025-05-26 20:19:32,899 - INFO - joeynmt.training - Epoch   1, Step:     2800, Batch Loss:     2.540157, Batch Acc: 0.408694, Tokens per Sec:     2576, Lr: 0.000300
2025-05-26 20:20:00,456 - INFO - joeynmt.training - Epoch   1, Step:     2900, Batch Loss:     2.603046, Batch Acc: 0.411243, Tokens per Sec:     2676, Lr: 0.000300
2025-05-26 20:20:30,439 - INFO - joeynmt.training - Epoch   1, Step:     3000, Batch Loss:     2.622579, Batch Acc: 0.410468, Tokens per Sec:     2455, Lr: 0.000300
2025-05-26 20:20:30,440 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-26 20:24:23,988 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.52, ppl:  12.49, acc:   0.42, generation: 233.5391[sec], evaluation: 0.0000[sec]
2025-05-26 20:24:23,991 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-26 20:24:24,172 - INFO - joeynmt.helpers - delete models/bpe_8k/500.ckpt
2025-05-26 20:24:24,189 - INFO - joeynmt.training - Example #0
2025-05-26 20:24:24,189 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-26 20:24:24,189 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-26 20:24:24,189 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['S@@', '<unk>', '@', 'ato', 'ho', 'detto', 'che', 'sono', 'stati', 'stati', 'i', 'miei', 'miei', 'miei', 'miei', 'anni', ',', 'la', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'vita', '.', '</s>']
2025-05-26 20:24:24,189 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-26 20:24:24,190 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-26 20:24:24,190 - INFO - joeynmt.training - 	Hypothesis: S<unk> @ ato ho detto che sono stati stati i miei miei miei miei anni , la mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia vita .
2025-05-26 20:24:24,190 - INFO - joeynmt.training - Example #1
2025-05-26 20:24:24,190 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-26 20:24:24,190 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-26 20:24:24,190 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'sono', 'la', 'nostra', 'nostra', 'nostra', 'nostra', 'nostra', 'nostra', 'nostra', 'cosa', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non']
2025-05-26 20:24:24,190 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-26 20:24:24,190 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-26 20:24:24,190 - INFO - joeynmt.training - 	Hypothesis: Ma non sono la nostra nostra nostra nostra nostra nostra nostra cosa non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non
2025-05-26 20:24:24,190 - INFO - joeynmt.training - Example #2
2025-05-26 20:24:24,190 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-26 20:24:24,190 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-26 20:24:24,190 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'mondo', 'è', 'la', 'sua', 'parte', 'di', 'ri@@', '<unk>', '@', 'ato', ',', 'la', 'sua', 'parte', 'di', 'una', 'parte', 'di', 'una', 'parte', 'di', 'una', 'parte', 'di', 'una', 'parte', 'di', 'una', 'parte', 'di', '.', '</s>']
2025-05-26 20:24:24,190 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-26 20:24:24,190 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-26 20:24:24,190 - INFO - joeynmt.training - 	Hypothesis: Il mondo è la sua parte di ri<unk> @ ato , la sua parte di una parte di una parte di una parte di una parte di una parte di .
2025-05-26 20:24:24,190 - INFO - joeynmt.training - Example #3
2025-05-26 20:24:24,190 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-26 20:24:24,190 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-26 20:24:24,190 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['È', 'un', 'po', "'", 'di', 'un', 'po', "'", 'di', 'un', 'po', "'", '.', '</s>']
2025-05-26 20:24:24,190 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-26 20:24:24,190 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-26 20:24:24,190 - INFO - joeynmt.training - 	Hypothesis: È un po ' di un po ' di un po ' .
2025-05-26 20:24:24,191 - INFO - joeynmt.training - Example #4
2025-05-26 20:24:24,191 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-26 20:24:24,191 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-26 20:24:24,191 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'mondo', 'che', 'ho', 'fatto', 'che', 'ho', 'avuto', 'un', 'po', "'", 'un', 'po', "'", 'di', 'anni', '.', '</s>']
2025-05-26 20:24:24,191 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-26 20:24:24,191 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-26 20:24:24,191 - INFO - joeynmt.training - 	Hypothesis: Il mondo che ho fatto che ho avuto un po ' un po ' di anni .
2025-05-26 20:24:52,944 - INFO - joeynmt.training - Epoch   1, Step:     3100, Batch Loss:     2.520663, Batch Acc: 0.413939, Tokens per Sec:     2521, Lr: 0.000300
2025-05-26 20:25:20,712 - INFO - joeynmt.training - Epoch   1, Step:     3200, Batch Loss:     2.522580, Batch Acc: 0.416873, Tokens per Sec:     2657, Lr: 0.000300
2025-05-26 20:25:50,310 - INFO - joeynmt.training - Epoch   1, Step:     3300, Batch Loss:     2.511249, Batch Acc: 0.417270, Tokens per Sec:     2451, Lr: 0.000300
2025-05-26 20:26:21,160 - INFO - joeynmt.training - Epoch   1, Step:     3400, Batch Loss:     2.446325, Batch Acc: 0.419066, Tokens per Sec:     2467, Lr: 0.000300
2025-05-26 20:26:49,823 - INFO - joeynmt.training - Epoch   1, Step:     3500, Batch Loss:     2.488400, Batch Acc: 0.422691, Tokens per Sec:     2518, Lr: 0.000300
2025-05-26 20:26:49,824 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-26 20:30:20,441 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.46, ppl:  11.71, acc:   0.43, generation: 210.6092[sec], evaluation: 0.0000[sec]
2025-05-26 20:30:20,444 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-26 20:30:20,590 - INFO - joeynmt.helpers - delete models/bpe_8k/1000.ckpt
2025-05-26 20:30:20,610 - INFO - joeynmt.training - Example #0
2025-05-26 20:30:20,610 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-26 20:30:20,610 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-26 20:30:20,610 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'mio', 'mio', 'mio', 'mio', 'mio', 'mio', 'mio', 'mio', 'mio', 'mio', 'mio', 'giorno', 'che', 'sono', 'che', 'sono', 'che', 'sono', 'che', 'sono', 'che', 'sono', 'che', 'sono', 'che', 'sono', 'in', 'un', 'paio', 'di', 'anni', ',', 'che', 'sono', 'stati', 'in', 'un', 'paio', 'di', 'anni', ',', 'che', 'sono', 'in', 'un', 'paio', 'di', 'anni', '.', '</s>']
2025-05-26 20:30:20,610 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-26 20:30:20,610 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-26 20:30:20,610 - INFO - joeynmt.training - 	Hypothesis: Il mio mio mio mio mio mio mio mio mio mio mio giorno che sono che sono che sono che sono che sono che sono che sono in un paio di anni , che sono stati in un paio di anni , che sono in un paio di anni .
2025-05-26 20:30:20,610 - INFO - joeynmt.training - Example #1
2025-05-26 20:30:20,610 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-26 20:30:20,610 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-26 20:30:20,610 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'è', 'il', 'mondo', 'è', 'il', 'mondo', 'del', 'mondo', 'del', 'mondo', '.', '</s>']
2025-05-26 20:30:20,610 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-26 20:30:20,610 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-26 20:30:20,611 - INFO - joeynmt.training - 	Hypothesis: Ma questo è il mondo è il mondo del mondo del mondo .
2025-05-26 20:30:20,611 - INFO - joeynmt.training - Example #2
2025-05-26 20:30:20,611 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-26 20:30:20,611 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-26 20:30:20,611 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'suo', 'sistema', 'è', 'il', 'suo', 'sistema', 'è', 'il', 'sistema', 'è', 'il', 'sistema', ',', 'è', 'una', 'è', 'il', 'mondo', ',', 'è', 'il', 'mondo', '.', '</s>']
2025-05-26 20:30:20,611 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-26 20:30:20,611 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-26 20:30:20,611 - INFO - joeynmt.training - 	Hypothesis: Il suo sistema è il suo sistema è il sistema è il sistema , è una è il mondo , è il mondo .
2025-05-26 20:30:20,611 - INFO - joeynmt.training - Example #3
2025-05-26 20:30:20,611 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-26 20:30:20,611 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-26 20:30:20,611 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', "'", 'un', 'po', "'", 'di', 'un', 'po', "'", 'in', 'un', 'po', "'", "'", "'", "'", 'in', 'un', 'po', "'", "'", "'", 'in', 'un', 'po', "'", '.', '</s>']
2025-05-26 20:30:20,611 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-26 20:30:20,611 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-26 20:30:20,611 - INFO - joeynmt.training - 	Hypothesis: E ' un po ' di un po ' in un po ' ' ' ' in un po ' ' ' in un po ' .
2025-05-26 20:30:20,611 - INFO - joeynmt.training - Example #4
2025-05-26 20:30:20,611 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-26 20:30:20,611 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-26 20:30:20,611 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'mio', 'mio', 'mio', 'mio', 'mio', 'mio', 'mio', 'giorno', 'che', 'mi', 'ha', 'fatto', 'un', 'po', "'", 'di', 'un', 'po', "'", 'di', 'anni', '.', '</s>']
2025-05-26 20:30:20,611 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-26 20:30:20,611 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-26 20:30:20,611 - INFO - joeynmt.training - 	Hypothesis: Il mio mio mio mio mio mio mio giorno che mi ha fatto un po ' di un po ' di anni .
2025-05-26 20:30:47,376 - INFO - joeynmt.training - Epoch   1, Step:     3600, Batch Loss:     2.654683, Batch Acc: 0.422160, Tokens per Sec:     2659, Lr: 0.000300
2025-05-26 20:31:15,269 - INFO - joeynmt.training - Epoch   1, Step:     3700, Batch Loss:     2.623317, Batch Acc: 0.425895, Tokens per Sec:     2691, Lr: 0.000300
2025-05-26 20:31:43,307 - INFO - joeynmt.training - Epoch   1, Step:     3800, Batch Loss:     2.402025, Batch Acc: 0.427510, Tokens per Sec:     2577, Lr: 0.000300
2025-05-26 20:32:10,659 - INFO - joeynmt.training - Epoch   1, Step:     3900, Batch Loss:     2.421711, Batch Acc: 0.431806, Tokens per Sec:     2705, Lr: 0.000300
2025-05-26 20:32:38,808 - INFO - joeynmt.training - Epoch   1, Step:     4000, Batch Loss:     2.454162, Batch Acc: 0.429389, Tokens per Sec:     2600, Lr: 0.000300
2025-05-26 20:32:38,809 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-26 20:36:01,246 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.39, ppl:  10.90, acc:   0.44, generation: 202.4301[sec], evaluation: 0.0000[sec]
2025-05-26 20:36:01,250 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-26 20:36:01,367 - INFO - joeynmt.helpers - delete models/bpe_8k/1500.ckpt
2025-05-26 20:36:01,393 - INFO - joeynmt.training - Example #0
2025-05-26 20:36:01,393 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-26 20:36:01,393 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-26 20:36:01,393 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ho', 'avuto', 'un', 'anno', 'ho', 'iniziato', 'a', 'due', 'anni', ',', 'sono', 'stati', 'a', 'due', 'anni', ',', 'che', 'la', 'parte', 'di', 'più', 'di', 'circa', 'il', 'suo', 'giorno', ',', 'il', 'suo', 'giorno', ',', 'il', 'mio', 'anno', ',', 'il', '20', 'anni', ',', 'il', '20', 'anni', '.', '</s>']
2025-05-26 20:36:01,393 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-26 20:36:01,393 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-26 20:36:01,393 - INFO - joeynmt.training - 	Hypothesis: Ho avuto un anno ho iniziato a due anni , sono stati a due anni , che la parte di più di circa il suo giorno , il suo giorno , il mio anno , il 20 anni , il 20 anni .
2025-05-26 20:36:01,393 - INFO - joeynmt.training - Example #1
2025-05-26 20:36:01,394 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-26 20:36:01,394 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-26 20:36:01,394 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'è', 'il', 'nostro', 'problema', 'di', 'questa', 'è', 'il', 'problema', 'di', 'questa', 'è', 'il', 'problema', 'del', 'problema', '.', '</s>']
2025-05-26 20:36:01,394 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-26 20:36:01,394 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-26 20:36:01,394 - INFO - joeynmt.training - 	Hypothesis: Ma questo è il nostro problema di questa è il problema di questa è il problema del problema .
2025-05-26 20:36:01,394 - INFO - joeynmt.training - Example #2
2025-05-26 20:36:01,394 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-26 20:36:01,394 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-26 20:36:01,394 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'parte', 'è', 'la', 'sua', 'parte', 'è', 'il', 'nostro', 'sistema', 'è', 'il', 'nostro', 'sistema', ',', 'è', 'il', 'nostro', 'sistema', ',', 'il', 'nostro', 'sistema', 'del', 'nostro', 'sistema', '.', '</s>']
2025-05-26 20:36:01,394 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-26 20:36:01,394 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-26 20:36:01,394 - INFO - joeynmt.training - 	Hypothesis: La parte è la sua parte è il nostro sistema è il nostro sistema , è il nostro sistema , il nostro sistema del nostro sistema .
2025-05-26 20:36:01,394 - INFO - joeynmt.training - Example #3
2025-05-26 20:36:01,394 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-26 20:36:01,394 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-26 20:36:01,394 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['È', 'un', 'po', "'", 'un', 'po', "'", 'in', 'un', 'po', "'", 'in', 'un', 'po', "'", 'in', 'un', 'po', "'", '.', '</s>']
2025-05-26 20:36:01,394 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-26 20:36:01,394 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-26 20:36:01,394 - INFO - joeynmt.training - 	Hypothesis: È un po ' un po ' in un po ' in un po ' in un po ' .
2025-05-26 20:36:01,394 - INFO - joeynmt.training - Example #4
2025-05-26 20:36:01,394 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-26 20:36:01,395 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-26 20:36:01,395 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'mia', 'mia', 'volta', 'che', 'vi', 'può', 'essere', 'un', 'po', "'", 'di', 'un', 'po', "'", 'di', 'anni', '.', '</s>']
2025-05-26 20:36:01,395 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-26 20:36:01,395 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-26 20:36:01,395 - INFO - joeynmt.training - 	Hypothesis: La mia mia volta che vi può essere un po ' di un po ' di anni .
2025-05-26 20:36:28,218 - INFO - joeynmt.training - Epoch   1, Step:     4100, Batch Loss:     2.478364, Batch Acc: 0.430201, Tokens per Sec:     2672, Lr: 0.000300
2025-05-26 20:36:55,574 - INFO - joeynmt.training - Epoch   1, Step:     4200, Batch Loss:     2.403861, Batch Acc: 0.432598, Tokens per Sec:     2739, Lr: 0.000300
2025-05-26 20:37:23,038 - INFO - joeynmt.training - Epoch   1, Step:     4300, Batch Loss:     2.450138, Batch Acc: 0.435057, Tokens per Sec:     2656, Lr: 0.000300
2025-05-26 20:37:50,312 - INFO - joeynmt.training - Epoch   1, Step:     4400, Batch Loss:     2.435450, Batch Acc: 0.435460, Tokens per Sec:     2607, Lr: 0.000300
2025-05-26 20:38:17,977 - INFO - joeynmt.training - Epoch   1, Step:     4500, Batch Loss:     2.428712, Batch Acc: 0.440477, Tokens per Sec:     2629, Lr: 0.000300
2025-05-26 20:38:17,977 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-26 20:40:46,307 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.33, ppl:  10.26, acc:   0.45, generation: 148.3224[sec], evaluation: 0.0000[sec]
2025-05-26 20:40:46,308 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-26 20:40:46,451 - INFO - joeynmt.helpers - delete models/bpe_8k/2000.ckpt
2025-05-26 20:40:46,473 - INFO - joeynmt.training - Example #0
2025-05-26 20:40:46,474 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-26 20:40:46,474 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-26 20:40:46,474 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', 'miei', 'anni', 'ho', 'imparato', 'che', 'ho', 'imparato', 'che', 'ho', 'fatto', 'che', 'la', 'cosa', 'che', 'la', 'parte', 'del', '20', '%', 'del', '20', '%', ',', 'la', 'parte', 'del', '20', '%', 'del', '20', '%', ',', 'che', 'la', 'storia', 'del', '20', '%', 'del', '20', '%', 'del', '20', '%', 'di', '50', '%', 'del', '20', '%', '.', '</s>']
2025-05-26 20:40:46,474 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-26 20:40:46,474 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-26 20:40:46,474 - INFO - joeynmt.training - 	Hypothesis: I miei anni ho imparato che ho imparato che ho fatto che la cosa che la parte del 20 % del 20 % , la parte del 20 % del 20 % , che la storia del 20 % del 20 % del 20 % di 50 % del 20 % .
2025-05-26 20:40:46,474 - INFO - joeynmt.training - Example #1
2025-05-26 20:40:46,474 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-26 20:40:46,474 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-26 20:40:46,474 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'problema', ',', 'la', 'cosa', 'è', 'la', 'cosa', 'che', 'non', 'è', 'la', 'cosa', 'non', 'è', 'la', 'cosa', 'non', 'è', 'la', 'cosa', 'non', 'è', 'la', 'parte', 'del', 'sistema', '.', '</s>']
2025-05-26 20:40:46,474 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-26 20:40:46,474 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-26 20:40:46,474 - INFO - joeynmt.training - 	Hypothesis: Ma questo problema , la cosa è la cosa che non è la cosa non è la cosa non è la cosa non è la parte del sistema .
2025-05-26 20:40:46,474 - INFO - joeynmt.training - Example #2
2025-05-26 20:40:46,474 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-26 20:40:46,474 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-26 20:40:46,474 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'parte', 'è', 'la', 'sua', 'parte', 'è', 'la', 'parte', 'è', 'un', 'sistema', 'è', 'un', 'sistema', ',', 'è', 'un', 'sistema', 'del', 'sistema', 'del', 'sistema', '.', '</s>']
2025-05-26 20:40:46,474 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-26 20:40:46,474 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-26 20:40:46,474 - INFO - joeynmt.training - 	Hypothesis: La parte è la sua parte è la parte è un sistema è un sistema , è un sistema del sistema del sistema .
2025-05-26 20:40:46,474 - INFO - joeynmt.training - Example #3
2025-05-26 20:40:46,474 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-26 20:40:46,475 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-26 20:40:46,475 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', "'", 'di', 'ri@@', '<unk>', '@', 'c@@', '<unk>', '@', 'c@@', '<unk>', '@', 'are', 'in', 'modo', 'in', 'modo', 'in', 'realtà', '.', '</s>']
2025-05-26 20:40:46,475 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-26 20:40:46,475 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-26 20:40:46,475 - INFO - joeynmt.training - 	Hypothesis: E ' di ri<unk> @ c<unk> @ c<unk> @ are in modo in modo in realtà .
2025-05-26 20:40:46,475 - INFO - joeynmt.training - Example #4
2025-05-26 20:40:46,475 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-26 20:40:46,475 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-26 20:40:46,475 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'mio', 'giorno', 'ho', 'fatto', 'un', 'tipo', 'di', 'un', 'tipo', 'di', 'un', 'tipo', 'di', 'un', 'gruppo', 'di', 'cosa', 'è', 'quello', 'che', 'è', 'quello', 'che', 'è', 'stato', 'stato', 'stato', 'stato', 'stato', '.', '</s>']
2025-05-26 20:40:46,475 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-26 20:40:46,475 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-26 20:40:46,475 - INFO - joeynmt.training - 	Hypothesis: Il mio giorno ho fatto un tipo di un tipo di un tipo di un gruppo di cosa è quello che è quello che è stato stato stato stato stato .
2025-05-26 20:47:31,384 - INFO - joeynmt.training - Epoch   1, Step:     4600, Batch Loss:     2.277764, Batch Acc: 0.443700, Tokens per Sec:      175, Lr: 0.000300
2025-05-26 20:47:59,197 - INFO - joeynmt.training - Epoch   1, Step:     4700, Batch Loss:     2.399505, Batch Acc: 0.440959, Tokens per Sec:     2666, Lr: 0.000300
2025-05-26 20:48:26,190 - INFO - joeynmt.training - Epoch   1, Step:     4800, Batch Loss:     2.419429, Batch Acc: 0.442550, Tokens per Sec:     2627, Lr: 0.000300
2025-05-26 20:48:55,187 - INFO - joeynmt.training - Epoch   1, Step:     4900, Batch Loss:     2.259264, Batch Acc: 0.446762, Tokens per Sec:     2564, Lr: 0.000300
2025-05-26 20:49:22,932 - INFO - joeynmt.training - Epoch   1, Step:     5000, Batch Loss:     2.371839, Batch Acc: 0.448034, Tokens per Sec:     2628, Lr: 0.000300
2025-05-26 20:49:22,932 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-26 20:53:14,684 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.28, ppl:   9.80, acc:   0.46, generation: 231.7428[sec], evaluation: 0.0000[sec]
2025-05-26 20:53:14,688 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-26 20:53:14,821 - INFO - joeynmt.helpers - delete models/bpe_8k/2500.ckpt
2025-05-26 20:53:14,854 - INFO - joeynmt.training - Example #0
2025-05-26 20:53:14,854 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-26 20:53:14,854 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-26 20:53:14,854 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['S@@', '<unk>', '@', 'ai', 'tre', 'anni', 'ho', 'iniziato', 'a', 'queste', 'due', 'anni', 'che', 'sono', 'stati', 'in', 'cui', 'sono', 'stati', 'in', 'cui', 'la', 'maggior', 'parte', 'di', 'più', 'di', 'due', 'anni', ',', 'che', 'la', 'maggior', 'parte', 'di', 'due', 'anni', ',', 'che', 'il', '30', 'anni', ',', 'il', '30', 'anni', ',', 'il', '30', 'anni', ',', 'il', '30', 'anni', ',', 'il', '30', 'anni', ',', 'il', '30', '%', ',', 'il', '30', '%', ',', 'il', '30', '%', '.', '</s>']
2025-05-26 20:53:14,854 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-26 20:53:14,854 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-26 20:53:14,854 - INFO - joeynmt.training - 	Hypothesis: S<unk> @ ai tre anni ho iniziato a queste due anni che sono stati in cui sono stati in cui la maggior parte di più di due anni , che la maggior parte di due anni , che il 30 anni , il 30 anni , il 30 anni , il 30 anni , il 30 anni , il 30 % , il 30 % , il 30 % .
2025-05-26 20:53:14,854 - INFO - joeynmt.training - Example #1
2025-05-26 20:53:14,854 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-26 20:53:14,854 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-26 20:53:14,854 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'è', 'la', 'struttura', 'di', 'questa', 'è', 'la', 'parte', 'di', 'questa', 'è', 'la', 'risposta', 'perché', 'non', 'non', 'è', 'la', 'teoria', 'della', 'luce', '.', '</s>']
2025-05-26 20:53:14,854 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-26 20:53:14,854 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-26 20:53:14,854 - INFO - joeynmt.training - 	Hypothesis: Ma questo è la struttura di questa è la parte di questa è la risposta perché non non è la teoria della luce .
2025-05-26 20:53:14,854 - INFO - joeynmt.training - Example #2
2025-05-26 20:53:14,854 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-26 20:53:14,854 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-26 20:53:14,854 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'c@@', '<unk>', '@', 'zione', 'di', 'un', 'po', "'", "'", "'", 'di', 'un', 'po', "'", ',', 'è', 'un', 'sistema', ',', 'il', 'sistema', 'di', 'un', 'sistema', 'di', 'sistema', '.', '</s>']
2025-05-26 20:53:14,855 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-26 20:53:14,855 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-26 20:53:14,855 - INFO - joeynmt.training - 	Hypothesis: Il c<unk> @ zione di un po ' ' ' di un po ' , è un sistema , il sistema di un sistema di sistema .
2025-05-26 20:53:14,855 - INFO - joeynmt.training - Example #3
2025-05-26 20:53:14,855 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-26 20:53:14,855 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-26 20:53:14,855 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', "'", 'un', 'po', "'", "'", 'di', 'un', 'po', "'", 'di', 'un', 'po', "'", '.', '</s>']
2025-05-26 20:53:14,855 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-26 20:53:14,855 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-26 20:53:14,855 - INFO - joeynmt.training - 	Hypothesis: E ' un po ' ' di un po ' di un po ' .
2025-05-26 20:53:14,855 - INFO - joeynmt.training - Example #4
2025-05-26 20:53:14,855 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-26 20:53:14,855 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-26 20:53:14,855 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'mio', 'primo', 'anno', 'ho', 'fatto', 'un', 'po', "'", 'di', 'un', 'po', "'", 'di', 'un', 'paio', 'di', 'anni', '.', '</s>']
2025-05-26 20:53:14,855 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-26 20:53:14,855 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-26 20:53:14,855 - INFO - joeynmt.training - 	Hypothesis: Il mio primo anno ho fatto un po ' di un po ' di un paio di anni .
2025-05-26 20:53:14,856 - INFO - joeynmt.training - Epoch   1: total training loss 13425.74
2025-05-26 20:53:14,856 - INFO - joeynmt.training - EPOCH 2
2025-05-26 20:53:43,764 - INFO - joeynmt.training - Epoch   2, Step:     5100, Batch Loss:     2.375703, Batch Acc: 0.451783, Tokens per Sec:     2603, Lr: 0.000300
2025-05-26 20:54:13,495 - INFO - joeynmt.training - Epoch   2, Step:     5200, Batch Loss:     2.208740, Batch Acc: 0.450225, Tokens per Sec:     2489, Lr: 0.000300
2025-05-26 20:54:41,423 - INFO - joeynmt.training - Epoch   2, Step:     5300, Batch Loss:     2.282806, Batch Acc: 0.452216, Tokens per Sec:     2662, Lr: 0.000300
2025-05-26 20:55:09,880 - INFO - joeynmt.training - Epoch   2, Step:     5400, Batch Loss:     2.284256, Batch Acc: 0.457423, Tokens per Sec:     2699, Lr: 0.000300
2025-05-26 20:55:37,024 - INFO - joeynmt.training - Epoch   2, Step:     5500, Batch Loss:     2.236821, Batch Acc: 0.456254, Tokens per Sec:     2658, Lr: 0.000300
2025-05-26 20:55:37,024 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-26 20:59:06,979 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.21, ppl:   9.10, acc:   0.47, generation: 209.9470[sec], evaluation: 0.0000[sec]
2025-05-26 20:59:06,981 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-26 20:59:07,104 - INFO - joeynmt.helpers - delete models/bpe_8k/3000.ckpt
2025-05-26 20:59:07,137 - INFO - joeynmt.training - Example #0
2025-05-26 20:59:07,137 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-26 20:59:07,138 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-26 20:59:07,138 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prima', 'volta', 'che', 'ho', 'scoperto', 'che', 'ho', 'scoperto', 'che', 'questi', 'due', 'anni', 'che', 'ha', 'fatto', 'che', 'la', 'di@@', '<unk>', '@', 'zione', 'che', 'la', 'di@@', '<unk>', '@', 'zione', 'di', 'tre', 'anni', ',', 'che', 'è', 'stata', 'il', 'più', 'di', 'tre', 'anni', ',', 'che', 'è', 'stata', 'stata', 'il', '50', 'anni', ',', 'il', '50', 'anni', ',', 'il', '50', 'anni', ',', 'il', '50', 'anni', ',', 'il', '50', '%', ',', 'il', '50', 'anni', '.', '</s>']
2025-05-26 20:59:07,138 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-26 20:59:07,138 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-26 20:59:07,138 - INFO - joeynmt.training - 	Hypothesis: La prima volta che ho scoperto che ho scoperto che questi due anni che ha fatto che la di<unk> @ zione che la di<unk> @ zione di tre anni , che è stata il più di tre anni , che è stata stata il 50 anni , il 50 anni , il 50 anni , il 50 anni , il 50 % , il 50 anni .
2025-05-26 20:59:07,138 - INFO - joeynmt.training - Example #1
2025-05-26 20:59:07,138 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-26 20:59:07,138 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-26 20:59:07,138 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'problema', 'di', 'questa', 'è', 'la', 'parte', 'della', 'parte', 'della', 'non', 'è', 'che', 'non', 'è', 'che', 'non', 'è', 'la', 'stessa', 'non', 'è', 'la', 'luce', '.', '</s>']
2025-05-26 20:59:07,138 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-26 20:59:07,138 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-26 20:59:07,138 - INFO - joeynmt.training - 	Hypothesis: Ma questo problema di questa è la parte della parte della non è che non è che non è la stessa non è la luce .
2025-05-26 20:59:07,138 - INFO - joeynmt.training - Example #2
2025-05-26 20:59:07,138 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-26 20:59:07,138 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-26 20:59:07,138 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'rete', 'di', 'ri@@', '<unk>', '@', 'zione', 'è', 'un', 'senso', ',', 'è', 'un', 'senso', ',', 'un', 'senso', ',', 'il', 'sistema', 'del', 'sistema', 'del', 'sistema', 'del', 'sistema', '.', '</s>']
2025-05-26 20:59:07,138 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-26 20:59:07,138 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-26 20:59:07,138 - INFO - joeynmt.training - 	Hypothesis: La rete di ri<unk> @ zione è un senso , è un senso , un senso , il sistema del sistema del sistema del sistema .
2025-05-26 20:59:07,138 - INFO - joeynmt.training - Example #3
2025-05-26 20:59:07,138 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-26 20:59:07,139 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-26 20:59:07,139 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['È', 'un', 'po', "'", 'di', 'loro', 'in', 'modo', 'in', 'modo', 'in', 'cui', 'si', 'trova', 'in', 'cui', 'si', 'in', 'cui', 'si', 'trova', '.', '</s>']
2025-05-26 20:59:07,139 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-26 20:59:07,139 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-26 20:59:07,139 - INFO - joeynmt.training - 	Hypothesis: È un po ' di loro in modo in modo in cui si trova in cui si in cui si trova .
2025-05-26 20:59:07,139 - INFO - joeynmt.training - Example #4
2025-05-26 20:59:07,139 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-26 20:59:07,139 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-26 20:59:07,139 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'fine', ',', 'ho', 'fatto', 'un', 'po', "'", 'di', 'un', 'po', "'", 'di', 'un', 'po', "'", 'anni', '.', '</s>']
2025-05-26 20:59:07,139 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-26 20:59:07,139 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-26 20:59:07,139 - INFO - joeynmt.training - 	Hypothesis: La fine , ho fatto un po ' di un po ' di un po ' anni .
2025-05-26 20:59:35,456 - INFO - joeynmt.training - Epoch   2, Step:     5600, Batch Loss:     2.368870, Batch Acc: 0.459459, Tokens per Sec:     2571, Lr: 0.000300
2025-05-26 21:00:02,870 - INFO - joeynmt.training - Epoch   2, Step:     5700, Batch Loss:     2.290369, Batch Acc: 0.459696, Tokens per Sec:     2716, Lr: 0.000300
2025-05-26 21:00:31,392 - INFO - joeynmt.training - Epoch   2, Step:     5800, Batch Loss:     2.487382, Batch Acc: 0.461201, Tokens per Sec:     2588, Lr: 0.000300
2025-05-26 21:00:58,679 - INFO - joeynmt.training - Epoch   2, Step:     5900, Batch Loss:     2.347532, Batch Acc: 0.459628, Tokens per Sec:     2679, Lr: 0.000300
2025-05-26 21:01:25,725 - INFO - joeynmt.training - Epoch   2, Step:     6000, Batch Loss:     2.086077, Batch Acc: 0.471551, Tokens per Sec:     2672, Lr: 0.000300
2025-05-26 21:01:25,725 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-26 21:03:33,826 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.15, ppl:   8.58, acc:   0.48, generation: 128.0942[sec], evaluation: 0.0000[sec]
2025-05-26 21:03:33,827 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-26 21:03:33,948 - INFO - joeynmt.helpers - delete models/bpe_8k/3500.ckpt
2025-05-26 21:03:33,968 - INFO - joeynmt.training - Example #0
2025-05-26 21:03:33,968 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-26 21:03:33,968 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-26 21:03:33,968 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'Cina', 'ho', 'visto', 'queste', 'due', 'anni', 'ho', 'visto', 'queste', 'due', 'anni', '.', '</s>']
2025-05-26 21:03:33,968 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-26 21:03:33,968 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-26 21:03:33,968 - INFO - joeynmt.training - 	Hypothesis: La Cina ho visto queste due anni ho visto queste due anni .
2025-05-26 21:03:33,968 - INFO - joeynmt.training - Example #1
2025-05-26 21:03:33,968 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-26 21:03:33,968 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-26 21:03:33,968 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'problema', 'del', 'problema', 'del', 'problema', 'del', 'problema', 'perché', 'non', 'è', 'che', 'non', 'è', 'che', 'non', 'è', 'che', 'non', 'è', 'che', 'non', 'è', 'la', 'parte', 'della', 'luce', '.', '</s>']
2025-05-26 21:03:33,968 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-26 21:03:33,968 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-26 21:03:33,968 - INFO - joeynmt.training - 	Hypothesis: Ma questo problema del problema del problema del problema perché non è che non è che non è che non è che non è la parte della luce .
2025-05-26 21:03:33,969 - INFO - joeynmt.training - Example #2
2025-05-26 21:03:33,969 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-26 21:03:33,969 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-26 21:03:33,969 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'sistema', 'è', 'in', 'cui', 'è', 'in', 'un', 'senso', ',', 'è', 'in', 'un', 'senso', ',', 'il', 'senso', ',', 'il', 'sistema', 'del', 'sistema', 'del', 'sistema', 'del', 'sistema', '.', '</s>']
2025-05-26 21:03:33,969 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-26 21:03:33,969 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-26 21:03:33,969 - INFO - joeynmt.training - 	Hypothesis: Il sistema è in cui è in un senso , è in un senso , il senso , il sistema del sistema del sistema del sistema .
2025-05-26 21:03:33,969 - INFO - joeynmt.training - Example #3
2025-05-26 21:03:33,969 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-26 21:03:33,969 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-26 21:03:33,969 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['È', 'un', 'po', "'", 'di', 'c@@', '<unk>', '@', 'ore', 'in', 'modo', 'in', 'cui', 'si', 'può', 'ri@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'si@@', '<unk>', '@', 'to', '.', '</s>']
2025-05-26 21:03:33,969 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-26 21:03:33,969 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-26 21:03:33,969 - INFO - joeynmt.training - 	Hypothesis: È un po ' di c<unk> @ ore in modo in cui si può ri<unk> @ ri<unk> @ ri<unk> @ ri<unk> @ ri<unk> @ ri<unk> @ ri<unk> @ ri<unk> @ ri<unk> @ ri<unk> @ si<unk> @ to .
2025-05-26 21:03:33,969 - INFO - joeynmt.training - Example #4
2025-05-26 21:03:33,969 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-26 21:03:33,969 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-26 21:03:33,969 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'vostro', 'lavoro', 'che', 'vi', 'vi', 'vi', 'mostrerò', 'un', 'po', "'", 'un', 'po', "'", 'di', 'un', 'po', "'", 'anni', '.', '</s>']
2025-05-26 21:03:33,969 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-26 21:03:33,969 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-26 21:03:33,969 - INFO - joeynmt.training - 	Hypothesis: Il vostro lavoro che vi vi vi mostrerò un po ' un po ' di un po ' anni .
2025-05-26 21:04:01,623 - INFO - joeynmt.training - Epoch   2, Step:     6100, Batch Loss:     2.212488, Batch Acc: 0.468344, Tokens per Sec:     2508, Lr: 0.000300
2025-05-26 21:04:29,408 - INFO - joeynmt.training - Epoch   2, Step:     6200, Batch Loss:     2.286754, Batch Acc: 0.470317, Tokens per Sec:     2643, Lr: 0.000300
2025-05-26 21:04:57,076 - INFO - joeynmt.training - Epoch   2, Step:     6300, Batch Loss:     2.175383, Batch Acc: 0.473217, Tokens per Sec:     2722, Lr: 0.000300
2025-05-26 21:05:24,498 - INFO - joeynmt.training - Epoch   2, Step:     6400, Batch Loss:     2.084110, Batch Acc: 0.475148, Tokens per Sec:     2707, Lr: 0.000300
2025-05-26 21:05:52,586 - INFO - joeynmt.training - Epoch   2, Step:     6500, Batch Loss:     2.109558, Batch Acc: 0.474783, Tokens per Sec:     2678, Lr: 0.000300
2025-05-26 21:05:52,587 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-26 21:08:37,560 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.09, ppl:   8.09, acc:   0.49, generation: 164.9662[sec], evaluation: 0.0000[sec]
2025-05-26 21:08:37,562 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-26 21:08:37,685 - INFO - joeynmt.helpers - delete models/bpe_8k/4000.ckpt
2025-05-26 21:08:37,704 - INFO - joeynmt.training - Example #0
2025-05-26 21:08:37,704 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-26 21:08:37,704 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-26 21:08:37,704 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'ho', 'scoperto', 'che', 'ho', 'scoperto', 'che', 'questo', 'che', 'ho', 'scoperto', 'che', 'questo', 'che', 'ha', 'fatto', 'che', "l'", 'ultima', 'volta', 'che', 'è', 'il', 'più', 'più', 'di', 'tre', 'milioni', 'di', 'anni', ',', 'che', 'è', 'più', 'di', 'tre', 'milioni', 'di', 'anni', ',', 'che', 'è', 'stata', 'il', '50', 'milioni', 'di', 'anni', ',', 'il', '50', 'milioni', 'di', 'anni', '.', '</s>']
2025-05-26 21:08:37,705 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-26 21:08:37,705 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-26 21:08:37,705 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso ho scoperto che ho scoperto che questo che ho scoperto che questo che ha fatto che l' ultima volta che è il più più di tre milioni di anni , che è più di tre milioni di anni , che è stata il 50 milioni di anni , il 50 milioni di anni .
2025-05-26 21:08:37,705 - INFO - joeynmt.training - Example #1
2025-05-26 21:08:37,705 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-26 21:08:37,705 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-26 21:08:37,705 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'è', 'che', 'la', 'stessa', 'parte', 'di', 'questo', 'problema', 'perché', 'non', 'è', 'che', 'non', 'è', 'che', 'è', 'che', 'non', 'è', 'la', 'sua', 'parte', 'del', 'problema', 'che', 'non', 'è', 'la', 'sua', 'parte', 'del', 'problema', '.', '</s>']
2025-05-26 21:08:37,705 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-26 21:08:37,705 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-26 21:08:37,705 - INFO - joeynmt.training - 	Hypothesis: Ma questo è che la stessa parte di questo problema perché non è che non è che è che non è la sua parte del problema che non è la sua parte del problema .
2025-05-26 21:08:37,705 - INFO - joeynmt.training - Example #2
2025-05-26 21:08:37,705 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-26 21:08:37,705 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-26 21:08:37,705 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'p@@', '<unk>', '@', 'ica', 'è', 'la', 'ri@@', '<unk>', '@', 'zione', 'è', 'un', 'senso', ',', 'è', 'un', 'senso', ',', 'il', 'sistema', 'di', 'energia', 'del', 'sistema', '.', '</s>']
2025-05-26 21:08:37,705 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-26 21:08:37,705 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-26 21:08:37,705 - INFO - joeynmt.training - 	Hypothesis: Il p<unk> @ ica è la ri<unk> @ zione è un senso , è un senso , il sistema di energia del sistema .
2025-05-26 21:08:37,705 - INFO - joeynmt.training - Example #3
2025-05-26 21:08:37,705 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-26 21:08:37,706 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-26 21:08:37,706 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['È', 'un', 'p@@', '<unk>', '@', 'ore', 'in', 'modo', 'in', 'cui', 'si', 'può', 'essere', 'in', 'cui', 'il', 'suo', 'suo', 'suo', 'posto', '.', '</s>']
2025-05-26 21:08:37,706 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-26 21:08:37,706 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-26 21:08:37,706 - INFO - joeynmt.training - 	Hypothesis: È un p<unk> @ ore in modo in cui si può essere in cui il suo suo suo posto .
2025-05-26 21:08:37,706 - INFO - joeynmt.training - Example #4
2025-05-26 21:08:37,706 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-26 21:08:37,706 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-26 21:08:37,706 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'volta', 'che', 'vi', 'mostrerò', 'un', 'po', "'", 'di', 'un', 'po', "'", "'", 'di', 'ciò', 'che', 'è', 'successo', 'è', 'successo', '.', '</s>']
2025-05-26 21:08:37,706 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-26 21:08:37,706 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-26 21:08:37,706 - INFO - joeynmt.training - 	Hypothesis: La prossima volta che vi mostrerò un po ' di un po ' ' di ciò che è successo è successo .
2025-05-26 21:09:05,687 - INFO - joeynmt.training - Epoch   2, Step:     6600, Batch Loss:     2.098772, Batch Acc: 0.478911, Tokens per Sec:     2612, Lr: 0.000300
2025-05-26 21:09:33,723 - INFO - joeynmt.training - Epoch   2, Step:     6700, Batch Loss:     2.122907, Batch Acc: 0.476414, Tokens per Sec:     2630, Lr: 0.000300
2025-05-26 21:10:01,371 - INFO - joeynmt.training - Epoch   2, Step:     6800, Batch Loss:     2.172529, Batch Acc: 0.481320, Tokens per Sec:     2617, Lr: 0.000300
2025-05-26 21:10:30,175 - INFO - joeynmt.training - Epoch   2, Step:     6900, Batch Loss:     2.140670, Batch Acc: 0.483734, Tokens per Sec:     2583, Lr: 0.000300
2025-05-26 21:10:57,455 - INFO - joeynmt.training - Epoch   2, Step:     7000, Batch Loss:     2.100803, Batch Acc: 0.485167, Tokens per Sec:     2794, Lr: 0.000300
2025-05-26 21:10:57,455 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-26 21:13:52,393 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.04, ppl:   7.66, acc:   0.50, generation: 174.9307[sec], evaluation: 0.0000[sec]
2025-05-26 21:13:52,396 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-26 21:13:52,518 - INFO - joeynmt.helpers - delete models/bpe_8k/4500.ckpt
2025-05-26 21:13:52,537 - INFO - joeynmt.training - Example #0
2025-05-26 21:13:52,537 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-26 21:13:52,537 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-26 21:13:52,537 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', '@', 'anno', 'anno', 'anno', 'ho', 'imparato', 'queste', 'due', 'due', 'persone', 'che', 'ha', 'fatto', 'che', 'ha', 'fatto', 'che', "l'", 'in@@', '<unk>', '@', 'ente', ',', 'che', 'il', 'più', 'più', 'di', 'tre', 'milioni', 'di', 'anni', ',', 'che', 'ha', 'fatto', 'che', 'è', 'stata', 'la', 'maggior', 'parte', 'delle', 'tre', 'milioni', 'di', 'anni', ',', 'che', 'ha', 'stata', 'la', 'maggior', 'parte', 'delle', 'tre', 'milioni', 'di', 'persone', 'che', 'ha', 'stata', 'la', 'sua', 'ri@@', '<unk>', '@', 'de', '.', '</s>']
2025-05-26 21:13:52,537 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-26 21:13:52,537 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-26 21:13:52,537 - INFO - joeynmt.training - 	Hypothesis: L<unk> @ anno anno anno ho imparato queste due due persone che ha fatto che ha fatto che l' in<unk> @ ente , che il più più di tre milioni di anni , che ha fatto che è stata la maggior parte delle tre milioni di anni , che ha stata la maggior parte delle tre milioni di persone che ha stata la sua ri<unk> @ de .
2025-05-26 21:13:52,537 - INFO - joeynmt.training - Example #1
2025-05-26 21:13:52,537 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-26 21:13:52,537 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-26 21:13:52,537 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'sappiamo', 'che', 'la', 'maggior', 'parte', 'di', 'questo', 'problema', 'perché', 'non', 'è', 'la', 'cosa', 'che', 'non', 'è', 'la', 'gente', 'che', 'non', 'è', 'la', 'di@@', '<unk>', '@', 'zione', '.', '</s>']
2025-05-26 21:13:52,537 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-26 21:13:52,537 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-26 21:13:52,537 - INFO - joeynmt.training - 	Hypothesis: Ma questo sappiamo che la maggior parte di questo problema perché non è la cosa che non è la gente che non è la di<unk> @ zione .
2025-05-26 21:13:52,537 - INFO - joeynmt.training - Example #2
2025-05-26 21:13:52,537 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-26 21:13:52,537 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-26 21:13:52,537 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'p@@', '<unk>', '@', 'ica', 'è', 'in', 'un', 'senso', ',', 'in', 'un', 'senso', ',', 'il', 'sistema', ',', 'il', 'sistema', 'del', 'sistema', 'del', 'sistema', 'del', 'sistema', '.', '</s>']
2025-05-26 21:13:52,538 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-26 21:13:52,538 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-26 21:13:52,538 - INFO - joeynmt.training - 	Hypothesis: Il p<unk> @ ica è in un senso , in un senso , il sistema , il sistema del sistema del sistema del sistema .
2025-05-26 21:13:52,538 - INFO - joeynmt.training - Example #3
2025-05-26 21:13:52,538 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-26 21:13:52,538 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-26 21:13:52,538 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['È', 'un', 'p@@', '<unk>', '@', 'ina', 'in', 'modo', 'in', 'cui', 'si', 'si', 'ha', 'ri@@', '<unk>', '@', 'sul@@', '<unk>', '@', 'to', 'e', 'il', 'suo', 'fatto', '.', '</s>']
2025-05-26 21:13:52,538 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-26 21:13:52,538 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-26 21:13:52,538 - INFO - joeynmt.training - 	Hypothesis: È un p<unk> @ ina in modo in cui si si ha ri<unk> @ sul<unk> @ to e il suo fatto .
2025-05-26 21:13:52,538 - INFO - joeynmt.training - Example #4
2025-05-26 21:13:52,538 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-26 21:13:52,538 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-26 21:13:52,538 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'prossimo', 'punto', 'che', 'vi', 'vi', 'vi', 'vi', 'mostrerò', 'un', 'un', 'po', "'", 'di', 'un', 'po', "'", 'di', 'cosa', 'accad@@', '<unk>', '@', 'uto', 'di', 'cosa', 'accad@@', '<unk>', '@', 'uto', '.', '</s>']
2025-05-26 21:13:52,538 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-26 21:13:52,538 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-26 21:13:52,538 - INFO - joeynmt.training - 	Hypothesis: Il prossimo punto che vi vi vi vi mostrerò un un po ' di un po ' di cosa accad<unk> @ uto di cosa accad<unk> @ uto .
2025-05-26 21:14:20,147 - INFO - joeynmt.training - Epoch   2, Step:     7100, Batch Loss:     2.041267, Batch Acc: 0.485455, Tokens per Sec:     2646, Lr: 0.000300
2025-05-26 21:14:47,581 - INFO - joeynmt.training - Epoch   2, Step:     7200, Batch Loss:     2.102784, Batch Acc: 0.488708, Tokens per Sec:     2652, Lr: 0.000300
2025-05-26 21:15:15,650 - INFO - joeynmt.training - Epoch   2, Step:     7300, Batch Loss:     2.172208, Batch Acc: 0.494810, Tokens per Sec:     2626, Lr: 0.000300
2025-05-26 21:15:43,239 - INFO - joeynmt.training - Epoch   2, Step:     7400, Batch Loss:     2.205292, Batch Acc: 0.489386, Tokens per Sec:     2633, Lr: 0.000300
2025-05-26 21:16:11,145 - INFO - joeynmt.training - Epoch   2, Step:     7500, Batch Loss:     2.291482, Batch Acc: 0.492939, Tokens per Sec:     2715, Lr: 0.000300
2025-05-26 21:16:11,145 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-26 21:18:59,315 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.99, ppl:   7.29, acc:   0.52, generation: 168.1633[sec], evaluation: 0.0000[sec]
2025-05-26 21:18:59,318 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-26 21:18:59,448 - INFO - joeynmt.helpers - delete models/bpe_8k/5000.ckpt
2025-05-26 21:18:59,454 - INFO - joeynmt.training - Example #0
2025-05-26 21:18:59,455 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-26 21:18:59,455 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-26 21:18:59,455 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', '@', 'anno', 'ho', 'mostrato', 'queste', 'due', 'anni', ',', 'che', 'mi', 'ha', 'mostrato', 'che', 'la', 'di@@', '<unk>', '@', 'zione', 'che', 'la', 'st@@', '<unk>', '@', 'ica', ',', 'che', 'il', 'più', 'di', 'due', 'milioni', 'di', 'anni', 'ha', 'stata', 'la', 'maggior', 'parte', 'del', '2@@', '<unk>', '@', '8', 'milioni', 'di', 'anni', ',', 'ha', 'stata', 'la', 'maggior', 'parte', 'del', '9@@', '<unk>', '@', '8', '%', '.', '</s>']
2025-05-26 21:18:59,455 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-26 21:18:59,455 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-26 21:18:59,455 - INFO - joeynmt.training - 	Hypothesis: L<unk> @ anno ho mostrato queste due anni , che mi ha mostrato che la di<unk> @ zione che la st<unk> @ ica , che il più di due milioni di anni ha stata la maggior parte del 2<unk> @ 8 milioni di anni , ha stata la maggior parte del 9<unk> @ 8 % .
2025-05-26 21:18:59,455 - INFO - joeynmt.training - Example #1
2025-05-26 21:18:59,455 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-26 21:18:59,455 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-26 21:18:59,455 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'sappiamo', 'che', 'la', 'soluzione', 'di', 'questo', 'problema', 'perché', 'non', 'è', 'la', 'cosa', 'non', 'è', 'la', 'cosa', 'non', 'è', 'la', 'cosa', 'non', 'è', 'la', 's@@', '<unk>', '@', 'atto', '.', '</s>']
2025-05-26 21:18:59,455 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-26 21:18:59,455 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-26 21:18:59,455 - INFO - joeynmt.training - 	Hypothesis: Ma questo sappiamo che la soluzione di questo problema perché non è la cosa non è la cosa non è la cosa non è la s<unk> @ atto .
2025-05-26 21:18:59,455 - INFO - joeynmt.training - Example #2
2025-05-26 21:18:59,456 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-26 21:18:59,456 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-26 21:18:59,456 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'p@@', '<unk>', '@', 'ico', 'è', ',', 'in', 'un', 'senso', ',', 'in', 'un', 'senso', ',', 'in', 'un', 'senso', ',', 'il', 'sistema', ',', 'il', 'sistema', 'globale', '.', '</s>']
2025-05-26 21:18:59,456 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-26 21:18:59,456 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-26 21:18:59,456 - INFO - joeynmt.training - 	Hypothesis: Il p<unk> @ ico è , in un senso , in un senso , in un senso , il sistema , il sistema globale .
2025-05-26 21:18:59,456 - INFO - joeynmt.training - Example #3
2025-05-26 21:18:59,456 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-26 21:18:59,456 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-26 21:18:59,456 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['È', 'un', 'p@@', '<unk>', '@', 'ina', 'in', 'modo', 'in', 'cui', 'si', 'tratta', 'di', 'un', 'p@@', '<unk>', '@', 'etto', 'e', 'si', 'tratta', '.', '</s>']
2025-05-26 21:18:59,456 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-26 21:18:59,456 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-26 21:18:59,456 - INFO - joeynmt.training - 	Hypothesis: È un p<unk> @ ina in modo in cui si tratta di un p<unk> @ etto e si tratta .
2025-05-26 21:18:59,456 - INFO - joeynmt.training - Example #4
2025-05-26 21:18:59,456 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-26 21:18:59,456 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-26 21:18:59,456 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'volta', 'che', 'vi', 'vi', 'vi', 'vi', 'vi', 'mostrerò', 'un', 'al@@', '<unk>', '@', 'da', 'un', 'al@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', '-@@', '<unk>', '@', 'la', 'prossima', 'volta', 'che', 'vi', 'vi', 'vi', 'vi', 'vi', 'vi', 'vi', 'vi', 'vi', 'mostrerò', 'un', 'po', "'", 'anni', '.', '</s>']
2025-05-26 21:18:59,457 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-26 21:18:59,457 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-26 21:18:59,457 - INFO - joeynmt.training - 	Hypothesis: La prossima volta che vi vi vi vi vi mostrerò un al<unk> @ da un al<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ -<unk> @ la prossima volta che vi vi vi vi vi vi vi vi vi mostrerò un po ' anni .
2025-05-26 21:19:27,398 - INFO - joeynmt.training - Epoch   2, Step:     7600, Batch Loss:     1.943547, Batch Acc: 0.494963, Tokens per Sec:     2630, Lr: 0.000300
2025-05-26 21:19:56,121 - INFO - joeynmt.training - Epoch   2, Step:     7700, Batch Loss:     2.007759, Batch Acc: 0.499934, Tokens per Sec:     2636, Lr: 0.000300
2025-05-26 21:20:24,180 - INFO - joeynmt.training - Epoch   2, Step:     7800, Batch Loss:     2.144871, Batch Acc: 0.494825, Tokens per Sec:     2614, Lr: 0.000300
2025-05-26 21:20:52,442 - INFO - joeynmt.training - Epoch   2, Step:     7900, Batch Loss:     1.992166, Batch Acc: 0.501600, Tokens per Sec:     2620, Lr: 0.000300
2025-05-26 21:21:20,541 - INFO - joeynmt.training - Epoch   2, Step:     8000, Batch Loss:     2.004716, Batch Acc: 0.501524, Tokens per Sec:     2663, Lr: 0.000300
2025-05-26 21:21:20,542 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-26 21:24:25,142 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.95, ppl:   7.00, acc:   0.53, generation: 184.5925[sec], evaluation: 0.0000[sec]
2025-05-26 21:24:25,145 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-26 21:24:25,324 - INFO - joeynmt.helpers - delete models/bpe_8k/5500.ckpt
2025-05-26 21:24:25,350 - INFO - joeynmt.training - Example #0
2025-05-26 21:24:25,350 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-26 21:24:25,350 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-26 21:24:25,350 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'ho', 'mostrato', 'questi', 'due', 'due', 'anni', ',', 'ho', 'mostrato', 'che', 'le', 'due', 'anni', 'sono', 'stati', 'stati', 'stati', 'in', 'cui', "l'", 'in@@', '<unk>', '@', 'st@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'cento', 'di', 'tre', 'anni', 'ha', 'la', 'maggior', 'parte', 'di', 'tre', 'anni', 'ha', 'stato', 'il', '9@@', '<unk>', '@', '8', '%', 'di', 'tre', 'anni', ',', 'ha', 'stato', 'il', '9@@', '<unk>', '@', '8', '%', '.', '</s>']
2025-05-26 21:24:25,350 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-26 21:24:25,350 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-26 21:24:25,350 - INFO - joeynmt.training - 	Hypothesis: L' anno ho mostrato questi due due anni , ho mostrato che le due anni sono stati stati stati in cui l' in<unk> @ st<unk> @ ico , che per cento di tre anni ha la maggior parte di tre anni ha stato il 9<unk> @ 8 % di tre anni , ha stato il 9<unk> @ 8 % .
2025-05-26 21:24:25,350 - INFO - joeynmt.training - Example #1
2025-05-26 21:24:25,350 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-26 21:24:25,350 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-26 21:24:25,350 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'è', 'la', 'parte', 'del', 'problema', 'perché', 'non', 'è', 'il', 'problema', 'perché', 'non', 'è', 'il', 'problema', 'non', 'è', 'il', 'piano', 'perché', 'non', 'si', 've@@', '<unk>', '@', 'm@@', '<unk>', '@', 'enza', 'del', 'ghiaccio', '.', '</s>']
2025-05-26 21:24:25,350 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-26 21:24:25,350 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-26 21:24:25,350 - INFO - joeynmt.training - 	Hypothesis: Ma questo è la parte del problema perché non è il problema perché non è il problema non è il piano perché non si ve<unk> @ m<unk> @ enza del ghiaccio .
2025-05-26 21:24:25,350 - INFO - joeynmt.training - Example #2
2025-05-26 21:24:25,350 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-26 21:24:25,350 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-26 21:24:25,350 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'cal@@', '<unk>', '@', 'do', 'è', ',', 'in', 'un', 'senso', ',', 'in', 'un', 'senso', ',', 'il', 'sistema', 'di', 'sistema', 'di', 'sistema', 'di', 'sistema', 'del', 'sistema', 'del', 'sistema', 'mondiale', '.', '</s>']
2025-05-26 21:24:25,351 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-26 21:24:25,351 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-26 21:24:25,351 - INFO - joeynmt.training - 	Hypothesis: Il cal<unk> @ do è , in un senso , in un senso , il sistema di sistema di sistema di sistema del sistema del sistema mondiale .
2025-05-26 21:24:25,351 - INFO - joeynmt.training - Example #3
2025-05-26 21:24:25,351 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-26 21:24:25,351 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-26 21:24:25,351 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['È', 'un', 'con@@', '<unk>', '@', 'g@@', '<unk>', '@', 'ere', 'in', 'grado', 'di', 's@@', '<unk>', '@', 'enti', 'e', 'il', 'suo', 'fatto', '.', '</s>']
2025-05-26 21:24:25,351 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-26 21:24:25,351 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-26 21:24:25,351 - INFO - joeynmt.training - 	Hypothesis: È un con<unk> @ g<unk> @ ere in grado di s<unk> @ enti e il suo fatto .
2025-05-26 21:24:25,351 - INFO - joeynmt.training - Example #4
2025-05-26 21:24:25,351 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-26 21:24:25,351 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-26 21:24:25,351 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'volta', 'che', 'vi', 'mostro', 'un', 'po', "'", 'di', 'un', 'po', "'", 'di', 'un', 'po', "'", 'di', 'quello', 'che', 'è', 'successo', '.', '</s>']
2025-05-26 21:24:25,351 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-26 21:24:25,351 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-26 21:24:25,351 - INFO - joeynmt.training - 	Hypothesis: La prossima volta che vi mostro un po ' di un po ' di un po ' di quello che è successo .
2025-05-26 21:24:54,507 - INFO - joeynmt.training - Epoch   2, Step:     8100, Batch Loss:     1.926619, Batch Acc: 0.505459, Tokens per Sec:     2527, Lr: 0.000300
2025-05-26 21:26:38,485 - INFO - joeynmt.training - Epoch   2, Step:     8200, Batch Loss:     2.014471, Batch Acc: 0.505144, Tokens per Sec:      729, Lr: 0.000300
2025-05-26 21:27:06,370 - INFO - joeynmt.training - Epoch   2, Step:     8300, Batch Loss:     1.973758, Batch Acc: 0.507193, Tokens per Sec:     2627, Lr: 0.000300
2025-05-26 21:27:34,195 - INFO - joeynmt.training - Epoch   2, Step:     8400, Batch Loss:     2.053184, Batch Acc: 0.509343, Tokens per Sec:     2627, Lr: 0.000300
2025-05-26 21:28:01,647 - INFO - joeynmt.training - Epoch   2, Step:     8500, Batch Loss:     2.175636, Batch Acc: 0.513664, Tokens per Sec:     2623, Lr: 0.000300
2025-05-26 21:28:01,647 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-26 21:31:01,434 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.90, ppl:   6.71, acc:   0.54, generation: 179.7793[sec], evaluation: 0.0000[sec]
2025-05-26 21:31:01,437 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-26 21:31:01,567 - INFO - joeynmt.helpers - delete models/bpe_8k/6000.ckpt
2025-05-26 21:31:01,597 - INFO - joeynmt.training - Example #0
2025-05-26 21:31:01,597 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-26 21:31:01,597 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-26 21:31:01,597 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', '@', 'anno', 'ho', 'mostrato', 'queste', 'due', 'due', 'anni', ',', 'ho', 'mostrato', 'che', 'la', 'ri@@', '<unk>', '@', 'chiede', 'che', 'la', 'maggior', 'parte', 'del', 'cal@@', '<unk>', '@', 'do', ',', 'che', 'per', 'cento', 'di', 'tre', 'anni', 'ha', 'la', 'maggior', 'parte', 'del', 'mezzo', 'di', 'tre', 'milioni', 'di', 'anni', 'ha', 'stato', 'stato', 'stato', 'stato', 'stato', 'stato', 'stato', "l'", 'anno', ',', 'ha', 'stato', 'stato', 'stato', 'il', '40', '%', 'di', '40', '%', '.', '</s>']
2025-05-26 21:31:01,597 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-26 21:31:01,597 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-26 21:31:01,597 - INFO - joeynmt.training - 	Hypothesis: L<unk> @ anno ho mostrato queste due due anni , ho mostrato che la ri<unk> @ chiede che la maggior parte del cal<unk> @ do , che per cento di tre anni ha la maggior parte del mezzo di tre milioni di anni ha stato stato stato stato stato stato stato l' anno , ha stato stato stato il 40 % di 40 % .
2025-05-26 21:31:01,597 - INFO - joeynmt.training - Example #1
2025-05-26 21:31:01,597 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-26 21:31:01,597 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-26 21:31:01,597 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'problema', 'di', 'questo', 'problema', 'perché', 'non', 'è', 'il', 'problema', 'perché', 'non', 'fa', 'il', 'problema', 'non', 'è', 'il', 'problema', 'perché', 'non', 'fa', 'il', 'tr@@', '<unk>', '@', 'etto', '.', '</s>']
2025-05-26 21:31:01,597 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-26 21:31:01,597 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-26 21:31:01,597 - INFO - joeynmt.training - 	Hypothesis: Ma questo problema di questo problema perché non è il problema perché non fa il problema non è il problema perché non fa il tr<unk> @ etto .
2025-05-26 21:31:01,597 - INFO - joeynmt.training - Example #2
2025-05-26 21:31:01,598 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-26 21:31:01,598 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-26 21:31:01,598 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'sistema', 'di', 'al@@', '<unk>', '@', 'ico', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'sistema', ',', 'il', 'sistema', 'del', 'sistema', 'globale', '.', '</s>']
2025-05-26 21:31:01,598 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-26 21:31:01,598 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-26 21:31:01,598 - INFO - joeynmt.training - 	Hypothesis: Il sistema di al<unk> @ ico è , in un senso , il sistema , il sistema del sistema globale .
2025-05-26 21:31:01,598 - INFO - joeynmt.training - Example #3
2025-05-26 21:31:01,598 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-26 21:31:01,598 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-26 21:31:01,598 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['È', 'un', 'ar@@', '<unk>', '@', 'co', 'in', 'in', 'tutto', 'il', 'suo', 'suo', 'suo', 'suo', 'centro', '.', '</s>']
2025-05-26 21:31:01,598 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-26 21:31:01,598 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-26 21:31:01,598 - INFO - joeynmt.training - 	Hypothesis: È un ar<unk> @ co in in tutto il suo suo suo suo centro .
2025-05-26 21:31:01,598 - INFO - joeynmt.training - Example #4
2025-05-26 21:31:01,598 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-26 21:31:01,598 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-26 21:31:01,598 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'volta', 'che', 'vi', 'mostro', 'un', 'po', "'", 'di', 'un', 'po', "'", 'di', 'un', 'cal@@', '<unk>', '@', 'do', 'di', 'quello', 'che', 'è', 'successo', '.', '</s>']
2025-05-26 21:31:01,598 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-26 21:31:01,598 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-26 21:31:01,598 - INFO - joeynmt.training - 	Hypothesis: La prossima volta che vi mostro un po ' di un po ' di un cal<unk> @ do di quello che è successo .
2025-05-26 21:31:29,701 - INFO - joeynmt.training - Epoch   2, Step:     8600, Batch Loss:     2.049395, Batch Acc: 0.509335, Tokens per Sec:     2545, Lr: 0.000300
2025-05-26 21:31:58,290 - INFO - joeynmt.training - Epoch   2, Step:     8700, Batch Loss:     2.023715, Batch Acc: 0.514195, Tokens per Sec:     2654, Lr: 0.000300
2025-05-26 21:32:26,338 - INFO - joeynmt.training - Epoch   2, Step:     8800, Batch Loss:     1.954112, Batch Acc: 0.512687, Tokens per Sec:     2570, Lr: 0.000300
2025-05-26 21:32:53,856 - INFO - joeynmt.training - Epoch   2, Step:     8900, Batch Loss:     1.924906, Batch Acc: 0.514909, Tokens per Sec:     2631, Lr: 0.000300
2025-05-26 21:33:21,071 - INFO - joeynmt.training - Epoch   2, Step:     9000, Batch Loss:     1.863045, Batch Acc: 0.518812, Tokens per Sec:     2696, Lr: 0.000300
2025-05-26 21:33:21,072 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-26 21:36:28,415 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.86, ppl:   6.41, acc:   0.55, generation: 187.3359[sec], evaluation: 0.0000[sec]
2025-05-26 21:36:28,418 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-26 21:36:28,566 - INFO - joeynmt.helpers - delete models/bpe_8k/6500.ckpt
2025-05-26 21:36:28,591 - INFO - joeynmt.training - Example #0
2025-05-26 21:36:28,591 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-26 21:36:28,591 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-26 21:36:28,591 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'ho', 'mostrato', 'queste', 'due', 'due', 'due', 'due', 'anni', 'ha', 'mostrato', 'queste', 'due', 'due', 'anni', 'ha', 'dimostr@@', '<unk>', '@', 'ato', 'che', 'la', 'maggior', 'parte', 'del', 'cal@@', '<unk>', '@', 'o', ',', 'che', 'per', 'la', 'maggior', 'parte', 'del', '2@@', '<unk>', '@', '8', 'milioni', 'di', 'anni', 'ha', 'fatto', 'la', 'maggior', 'parte', 'del', '2@@', '<unk>', '@', '8', '%', ',', 'ha', 'fatto', 'più', 'di', '40', '%', '.', '</s>']
2025-05-26 21:36:28,591 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-26 21:36:28,591 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-26 21:36:28,591 - INFO - joeynmt.training - 	Hypothesis: L' anno ho mostrato queste due due due due anni ha mostrato queste due due anni ha dimostr<unk> @ ato che la maggior parte del cal<unk> @ o , che per la maggior parte del 2<unk> @ 8 milioni di anni ha fatto la maggior parte del 2<unk> @ 8 % , ha fatto più di 40 % .
2025-05-26 21:36:28,591 - INFO - joeynmt.training - Example #1
2025-05-26 21:36:28,591 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-26 21:36:28,591 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-26 21:36:28,591 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questa', 'cosa', 'che', 'la', 'questione', 'del', 'problema', 'perché', 'non', 'è', 'la', 'non', 'vi', 'vedete', 'il', 'problema', 'perché', 'non', 'vi', 'vedete', 'la', 'prima', 'non', 'vedete', 'il', 'par@@', '<unk>', '@', 'ore', '.', '</s>']
2025-05-26 21:36:28,592 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-26 21:36:28,592 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-26 21:36:28,592 - INFO - joeynmt.training - 	Hypothesis: Ma questa cosa che la questione del problema perché non è la non vi vedete il problema perché non vi vedete la prima non vedete il par<unk> @ ore .
2025-05-26 21:36:28,592 - INFO - joeynmt.training - Example #2
2025-05-26 21:36:28,592 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-26 21:36:28,592 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-26 21:36:28,592 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'cal@@', '<unk>', '@', 'ore', 'è', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'vero', ',', 'il', 'sistema', 'del', 'sistema', 'globale', '.', '</s>']
2025-05-26 21:36:28,592 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-26 21:36:28,592 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-26 21:36:28,592 - INFO - joeynmt.training - 	Hypothesis: Il cal<unk> @ ore è è , in un senso , il vero , il sistema del sistema globale .
2025-05-26 21:36:28,592 - INFO - joeynmt.training - Example #3
2025-05-26 21:36:28,592 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-26 21:36:28,592 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-26 21:36:28,592 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['È', 'un', 'po', "'", 'di', 'v@@', '<unk>', '@', 'ino', 'e', 'si', 'ri@@', '<unk>', '@', 'p@@', '<unk>', '@', 'ente', '.', '</s>']
2025-05-26 21:36:28,592 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-26 21:36:28,592 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-26 21:36:28,592 - INFO - joeynmt.training - 	Hypothesis: È un po ' di v<unk> @ ino e si ri<unk> @ p<unk> @ ente .
2025-05-26 21:36:28,592 - INFO - joeynmt.training - Example #4
2025-05-26 21:36:28,592 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-26 21:36:28,592 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-26 21:36:28,592 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'prossima', 'immagine', 'che', 'vi', 'mostrerò', 'un', 'po', "'", 'di', 'un', 'po', "'", 'di', 'ciò', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-26 21:36:28,592 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-26 21:36:28,592 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-26 21:36:28,592 - INFO - joeynmt.training - 	Hypothesis: La prossima prossima immagine che vi mostrerò un po ' di un po ' di ciò che è successo negli ultimi 25 anni .
2025-05-26 21:36:56,901 - INFO - joeynmt.training - Epoch   2, Step:     9100, Batch Loss:     1.972189, Batch Acc: 0.514188, Tokens per Sec:     2595, Lr: 0.000300
2025-05-26 21:37:25,456 - INFO - joeynmt.training - Epoch   2, Step:     9200, Batch Loss:     1.869658, Batch Acc: 0.520036, Tokens per Sec:     2601, Lr: 0.000300
2025-05-26 21:37:53,090 - INFO - joeynmt.training - Epoch   2, Step:     9300, Batch Loss:     2.050935, Batch Acc: 0.520595, Tokens per Sec:     2724, Lr: 0.000300
2025-05-26 21:38:20,642 - INFO - joeynmt.training - Epoch   2, Step:     9400, Batch Loss:     1.940038, Batch Acc: 0.523349, Tokens per Sec:     2618, Lr: 0.000300
2025-05-26 21:38:48,275 - INFO - joeynmt.training - Epoch   2, Step:     9500, Batch Loss:     1.922820, Batch Acc: 0.523224, Tokens per Sec:     2642, Lr: 0.000300
2025-05-26 21:38:48,276 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-26 21:41:40,699 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.83, ppl:   6.22, acc:   0.56, generation: 172.4156[sec], evaluation: 0.0000[sec]
2025-05-26 21:41:40,700 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-26 21:41:40,822 - INFO - joeynmt.helpers - delete models/bpe_8k/7000.ckpt
2025-05-26 21:41:40,846 - INFO - joeynmt.training - Example #0
2025-05-26 21:41:40,847 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-26 21:41:40,847 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-26 21:41:40,847 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'ho', 'mostrato', 'questi', 'due', 'due', 'anni', 'ho', 'mostrato', 'questi', 'due', 'volte', 'che', 'questa', 'dimostr@@', '<unk>', '@', 'azione', 'che', "l'", 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ina', ',', 'che', 'per', 'la', 'maggior', 'parte', 'delle', 'tre', 'milioni', 'di', 'anni', 'ha', 'le', 'dimensioni', 'del', '4@@', '<unk>', '@', '8', 'milioni', 'di', 'anni', 'ha', 'le', 'dimensioni', 'del', '4@@', '<unk>', '@', '8', '%', '.', '</s>']
2025-05-26 21:41:40,847 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-26 21:41:40,847 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-26 21:41:40,847 - INFO - joeynmt.training - 	Hypothesis: L' anno ho mostrato questi due due anni ho mostrato questi due volte che questa dimostr<unk> @ azione che l' ar<unk> @ c<unk> @ ina , che per la maggior parte delle tre milioni di anni ha le dimensioni del 4<unk> @ 8 milioni di anni ha le dimensioni del 4<unk> @ 8 % .
2025-05-26 21:41:40,847 - INFO - joeynmt.training - Example #1
2025-05-26 21:41:40,847 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-26 21:41:40,847 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-26 21:41:40,847 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'tipo', 'di', 'questo', 'problema', 'perché', 'non', 'vedete', 'il', 'problema', 'non', 'vedete', 'il', 'problema', 'non', 'vedete', 'il', 'problema', 'non', 'vedete', 'il', 'problema', 'non', 'vedete', 'il', 'punto', 'di', 'ghiaccio', '.', '</s>']
2025-05-26 21:41:40,847 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-26 21:41:40,847 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-26 21:41:40,847 - INFO - joeynmt.training - 	Hypothesis: Ma questo tipo di questo problema perché non vedete il problema non vedete il problema non vedete il problema non vedete il problema non vedete il punto di ghiaccio .
2025-05-26 21:41:40,847 - INFO - joeynmt.training - Example #2
2025-05-26 21:41:40,847 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-26 21:41:40,847 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-26 21:41:40,847 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'è', 'il', 'ghiaccio', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'globale', 'globale', 'globale', '.', '</s>']
2025-05-26 21:41:40,848 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-26 21:41:40,848 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-26 21:41:40,848 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio è il ghiaccio è , in un senso , il senso , il cuore del sistema globale globale globale .
2025-05-26 21:41:40,848 - INFO - joeynmt.training - Example #3
2025-05-26 21:41:40,848 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-26 21:41:40,848 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-26 21:41:40,848 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'tratta', 'di', 'es@@', '<unk>', '@', 'au@@', '<unk>', '@', 'g@@', '<unk>', '@', 'ono', 'e', 'ri@@', '<unk>', '@', 'port@@', '<unk>', '@', 'ano', 'in', 'estate', '.', '</s>']
2025-05-26 21:41:40,848 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-26 21:41:40,848 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-26 21:41:40,848 - INFO - joeynmt.training - 	Hypothesis: Si tratta di es<unk> @ au<unk> @ g<unk> @ ono e ri<unk> @ port<unk> @ ano in estate .
2025-05-26 21:41:40,848 - INFO - joeynmt.training - Example #4
2025-05-26 21:41:40,848 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-26 21:41:40,848 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-26 21:41:40,848 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'ultima', 'ultima', 'ultima', 'ultima', 'ultima', 'ultima', 'ultima', 'ultima', 'ultima', 'cosa', 'è', 'successo', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-26 21:41:40,848 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-26 21:41:40,848 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-26 21:41:40,848 - INFO - joeynmt.training - 	Hypothesis: La prossima ultima ultima ultima ultima ultima ultima ultima ultima ultima cosa è successo di quello che è successo negli ultimi 25 anni .
2025-05-26 21:42:08,917 - INFO - joeynmt.training - Epoch   2, Step:     9600, Batch Loss:     1.980121, Batch Acc: 0.524928, Tokens per Sec:     2583, Lr: 0.000300
2025-05-26 21:42:36,374 - INFO - joeynmt.training - Epoch   2, Step:     9700, Batch Loss:     1.827018, Batch Acc: 0.528163, Tokens per Sec:     2705, Lr: 0.000300
2025-05-26 21:43:04,002 - INFO - joeynmt.training - Epoch   2, Step:     9800, Batch Loss:     2.023247, Batch Acc: 0.528590, Tokens per Sec:     2696, Lr: 0.000300
2025-05-26 21:43:31,908 - INFO - joeynmt.training - Epoch   2, Step:     9900, Batch Loss:     1.832989, Batch Acc: 0.529843, Tokens per Sec:     2651, Lr: 0.000300
2025-05-26 21:43:54,671 - INFO - joeynmt.training - Epoch   2: total training loss 10426.79
2025-05-26 21:43:54,671 - INFO - joeynmt.training - EPOCH 3
2025-05-26 21:43:59,840 - INFO - joeynmt.training - Epoch   3, Step:    10000, Batch Loss:     1.956757, Batch Acc: 0.540478, Tokens per Sec:     2354, Lr: 0.000300
2025-05-26 21:43:59,840 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-26 21:46:40,443 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.80, ppl:   6.03, acc:   0.56, generation: 160.5951[sec], evaluation: 0.0000[sec]
2025-05-26 21:46:40,444 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-26 21:46:40,573 - INFO - joeynmt.helpers - delete models/bpe_8k/7500.ckpt
2025-05-26 21:46:40,596 - INFO - joeynmt.training - Example #0
2025-05-26 21:46:40,597 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-26 21:46:40,597 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-26 21:46:40,597 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'ho', 'mostrato', 'queste', 'due', 'due', 'f@@', '<unk>', '@', 'ine', 'così', 'che', 'la', 'dimostr@@', '<unk>', '@', 'azione', 'che', 'la', 'linea', 'del', 'cal@@', '<unk>', '@', 'do', ',', 'che', 'la', 'maggior', 'parte', 'del', 'cal@@', '<unk>', '@', 'colo', ',', 'che', 'per', 'la', 'maggior', 'parte', 'del', '4@@', '<unk>', '@', '8', 'milioni', 'di', 'anni', 'ha', 'stata', 'la', 'dimensione', 'del', '4@@', '<unk>', '@', '8', '%', ',', 'ha', 'stata', 'la', 'maggior', 'parte', 'del', '40', '%', '.', '</s>']
2025-05-26 21:46:40,597 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-26 21:46:40,597 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-26 21:46:40,597 - INFO - joeynmt.training - 	Hypothesis: L' anno ho mostrato queste due due f<unk> @ ine così che la dimostr<unk> @ azione che la linea del cal<unk> @ do , che la maggior parte del cal<unk> @ colo , che per la maggior parte del 4<unk> @ 8 milioni di anni ha stata la dimensione del 4<unk> @ 8 % , ha stata la maggior parte del 40 % .
2025-05-26 21:46:40,597 - INFO - joeynmt.training - Example #1
2025-05-26 21:46:40,597 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-26 21:46:40,597 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-26 21:46:40,597 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questa', 'cosa', 'la', 'questione', 'di', 'questo', 'problema', 'perché', 'non', 'è', 'che', 'non', 'vi', 'mostr@@', '<unk>', '@', 'ano', 'la', 'questione', 'di', 'questo', 'problema', 'perché', 'non', 'vi', 'mostr@@', '<unk>', '@', 'o', '.', '</s>']
2025-05-26 21:46:40,597 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-26 21:46:40,597 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-26 21:46:40,597 - INFO - joeynmt.training - 	Hypothesis: Ma questa cosa la questione di questo problema perché non è che non vi mostr<unk> @ ano la questione di questo problema perché non vi mostr<unk> @ o .
2025-05-26 21:46:40,597 - INFO - joeynmt.training - Example #2
2025-05-26 21:46:40,597 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-26 21:46:40,597 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-26 21:46:40,597 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'p@@', '<unk>', '@', 'atto', 'è', ',', 'il', 'cal@@', '<unk>', '@', 'do', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'del', 'sistema', 'globale', '.', '</s>']
2025-05-26 21:46:40,597 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-26 21:46:40,597 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-26 21:46:40,598 - INFO - joeynmt.training - 	Hypothesis: Il p<unk> @ atto è , il cal<unk> @ do , in un senso , il cuore del sistema del sistema globale .
2025-05-26 21:46:40,598 - INFO - joeynmt.training - Example #3
2025-05-26 21:46:40,598 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-26 21:46:40,598 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-26 21:46:40,598 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'tratta', 'di', 's@@', '<unk>', '@', 'compar@@', '<unk>', '@', 'to', 'in', 'piedi', 'e', 'di@@', '<unk>', '@', 'zione', 'in', 'estate', '.', '</s>']
2025-05-26 21:46:40,598 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-26 21:46:40,598 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-26 21:46:40,598 - INFO - joeynmt.training - 	Hypothesis: Si tratta di s<unk> @ compar<unk> @ to in piedi e di<unk> @ zione in estate .
2025-05-26 21:46:40,598 - INFO - joeynmt.training - Example #4
2025-05-26 21:46:40,598 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-26 21:46:40,598 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-26 21:46:40,598 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'ultima', 'ultima', 'ultima', 'ultima', 'ultima', 'ultima', 'ultima', 'ultima', 'ultima', 'volta', 'che', 'è', 'successo', 'nel', '25', 'anni', '.', '</s>']
2025-05-26 21:46:40,598 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-26 21:46:40,598 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-26 21:46:40,598 - INFO - joeynmt.training - 	Hypothesis: La prossima ultima ultima ultima ultima ultima ultima ultima ultima ultima volta che è successo nel 25 anni .
2025-05-26 21:47:09,358 - INFO - joeynmt.training - Epoch   3, Step:    10100, Batch Loss:     1.873549, Batch Acc: 0.535636, Tokens per Sec:     2571, Lr: 0.000300
2025-05-26 21:47:37,886 - INFO - joeynmt.training - Epoch   3, Step:    10200, Batch Loss:     1.836774, Batch Acc: 0.540289, Tokens per Sec:     2564, Lr: 0.000300
2025-05-26 21:48:05,473 - INFO - joeynmt.training - Epoch   3, Step:    10300, Batch Loss:     1.995364, Batch Acc: 0.537127, Tokens per Sec:     2622, Lr: 0.000300
2025-05-26 21:48:32,476 - INFO - joeynmt.training - Epoch   3, Step:    10400, Batch Loss:     1.926976, Batch Acc: 0.539809, Tokens per Sec:     2697, Lr: 0.000300
2025-05-26 21:49:00,307 - INFO - joeynmt.training - Epoch   3, Step:    10500, Batch Loss:     1.743182, Batch Acc: 0.541572, Tokens per Sec:     2650, Lr: 0.000300
2025-05-26 21:49:00,307 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-26 21:51:54,176 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.76, ppl:   5.80, acc:   0.57, generation: 173.8609[sec], evaluation: 0.0000[sec]
2025-05-26 21:51:54,179 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-26 21:51:54,308 - INFO - joeynmt.helpers - delete models/bpe_8k/8000.ckpt
2025-05-26 21:51:54,317 - INFO - joeynmt.training - Example #0
2025-05-26 21:51:54,317 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-26 21:51:54,317 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-26 21:51:54,317 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'ho', 'mostrato', 'queste', 'due', 'f@@', '<unk>', '@', 'ine', 'che', "l'", 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ina', 'che', "l'", 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ol@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'la', 'maggior', 'parte', 'delle', 'dimensioni', 'di', 'circa', '4@@', '<unk>', '@', '8', 'anni', 'ha', 'avuto', 'la', 'dimensione', 'delle', 'dimensioni', 'di', '4@@', '<unk>', '@', '8', ',', 'hanno', 'avuto', "l'", 'ultima', 'quantità', 'di', '4@@', '<unk>', '@', '8', ',', 'hanno', 'avuto', 'la', 'dimensione', 'di', '4@@', '<unk>', '@', '8', '%', '.', '</s>']
2025-05-26 21:51:54,317 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-26 21:51:54,317 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-26 21:51:54,317 - INFO - joeynmt.training - 	Hypothesis: L' anno ho mostrato queste due f<unk> @ ine che l' ar<unk> @ c<unk> @ ina che l' ar<unk> @ c<unk> @ ol<unk> @ ico , che per la maggior parte delle dimensioni di circa 4<unk> @ 8 anni ha avuto la dimensione delle dimensioni di 4<unk> @ 8 , hanno avuto l' ultima quantità di 4<unk> @ 8 , hanno avuto la dimensione di 4<unk> @ 8 % .
2025-05-26 21:51:54,317 - INFO - joeynmt.training - Example #1
2025-05-26 21:51:54,317 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-26 21:51:54,317 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-26 21:51:54,317 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questa', 'cosa', 'che', 'la', 'nostra', 'parte', 'del', 'problema', ',', 'perché', 'non', 'mostra', 'la', 'linea', 'di', 'questo', 'problema', 'perché', 'non', 'mostra', 'la', 'linea', '.', '</s>']
2025-05-26 21:51:54,317 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-26 21:51:54,317 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-26 21:51:54,317 - INFO - joeynmt.training - 	Hypothesis: Ma questa cosa che la nostra parte del problema , perché non mostra la linea di questo problema perché non mostra la linea .
2025-05-26 21:51:54,317 - INFO - joeynmt.training - Example #2
2025-05-26 21:51:54,317 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-26 21:51:54,317 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-26 21:51:54,317 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 't@@', '<unk>', '@', 'ico', 'è', 'il', 'ghiaccio', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', ',', 'il', 'sistema', 'del', 'clima', 'globale', '.', '</s>']
2025-05-26 21:51:54,318 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-26 21:51:54,318 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-26 21:51:54,318 - INFO - joeynmt.training - 	Hypothesis: Il t<unk> @ ico è il ghiaccio , in un senso , il cuore , il sistema del clima globale .
2025-05-26 21:51:54,318 - INFO - joeynmt.training - Example #3
2025-05-26 21:51:54,318 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-26 21:51:54,318 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-26 21:51:54,318 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'tratta', 'di', 'es@@', '<unk>', '@', 'perto', 'in', 'piedi', 'e', 'il', 'contr@@', '<unk>', '@', 'atto', 'in', 'estate', '.', '</s>']
2025-05-26 21:51:54,318 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-26 21:51:54,318 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-26 21:51:54,318 - INFO - joeynmt.training - 	Hypothesis: Si tratta di es<unk> @ perto in piedi e il contr<unk> @ atto in estate .
2025-05-26 21:51:54,318 - INFO - joeynmt.training - Example #4
2025-05-26 21:51:54,318 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-26 21:51:54,318 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-26 21:51:54,318 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'es@@', '<unk>', '@', 'pia', 'che', 'vi', 'vi', 'mostro', 'un', 'po', "'", 'di', 'più', 'rapidamente', ',', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-26 21:51:54,318 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-26 21:51:54,318 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-26 21:51:54,318 - INFO - joeynmt.training - 	Hypothesis: La prossima es<unk> @ pia che vi vi mostro un po ' di più rapidamente , di quello che è successo negli ultimi 25 anni .
2025-05-26 21:52:22,604 - INFO - joeynmt.training - Epoch   3, Step:    10600, Batch Loss:     1.961511, Batch Acc: 0.539451, Tokens per Sec:     2488, Lr: 0.000300
2025-05-26 21:52:50,644 - INFO - joeynmt.training - Epoch   3, Step:    10700, Batch Loss:     1.976511, Batch Acc: 0.542982, Tokens per Sec:     2724, Lr: 0.000300
2025-05-26 21:53:19,050 - INFO - joeynmt.training - Epoch   3, Step:    10800, Batch Loss:     1.828942, Batch Acc: 0.543979, Tokens per Sec:     2654, Lr: 0.000300
2025-05-26 21:53:46,778 - INFO - joeynmt.training - Epoch   3, Step:    10900, Batch Loss:     1.773330, Batch Acc: 0.544898, Tokens per Sec:     2671, Lr: 0.000300
2025-05-26 21:54:14,320 - INFO - joeynmt.training - Epoch   3, Step:    11000, Batch Loss:     1.817604, Batch Acc: 0.546202, Tokens per Sec:     2627, Lr: 0.000300
2025-05-26 21:54:14,320 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-26 21:56:43,554 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.73, ppl:   5.65, acc:   0.58, generation: 149.2270[sec], evaluation: 0.0000[sec]
2025-05-26 21:56:43,556 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-26 21:56:43,675 - INFO - joeynmt.helpers - delete models/bpe_8k/8500.ckpt
2025-05-26 21:56:43,680 - INFO - joeynmt.training - Example #0
2025-05-26 21:56:43,680 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-26 21:56:43,680 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-26 21:56:43,680 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'ho', 'mostrato', 'queste', 'due', 'f@@', '<unk>', '@', 'ine', 'che', 'hanno', 'mostrato', 'queste', 'due', 'f@@', '<unk>', '@', 'asi', 'che', 'il', 'ghiaccio', 'di', 'due', 'milioni', 'di', 'anni', 'ha', 'fatto', 'che', 'il', 'più', 'di', 'tre', 'milioni', 'di', 'anni', 'ha', 'fatto', 'che', 'il', 'più', 'più', 'di', 'tre', 'anni', 'ha', 'le', 'dimensioni', 'di', 'tre', 'anni', 'ha', 'le', 'dimensioni', 'di', 'più', 'più', 'più', 'grande', ',', 'ha', 'le', 'sue', 'stati', 'stati', ',', 'hanno', 'preso', 'da', '40', '%', '.', '</s>']
2025-05-26 21:56:43,680 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-26 21:56:43,680 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-26 21:56:43,680 - INFO - joeynmt.training - 	Hypothesis: L' anno ho mostrato queste due f<unk> @ ine che hanno mostrato queste due f<unk> @ asi che il ghiaccio di due milioni di anni ha fatto che il più di tre milioni di anni ha fatto che il più più di tre anni ha le dimensioni di tre anni ha le dimensioni di più più più grande , ha le sue stati stati , hanno preso da 40 % .
2025-05-26 21:56:43,680 - INFO - joeynmt.training - Example #1
2025-05-26 21:56:43,680 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-26 21:56:43,680 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-26 21:56:43,680 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questa', 'cosa', 'fa', 'la', 'questione', 'di', 'questo', 'problema', ',', 'perché', 'non', 'è', 'il', 'problema', 'perché', 'non', 'è', 'il', 'concetto', 'che', 'non', 'mostra', 'il', 't@@', '<unk>', '@', 'etto', 'della', 'ghiaccio', '.', '</s>']
2025-05-26 21:56:43,681 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-26 21:56:43,681 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-26 21:56:43,681 - INFO - joeynmt.training - 	Hypothesis: Ma questa cosa fa la questione di questo problema , perché non è il problema perché non è il concetto che non mostra il t<unk> @ etto della ghiaccio .
2025-05-26 21:56:43,681 - INFO - joeynmt.training - Example #2
2025-05-26 21:56:43,681 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-26 21:56:43,681 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-26 21:56:43,681 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'è', 'il', 'ghiaccio', 'di', 'p@@', '<unk>', '@', 'etto', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'sistema', 'di', 'il', 'cambiamento', 'climatico', 'globale', '.', '</s>']
2025-05-26 21:56:43,681 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-26 21:56:43,681 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-26 21:56:43,681 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio è il ghiaccio di p<unk> @ etto è , in un senso , il sistema di il cambiamento climatico globale .
2025-05-26 21:56:43,681 - INFO - joeynmt.training - Example #3
2025-05-26 21:56:43,681 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-26 21:56:43,681 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-26 21:56:43,681 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', "'", 'es@@', '<unk>', '@', 'amin@@', '<unk>', '@', 'ali', 'in', 'piedi', 'e', 'il', 'p@@', '<unk>', '@', 'etto', 'e', 'il', 'p@@', '<unk>', '@', 'etto', '.', '</s>']
2025-05-26 21:56:43,681 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-26 21:56:43,681 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-26 21:56:43,681 - INFO - joeynmt.training - 	Hypothesis: E ' es<unk> @ amin<unk> @ ali in piedi e il p<unk> @ etto e il p<unk> @ etto .
2025-05-26 21:56:43,681 - INFO - joeynmt.training - Example #4
2025-05-26 21:56:43,682 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-26 21:56:43,682 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-26 21:56:43,682 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'os@@', '<unk>', '@', 'os@@', '<unk>', '@', 'os@@', '<unk>', '@', 'itiva', 'che', 'vi', 'mostrerò', 'un', 'po', "'", 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-26 21:56:43,682 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-26 21:56:43,682 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-26 21:56:43,682 - INFO - joeynmt.training - 	Hypothesis: La prossima os<unk> @ os<unk> @ os<unk> @ itiva che vi mostrerò un po ' di quello che è successo negli ultimi 25 anni .
2025-05-26 21:57:11,898 - INFO - joeynmt.training - Epoch   3, Step:    11100, Batch Loss:     1.775788, Batch Acc: 0.552237, Tokens per Sec:     2601, Lr: 0.000300
2025-05-26 21:57:39,903 - INFO - joeynmt.training - Epoch   3, Step:    11200, Batch Loss:     1.874916, Batch Acc: 0.545520, Tokens per Sec:     2571, Lr: 0.000300
2025-05-26 21:58:08,923 - INFO - joeynmt.training - Epoch   3, Step:    11300, Batch Loss:     1.844170, Batch Acc: 0.547009, Tokens per Sec:     2465, Lr: 0.000300
2025-05-26 21:58:36,768 - INFO - joeynmt.training - Epoch   3, Step:    11400, Batch Loss:     1.694982, Batch Acc: 0.549307, Tokens per Sec:     2586, Lr: 0.000300
2025-05-26 21:59:04,472 - INFO - joeynmt.training - Epoch   3, Step:    11500, Batch Loss:     1.734194, Batch Acc: 0.552213, Tokens per Sec:     2690, Lr: 0.000300
2025-05-26 21:59:04,472 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-26 22:01:33,487 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.70, ppl:   5.50, acc:   0.58, generation: 149.0072[sec], evaluation: 0.0000[sec]
2025-05-26 22:01:33,490 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-26 22:01:33,607 - INFO - joeynmt.helpers - delete models/bpe_8k/9000.ckpt
2025-05-26 22:01:33,611 - INFO - joeynmt.training - Example #0
2025-05-26 22:01:33,611 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-26 22:01:33,611 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-26 22:01:33,611 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'ho', 'mostrato', 'queste', 'due', 'due', 'f@@', '<unk>', '@', 'ine', 'così', 'che', 'la', 'dimostr@@', '<unk>', '@', 'azione', 'che', 'il', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', ',', 'che', 'il', 'più', 'di', 'più', 'di', 'più', 'alto', ',', 'che', 'per', 'la', 'maggior', 'parte', 'delle', 'due', 'anni', 'è', 'stata', 'la', 'dimensione', 'delle', 'parti', 'più', 'alto', ',', 'hanno', 'avuto', 'la', 'dimensione', 'delle', 'più', 'alto', ',', 'ha', 'fatto', 'il', '40', '%', '.', '</s>']
2025-05-26 22:01:33,612 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-26 22:01:33,612 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-26 22:01:33,612 - INFO - joeynmt.training - 	Hypothesis: L' anno ho mostrato queste due due f<unk> @ ine così che la dimostr<unk> @ azione che il ar<unk> @ c<unk> @ ico , che il più di più di più alto , che per la maggior parte delle due anni è stata la dimensione delle parti più alto , hanno avuto la dimensione delle più alto , ha fatto il 40 % .
2025-05-26 22:01:33,612 - INFO - joeynmt.training - Example #1
2025-05-26 22:01:33,612 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-26 22:01:33,612 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-26 22:01:33,612 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'è', 'la', 'vera', 'vera', 'vera', 'vera', ',', 'perché', 'non', 'si', 'mostra', 'la', 'prossima', 'parte', 'non', 'si', 'mostra', 'la', 'prossima', 'parte', 'del', 'ghiaccio', '.', '</s>']
2025-05-26 22:01:33,612 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-26 22:01:33,612 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-26 22:01:33,612 - INFO - joeynmt.training - 	Hypothesis: Ma questo è la vera vera vera vera , perché non si mostra la prossima parte non si mostra la prossima parte del ghiaccio .
2025-05-26 22:01:33,612 - INFO - joeynmt.training - Example #2
2025-05-26 22:01:33,612 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-26 22:01:33,612 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-26 22:01:33,612 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'c@@', '<unk>', '@', 'ico', 'è', 'il', 'cuore', 'del', 'sistema', 'globale', '.', '</s>']
2025-05-26 22:01:33,612 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-26 22:01:33,612 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-26 22:01:33,612 - INFO - joeynmt.training - 	Hypothesis: Il c<unk> @ ico è il cuore del sistema globale .
2025-05-26 22:01:33,613 - INFO - joeynmt.training - Example #3
2025-05-26 22:01:33,613 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-26 22:01:33,613 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-26 22:01:33,613 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['È', 'es@@', '<unk>', '@', 'amin@@', '<unk>', '@', 'ato', 'in', 'piedi', 'e', 'il', 'p@@', '<unk>', '@', 'etto', 'e', 'si', 'ri@@', '<unk>', '@', 'tiene', 'in', 'estate', '.', '</s>']
2025-05-26 22:01:33,613 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-26 22:01:33,613 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-26 22:01:33,613 - INFO - joeynmt.training - 	Hypothesis: È es<unk> @ amin<unk> @ ato in piedi e il p<unk> @ etto e si ri<unk> @ tiene in estate .
2025-05-26 22:01:33,613 - INFO - joeynmt.training - Example #4
2025-05-26 22:01:33,613 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-26 22:01:33,613 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-26 22:01:33,613 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'os@@', '<unk>', '@', 'itiva', 'che', 'vi', 'vi', 'vi', 'mostr@@', '<unk>', '@', 'ò', 'un', 'po', "'", 'di', 'più', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-26 22:01:33,613 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-26 22:01:33,613 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-26 22:01:33,613 - INFO - joeynmt.training - 	Hypothesis: La prossima os<unk> @ itiva che vi vi vi mostr<unk> @ ò un po ' di più di quello che è successo negli ultimi 25 anni .
2025-05-26 22:02:01,541 - INFO - joeynmt.training - Epoch   3, Step:    11600, Batch Loss:     1.897049, Batch Acc: 0.554872, Tokens per Sec:     2624, Lr: 0.000300
2025-05-26 22:02:30,091 - INFO - joeynmt.training - Epoch   3, Step:    11700, Batch Loss:     1.833897, Batch Acc: 0.549593, Tokens per Sec:     2563, Lr: 0.000300
2025-05-26 22:02:57,703 - INFO - joeynmt.training - Epoch   3, Step:    11800, Batch Loss:     1.797387, Batch Acc: 0.547720, Tokens per Sec:     2659, Lr: 0.000300
2025-05-26 22:03:25,195 - INFO - joeynmt.training - Epoch   3, Step:    11900, Batch Loss:     1.773910, Batch Acc: 0.554595, Tokens per Sec:     2761, Lr: 0.000300
2025-05-26 22:03:52,674 - INFO - joeynmt.training - Epoch   3, Step:    12000, Batch Loss:     1.861243, Batch Acc: 0.554146, Tokens per Sec:     2685, Lr: 0.000300
2025-05-26 22:03:52,674 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-26 22:08:26,051 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.68, ppl:   5.38, acc:   0.59, generation: 273.3695[sec], evaluation: 0.0000[sec]
2025-05-26 22:08:26,052 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-26 22:08:26,180 - INFO - joeynmt.helpers - delete models/bpe_8k/9500.ckpt
2025-05-26 22:08:26,185 - INFO - joeynmt.training - Example #0
2025-05-26 22:08:26,185 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-26 22:08:26,185 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-26 22:08:26,185 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'ho', 'mostrato', 'questi', 'due', 'due', 'ter@@', '<unk>', '@', 'mi', 'sono', 'due', 'ter@@', '<unk>', '@', 'mi', 'che', "l'", 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ant@@', '<unk>', '@', 'ico', ',', 'che', 'la', 'maggior', 'parte', 'dei', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'delle', 'tre', 'anni', 'è', 'stata', 'la', 'dimensione', 'delle', 'dimensioni', 'del', '40', '%', '.', '</s>']
2025-05-26 22:08:26,185 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-26 22:08:26,185 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-26 22:08:26,185 - INFO - joeynmt.training - 	Hypothesis: L' anno ho mostrato questi due due ter<unk> @ mi sono due ter<unk> @ mi che l' ar<unk> @ c<unk> @ ant<unk> @ ico , che la maggior parte dei tre milioni di anni è stata la dimensione delle tre anni è stata la dimensione delle dimensioni del 40 % .
2025-05-26 22:08:26,186 - INFO - joeynmt.training - Example #1
2025-05-26 22:08:26,186 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-26 22:08:26,186 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-26 22:08:26,186 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questa', 'cosa', 'sembra', 'la', 'vera', 'vera', 'vera', ',', 'perché', 'non', 'vi', 'mostr@@', '<unk>', '@', 'ano', 'il', 'grafico', 'non', 'mostra', 'la', 'la', 'letter@@', '<unk>', '@', 'a', '.', '</s>']
2025-05-26 22:08:26,186 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-26 22:08:26,186 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-26 22:08:26,186 - INFO - joeynmt.training - 	Hypothesis: Ma questa cosa sembra la vera vera vera , perché non vi mostr<unk> @ ano il grafico non mostra la la letter<unk> @ a .
2025-05-26 22:08:26,186 - INFO - joeynmt.training - Example #2
2025-05-26 22:08:26,186 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-26 22:08:26,186 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-26 22:08:26,186 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'globale', '.', '</s>']
2025-05-26 22:08:26,186 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-26 22:08:26,186 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-26 22:08:26,186 - INFO - joeynmt.training - 	Hypothesis: L' ar<unk> @ c<unk> @ ico è , in un senso , il cuore del sistema globale .
2025-05-26 22:08:26,186 - INFO - joeynmt.training - Example #3
2025-05-26 22:08:26,187 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-26 22:08:26,187 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-26 22:08:26,187 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'tratta', 'di', 'es@@', '<unk>', '@', 'amin@@', '<unk>', '@', 'ati', 'e', 'in', 'estate', '.', '</s>']
2025-05-26 22:08:26,187 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-26 22:08:26,187 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-26 22:08:26,187 - INFO - joeynmt.training - 	Hypothesis: Si tratta di es<unk> @ amin<unk> @ ati e in estate .
2025-05-26 22:08:26,187 - INFO - joeynmt.training - Example #4
2025-05-26 22:08:26,187 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-26 22:08:26,187 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-26 22:08:26,187 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'mente', 'vi', 'mostro', 'un', 'cal@@', '<unk>', '@', 'do', 'di', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-26 22:08:26,187 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-26 22:08:26,187 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-26 22:08:26,187 - INFO - joeynmt.training - 	Hypothesis: La prossima mente vi mostro un cal<unk> @ do di di quello che è successo negli ultimi 25 anni .
2025-05-26 22:08:54,027 - INFO - joeynmt.training - Epoch   3, Step:    12100, Batch Loss:     1.706358, Batch Acc: 0.558522, Tokens per Sec:     2611, Lr: 0.000300
2025-05-26 22:09:22,680 - INFO - joeynmt.training - Epoch   3, Step:    12200, Batch Loss:     1.940683, Batch Acc: 0.557293, Tokens per Sec:     2636, Lr: 0.000300
2025-05-26 22:09:50,548 - INFO - joeynmt.training - Epoch   3, Step:    12300, Batch Loss:     1.865886, Batch Acc: 0.559379, Tokens per Sec:     2687, Lr: 0.000300
2025-05-26 22:10:18,903 - INFO - joeynmt.training - Epoch   3, Step:    12400, Batch Loss:     1.735291, Batch Acc: 0.559691, Tokens per Sec:     2618, Lr: 0.000300
2025-05-26 22:10:46,371 - INFO - joeynmt.training - Epoch   3, Step:    12500, Batch Loss:     1.700275, Batch Acc: 0.560367, Tokens per Sec:     2648, Lr: 0.000300
2025-05-26 22:10:46,371 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-26 22:13:18,940 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.66, ppl:   5.27, acc:   0.59, generation: 152.5616[sec], evaluation: 0.0000[sec]
2025-05-26 22:13:18,941 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-26 22:13:19,055 - INFO - joeynmt.helpers - delete models/bpe_8k/10000.ckpt
2025-05-26 22:13:19,065 - INFO - joeynmt.training - Example #0
2025-05-26 22:13:19,065 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-26 22:13:19,065 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-26 22:13:19,065 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'ho', 'mostrato', 'queste', 'due', 'due', 'l@@', '<unk>', '@', 'ett@@', '<unk>', '@', 'enti', 'che', 'mostra', 'che', "l'", 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ant@@', '<unk>', '@', 'ico', ',', 'che', 'la', 'maggior', 'parte', 'delle', 'tre', 'anni', 'è', 'stata', 'la', 'dimensione', 'delle', 'tre', 'anni', 'è', 'stata', 'la', 'dimensione', 'delle', 'dimensioni', 'delle', 'dimensioni', 'del', '4@@', '<unk>', '@', '8', ',', 'ha', 'portato', 'la', 'dimensione', 'delle', 'dimensioni', 'del', '40', '%', '.', '</s>']
2025-05-26 22:13:19,065 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-26 22:13:19,065 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-26 22:13:19,065 - INFO - joeynmt.training - 	Hypothesis: L' anno ho mostrato queste due due l<unk> @ ett<unk> @ enti che mostra che l' ar<unk> @ c<unk> @ ant<unk> @ ico , che la maggior parte delle tre anni è stata la dimensione delle tre anni è stata la dimensione delle dimensioni delle dimensioni del 4<unk> @ 8 , ha portato la dimensione delle dimensioni del 40 % .
2025-05-26 22:13:19,066 - INFO - joeynmt.training - Example #1
2025-05-26 22:13:19,066 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-26 22:13:19,066 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-26 22:13:19,066 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questa', 'capi@@', '<unk>', '@', 'sce', 'la', 'vera', 'vera', 'vera', ',', 'perché', 'non', 'vi', 'mostra', 'la', 'prossima', 'ragione', 'per', 'cui', 'non', 'vi', 'vi', 'mostr@@', '<unk>', '@', 'ano', 'la', 'mag@@', '<unk>', '@', 'ia', 'del', 'ghiaccio', '.', '</s>']
2025-05-26 22:13:19,066 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-26 22:13:19,066 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-26 22:13:19,066 - INFO - joeynmt.training - 	Hypothesis: Ma questa capi<unk> @ sce la vera vera vera , perché non vi mostra la prossima ragione per cui non vi vi mostr<unk> @ ano la mag<unk> @ ia del ghiaccio .
2025-05-26 22:13:19,066 - INFO - joeynmt.training - Example #2
2025-05-26 22:13:19,066 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-26 22:13:19,066 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-26 22:13:19,066 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 't@@', '<unk>', '@', 'ico', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'del', 'clima', ',', 'il', 'cuore', 'del', 'sistema', 'globale', '.', '</s>']
2025-05-26 22:13:19,066 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-26 22:13:19,066 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-26 22:13:19,066 - INFO - joeynmt.training - 	Hypothesis: Il t<unk> @ ico è , in un senso , il cuore del sistema del clima , il cuore del sistema globale .
2025-05-26 22:13:19,066 - INFO - joeynmt.training - Example #3
2025-05-26 22:13:19,066 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-26 22:13:19,066 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-26 22:13:19,066 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', '<unk>', '@', 'amin@@', '<unk>', '@', 'ati', 'in', 'alto', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-26 22:13:19,066 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-26 22:13:19,066 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-26 22:13:19,066 - INFO - joeynmt.training - 	Hypothesis: Si es<unk> @ amin<unk> @ ati in alto e contr<unk> @ atti in estate .
2025-05-26 22:13:19,066 - INFO - joeynmt.training - Example #4
2025-05-26 22:13:19,067 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-26 22:13:19,067 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-26 22:13:19,067 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'os@@', '<unk>', '@', 'itiva', 'che', 'vi', 'mostr@@', '<unk>', '@', 'ò', 'un', 'po', "'", 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-26 22:13:19,067 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-26 22:13:19,067 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-26 22:13:19,067 - INFO - joeynmt.training - 	Hypothesis: La prossima os<unk> @ itiva che vi mostr<unk> @ ò un po ' di quello che è successo negli ultimi 25 anni .
2025-05-26 22:13:46,787 - INFO - joeynmt.training - Epoch   3, Step:    12600, Batch Loss:     1.832084, Batch Acc: 0.559504, Tokens per Sec:     2566, Lr: 0.000300
2025-05-26 22:14:14,111 - INFO - joeynmt.training - Epoch   3, Step:    12700, Batch Loss:     1.759503, Batch Acc: 0.563121, Tokens per Sec:     2688, Lr: 0.000300
2025-05-26 22:14:42,990 - INFO - joeynmt.training - Epoch   3, Step:    12800, Batch Loss:     1.846124, Batch Acc: 0.559819, Tokens per Sec:     2607, Lr: 0.000300
2025-05-26 22:15:09,936 - INFO - joeynmt.training - Epoch   3, Step:    12900, Batch Loss:     1.744299, Batch Acc: 0.562863, Tokens per Sec:     2723, Lr: 0.000300
2025-05-26 22:15:38,582 - INFO - joeynmt.training - Epoch   3, Step:    13000, Batch Loss:     1.704544, Batch Acc: 0.563938, Tokens per Sec:     2551, Lr: 0.000300
2025-05-26 22:15:38,582 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-26 22:17:50,461 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.64, ppl:   5.16, acc:   0.60, generation: 131.8723[sec], evaluation: 0.0000[sec]
2025-05-26 22:17:50,463 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-26 22:17:50,579 - INFO - joeynmt.helpers - delete models/bpe_8k/10500.ckpt
2025-05-26 22:17:50,584 - INFO - joeynmt.training - Example #0
2025-05-26 22:17:50,584 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-26 22:17:50,584 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-26 22:17:50,584 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'ho', 'mostrato', 'queste', 'due', 'due', 'f@@', '<unk>', '@', 'ine', 'così', 'che', 'dimostr@@', '<unk>', '@', 'ano', 'che', 'il', 'ghiaccio', 'che', 'il', 'ghiaccio', 'che', 'il', 'ghiaccio', 'che', 'il', 'ghiaccio', 'di', 'tre', 'anni', 'è', 'stata', 'la', 'dimensione', 'degli', 'ultimi', 'tre', 'anni', 'è', 'stata', 'la', 'dimensione', 'degli', 'ultimi', 'tre', 'anni', 'sono', 'stati', 'stati', 'stati', 'stati', ',', 'hanno', 'la', 'dimensione', 'delle', 'dimensioni', 'delle', 'dimensioni', 'del', '40', '%', '.', '</s>']
2025-05-26 22:17:50,584 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-26 22:17:50,584 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-26 22:17:50,584 - INFO - joeynmt.training - 	Hypothesis: L' anno ho mostrato queste due due f<unk> @ ine così che dimostr<unk> @ ano che il ghiaccio che il ghiaccio che il ghiaccio che il ghiaccio di tre anni è stata la dimensione degli ultimi tre anni è stata la dimensione degli ultimi tre anni sono stati stati stati stati , hanno la dimensione delle dimensioni delle dimensioni del 40 % .
2025-05-26 22:17:50,584 - INFO - joeynmt.training - Example #1
2025-05-26 22:17:50,585 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-26 22:17:50,585 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-26 22:17:50,585 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'è', 'la', 'questione', 'di', 'questo', 'problema', 'perché', 'non', 'ci', 'dice', 'che', 'non', 'vi', 'mostr@@', '<unk>', '@', 'ano', 'il', 'poll@@', '<unk>', '@', 'o', 'del', 'ghiaccio', '.', '</s>']
2025-05-26 22:17:50,585 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-26 22:17:50,585 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-26 22:17:50,585 - INFO - joeynmt.training - 	Hypothesis: Ma questo è la questione di questo problema perché non ci dice che non vi mostr<unk> @ ano il poll<unk> @ o del ghiaccio .
2025-05-26 22:17:50,585 - INFO - joeynmt.training - Example #2
2025-05-26 22:17:50,585 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-26 22:17:50,585 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-26 22:17:50,585 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'è', 'il', 'cuore', 'del', 'sistema', 'di', 'ca@@', '<unk>', '@', 'p', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'globale', '.', '</s>']
2025-05-26 22:17:50,585 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-26 22:17:50,585 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-26 22:17:50,585 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio è il cuore del sistema di ca<unk> @ p è , in un senso , il cuore del sistema globale .
2025-05-26 22:17:50,585 - INFO - joeynmt.training - Example #3
2025-05-26 22:17:50,585 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-26 22:17:50,585 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-26 22:17:50,585 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'tratta', 'di', 'un', 'al@@', '<unk>', '@', 'ore', 'e', 'il', 'contr@@', '<unk>', '@', 'att@@', '<unk>', '@', 'ore', 'e', 'il', 'contr@@', '<unk>', '@', 'at@@', '<unk>', '@', 'one', '.', '</s>']
2025-05-26 22:17:50,585 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-26 22:17:50,585 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-26 22:17:50,586 - INFO - joeynmt.training - 	Hypothesis: Si tratta di un al<unk> @ ore e il contr<unk> @ att<unk> @ ore e il contr<unk> @ at<unk> @ one .
2025-05-26 22:17:50,586 - INFO - joeynmt.training - Example #4
2025-05-26 22:17:50,586 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-26 22:17:50,586 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-26 22:17:50,586 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'os@@', '<unk>', '@', 'itiva', 'che', 'vi', 'mostrerò', 'un', 'po', "'", 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-26 22:17:50,586 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-26 22:17:50,586 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-26 22:17:50,586 - INFO - joeynmt.training - 	Hypothesis: La prossima os<unk> @ itiva che vi mostrerò un po ' di quello che è successo negli ultimi 25 anni .
2025-05-26 22:18:18,811 - INFO - joeynmt.training - Epoch   3, Step:    13100, Batch Loss:     1.820983, Batch Acc: 0.560128, Tokens per Sec:     2653, Lr: 0.000300
2025-05-26 22:18:46,537 - INFO - joeynmt.training - Epoch   3, Step:    13200, Batch Loss:     1.775490, Batch Acc: 0.568111, Tokens per Sec:     2679, Lr: 0.000300
2025-05-26 22:19:15,604 - INFO - joeynmt.training - Epoch   3, Step:    13300, Batch Loss:     1.882046, Batch Acc: 0.566090, Tokens per Sec:     2605, Lr: 0.000300
2025-05-26 22:19:43,405 - INFO - joeynmt.training - Epoch   3, Step:    13400, Batch Loss:     1.920189, Batch Acc: 0.564950, Tokens per Sec:     2705, Lr: 0.000300
2025-05-26 22:20:11,441 - INFO - joeynmt.training - Epoch   3, Step:    13500, Batch Loss:     1.680324, Batch Acc: 0.566590, Tokens per Sec:     2666, Lr: 0.000300
2025-05-26 22:20:11,441 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-26 22:22:40,555 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.63, ppl:   5.09, acc:   0.60, generation: 149.1066[sec], evaluation: 0.0000[sec]
2025-05-26 22:22:40,557 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-26 22:22:40,671 - INFO - joeynmt.helpers - delete models/bpe_8k/11000.ckpt
2025-05-26 22:22:40,684 - INFO - joeynmt.training - Example #0
2025-05-26 22:22:40,684 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-26 22:22:40,684 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-26 22:22:40,684 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'questi', 'due', 'pi@@', '<unk>', '@', 'ene', 'così', 'che', 'dimostr@@', '<unk>', '@', 'ano', 'che', 'il', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'degli', 'ultimi', 'tre', 'anni', 'è', 'stata', 'la', 'dimensione', 'delle', 'dimensioni', 'del', '40', '%', '.', '</s>']
2025-05-26 22:22:40,684 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-26 22:22:40,684 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-26 22:22:40,684 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso ho mostrato questi due pi<unk> @ ene così che dimostr<unk> @ ano che il ar<unk> @ c<unk> @ ico , che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione degli ultimi tre anni è stata la dimensione delle dimensioni del 40 % .
2025-05-26 22:22:40,684 - INFO - joeynmt.training - Example #1
2025-05-26 22:22:40,684 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-26 22:22:40,684 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-26 22:22:40,684 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', '<unk>', '@', 'sce', 'la', 'vera', 'vera', 'è', 'che', 'non', 'vi', 'mostr@@', '<unk>', '@', 'ano', 'il', 'problema', 'perché', 'non', 'vi', 'vi', 'mostr@@', '<unk>', '@', 'ano', 'il', 'poll@@', '<unk>', '@', 'o', '.', '</s>']
2025-05-26 22:22:40,684 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-26 22:22:40,684 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-26 22:22:40,684 - INFO - joeynmt.training - 	Hypothesis: Ma questo capi<unk> @ sce la vera vera è che non vi mostr<unk> @ ano il problema perché non vi vi mostr<unk> @ ano il poll<unk> @ o .
2025-05-26 22:22:40,684 - INFO - joeynmt.training - Example #2
2025-05-26 22:22:40,684 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-26 22:22:40,684 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-26 22:22:40,684 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 't@@', '<unk>', '@', 'ico', 'ico', 'ico', 'ico', 'ico', 'ico', 'è', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'globale', '.', '</s>']
2025-05-26 22:22:40,685 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-26 22:22:40,685 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-26 22:22:40,685 - INFO - joeynmt.training - 	Hypothesis: Il t<unk> @ ico ico ico ico ico ico è un senso , il cuore del sistema globale .
2025-05-26 22:22:40,685 - INFO - joeynmt.training - Example #3
2025-05-26 22:22:40,685 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-26 22:22:40,685 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-26 22:22:40,685 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'tratta', 'di', 'un', 'pan@@', '<unk>', '@', 'e', 'in', 'seguito', 'in', 'estate', '.', '</s>']
2025-05-26 22:22:40,685 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-26 22:22:40,685 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-26 22:22:40,685 - INFO - joeynmt.training - 	Hypothesis: Si tratta di un pan<unk> @ e in seguito in estate .
2025-05-26 22:22:40,685 - INFO - joeynmt.training - Example #4
2025-05-26 22:22:40,685 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-26 22:22:40,685 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-26 22:22:40,685 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'os@@', '<unk>', '@', 'itiva', 'vi', 'mostro', 'un', 'rapi@@', '<unk>', '@', 'do', 'di', 'più', 'veloce', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-26 22:22:40,685 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-26 22:22:40,685 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-26 22:22:40,685 - INFO - joeynmt.training - 	Hypothesis: La prossima os<unk> @ itiva vi mostro un rapi<unk> @ do di più veloce di quello che è successo negli ultimi 25 anni .
2025-05-26 22:23:09,343 - INFO - joeynmt.training - Epoch   3, Step:    13600, Batch Loss:     1.648683, Batch Acc: 0.569140, Tokens per Sec:     2600, Lr: 0.000300
2025-05-26 22:23:37,312 - INFO - joeynmt.training - Epoch   3, Step:    13700, Batch Loss:     1.902889, Batch Acc: 0.565265, Tokens per Sec:     2634, Lr: 0.000300
2025-05-26 22:24:04,575 - INFO - joeynmt.training - Epoch   3, Step:    13800, Batch Loss:     1.763079, Batch Acc: 0.571097, Tokens per Sec:     2668, Lr: 0.000300
2025-05-26 22:24:32,991 - INFO - joeynmt.training - Epoch   3, Step:    13900, Batch Loss:     1.744404, Batch Acc: 0.573012, Tokens per Sec:     2604, Lr: 0.000300
2025-05-26 22:25:01,661 - INFO - joeynmt.training - Epoch   3, Step:    14000, Batch Loss:     1.726528, Batch Acc: 0.571335, Tokens per Sec:     2552, Lr: 0.000300
2025-05-26 22:25:01,662 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-26 22:27:35,334 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.60, ppl:   4.97, acc:   0.61, generation: 153.6646[sec], evaluation: 0.0000[sec]
2025-05-26 22:27:35,337 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-26 22:27:35,470 - INFO - joeynmt.helpers - delete models/bpe_8k/11500.ckpt
2025-05-26 22:27:35,500 - INFO - joeynmt.training - Example #0
2025-05-26 22:27:35,500 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-26 22:27:35,500 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-26 22:27:35,500 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'ho', 'mostrato', 'queste', 'due', 'due', 'l@@', '<unk>', '@', 'ap@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'mostra', 'che', 'il', 'ghiaccio', 'che', 'il', 'ghiaccio', 'di', 'ghiaccio', 'di', 'tre', 'anni', 'è', 'stata', 'la', 'dimensione', 'del', 'ghiaccio', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'anni', 'è', 'stata', 'la', 'dimensione', 'di', 'tre', 'anni', 'è', 'stata', 'la', 'dimensione', 'delle', 'dimensioni', 'di', 'più', 'di', '4@@', '<unk>', '@', '8', '%', '.', '</s>']
2025-05-26 22:27:35,500 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-26 22:27:35,500 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-26 22:27:35,500 - INFO - joeynmt.training - 	Hypothesis: L' anno ho mostrato queste due due l<unk> @ ap<unk> @ ri<unk> @ mostra che il ghiaccio che il ghiaccio di ghiaccio di tre anni è stata la dimensione del ghiaccio , che per la maggior parte degli ultimi tre anni è stata la dimensione di tre anni è stata la dimensione delle dimensioni di più di 4<unk> @ 8 % .
2025-05-26 22:27:35,500 - INFO - joeynmt.training - Example #1
2025-05-26 22:27:35,500 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-26 22:27:35,500 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-26 22:27:35,500 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', '<unk>', '@', 'sce', 'la', 'vera', 'vera', ',', 'perché', 'non', 'mostra', 'la', 'pena', 'di', 'questo', 'problema', 'perché', 'non', 'mostra', 'la', 'th@@', '<unk>', '@', 'ick@@', '<unk>', '@', 'zza', 'del', 'ghiaccio', '.', '</s>']
2025-05-26 22:27:35,501 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-26 22:27:35,501 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-26 22:27:35,501 - INFO - joeynmt.training - 	Hypothesis: Ma questo capi<unk> @ sce la vera vera , perché non mostra la pena di questo problema perché non mostra la th<unk> @ ick<unk> @ zza del ghiaccio .
2025-05-26 22:27:35,501 - INFO - joeynmt.training - Example #2
2025-05-26 22:27:35,501 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-26 22:27:35,501 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-26 22:27:35,501 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'è', 'il', 'ghiaccio', 'di', 'ca@@', '<unk>', '@', 'p', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'globale', '.', '</s>']
2025-05-26 22:27:35,501 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-26 22:27:35,501 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-26 22:27:35,501 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio è il ghiaccio di ca<unk> @ p è , in un senso , il cuore del sistema globale .
2025-05-26 22:27:35,501 - INFO - joeynmt.training - Example #3
2025-05-26 22:27:35,501 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-26 22:27:35,501 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-26 22:27:35,501 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['È', 'un', 'pan@@', '<unk>', '@', 'i', 'in', 'v@@', '<unk>', '@', 'etro', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-26 22:27:35,501 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-26 22:27:35,501 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-26 22:27:35,501 - INFO - joeynmt.training - 	Hypothesis: È un pan<unk> @ i in v<unk> @ etro e contr<unk> @ atti in estate .
2025-05-26 22:27:35,501 - INFO - joeynmt.training - Example #4
2025-05-26 22:27:35,501 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-26 22:27:35,501 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-26 22:27:35,501 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'os@@', '<unk>', '@', 'itiva', 'che', 'vi', 'sarà', 'un', 'rapi@@', '<unk>', '@', 'do', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-26 22:27:35,502 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-26 22:27:35,502 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-26 22:27:35,502 - INFO - joeynmt.training - 	Hypothesis: La prossima os<unk> @ itiva che vi sarà un rapi<unk> @ do di quello che è successo negli ultimi 25 anni .
2025-05-26 22:28:04,977 - INFO - joeynmt.training - Epoch   3, Step:    14100, Batch Loss:     1.745486, Batch Acc: 0.571518, Tokens per Sec:     2532, Lr: 0.000300
2025-05-26 22:28:33,463 - INFO - joeynmt.training - Epoch   3, Step:    14200, Batch Loss:     1.817496, Batch Acc: 0.570344, Tokens per Sec:     2539, Lr: 0.000300
2025-05-26 22:29:00,848 - INFO - joeynmt.training - Epoch   3, Step:    14300, Batch Loss:     1.845648, Batch Acc: 0.571581, Tokens per Sec:     2710, Lr: 0.000300
2025-05-26 22:29:29,145 - INFO - joeynmt.training - Epoch   3, Step:    14400, Batch Loss:     1.760379, Batch Acc: 0.575563, Tokens per Sec:     2557, Lr: 0.000300
2025-05-26 22:29:57,355 - INFO - joeynmt.training - Epoch   3, Step:    14500, Batch Loss:     1.674955, Batch Acc: 0.570464, Tokens per Sec:     2631, Lr: 0.000300
2025-05-26 22:29:57,355 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-26 22:32:38,985 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.59, ppl:   4.92, acc:   0.61, generation: 161.6218[sec], evaluation: 0.0000[sec]
2025-05-26 22:32:38,989 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-26 22:32:39,112 - INFO - joeynmt.helpers - delete models/bpe_8k/12000.ckpt
2025-05-26 22:32:39,143 - INFO - joeynmt.training - Example #0
2025-05-26 22:32:39,143 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-26 22:32:39,144 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-26 22:32:39,144 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'due', 'pi@@', '<unk>', '@', 'che', 'che', 'dimostr@@', '<unk>', '@', 'ano', 'che', 'il', 'ghiaccio', 'che', 'dimostr@@', '<unk>', '@', 'ano', 'che', 'il', 'ghiaccio', 'che', "l'", 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ant@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'la', 'maggior', 'parte', 'delle', 'tre', 'anni', 'è', 'stata', 'la', 'dimensione', 'del', '4@@', '<unk>', '@', '8', '%', '.', '</s>']
2025-05-26 22:32:39,144 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-26 22:32:39,144 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-26 22:32:39,144 - INFO - joeynmt.training - 	Hypothesis: Il anno scorso ho mostrato queste due pi<unk> @ che che dimostr<unk> @ ano che il ghiaccio che dimostr<unk> @ ano che il ghiaccio che l' ar<unk> @ c<unk> @ ant<unk> @ ico , che per la maggior parte delle tre anni è stata la dimensione del 4<unk> @ 8 % .
2025-05-26 22:32:39,144 - INFO - joeynmt.training - Example #1
2025-05-26 22:32:39,144 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-26 22:32:39,144 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-26 22:32:39,144 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', '<unk>', '@', 'sce', 'la', 'questione', 'di', 'questo', 'problema', ',', 'perché', 'non', 'mostra', 'la', 'prossima', 'idea', 'di', 'questo', 'problema', 'perché', 'non', 'mostra', 'la', 'th@@', '<unk>', '@', 'ick@@', '<unk>', '@', 'zza', 'del', 'ghiaccio', '.', '</s>']
2025-05-26 22:32:39,144 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-26 22:32:39,144 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-26 22:32:39,144 - INFO - joeynmt.training - 	Hypothesis: Ma questo capi<unk> @ sce la questione di questo problema , perché non mostra la prossima idea di questo problema perché non mostra la th<unk> @ ick<unk> @ zza del ghiaccio .
2025-05-26 22:32:39,144 - INFO - joeynmt.training - Example #2
2025-05-26 22:32:39,144 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-26 22:32:39,144 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-26 22:32:39,144 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'è', 'il', 'ghiaccio', 'è', 'il', 'ghiaccio', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'corpo', ',', 'il', 'sistema', 'globale', '.', '</s>']
2025-05-26 22:32:39,144 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-26 22:32:39,144 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-26 22:32:39,144 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio è il ghiaccio è il ghiaccio è , in un senso , il corpo , il sistema globale .
2025-05-26 22:32:39,144 - INFO - joeynmt.training - Example #3
2025-05-26 22:32:39,144 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-26 22:32:39,145 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-26 22:32:39,145 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'tratta', 'di', 'es@@', '<unk>', '@', 'amin@@', '<unk>', '@', 'are', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-26 22:32:39,145 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-26 22:32:39,145 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-26 22:32:39,145 - INFO - joeynmt.training - 	Hypothesis: Si tratta di es<unk> @ amin<unk> @ are e contr<unk> @ atti in estate .
2025-05-26 22:32:39,145 - INFO - joeynmt.training - Example #4
2025-05-26 22:32:39,145 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-26 22:32:39,145 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-26 22:32:39,145 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'os@@', '<unk>', '@', 'itiva', 'che', 'vi', 'mostrerò', 'un', 'rapi@@', '<unk>', '@', 'do', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-26 22:32:39,145 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-26 22:32:39,145 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-26 22:32:39,145 - INFO - joeynmt.training - 	Hypothesis: La prossima os<unk> @ itiva che vi mostrerò un rapi<unk> @ do di quello che è successo negli ultimi 25 anni .
2025-05-26 22:33:06,844 - INFO - joeynmt.training - Epoch   3, Step:    14600, Batch Loss:     1.881354, Batch Acc: 0.572403, Tokens per Sec:     2664, Lr: 0.000300
2025-05-26 22:33:34,753 - INFO - joeynmt.training - Epoch   3, Step:    14700, Batch Loss:     1.678398, Batch Acc: 0.574317, Tokens per Sec:     2658, Lr: 0.000300
2025-05-26 22:34:04,179 - INFO - joeynmt.training - Epoch   3, Step:    14800, Batch Loss:     1.838388, Batch Acc: 0.573295, Tokens per Sec:     2505, Lr: 0.000300
2025-05-26 22:34:32,963 - INFO - joeynmt.training - Epoch   3, Step:    14900, Batch Loss:     1.535414, Batch Acc: 0.578762, Tokens per Sec:     2546, Lr: 0.000300
2025-05-26 22:34:52,244 - INFO - joeynmt.training - Epoch   3: total training loss 8916.65
2025-05-26 22:34:52,244 - INFO - joeynmt.training - EPOCH 4
2025-05-26 22:35:00,357 - INFO - joeynmt.training - Epoch   4, Step:    15000, Batch Loss:     1.643763, Batch Acc: 0.587651, Tokens per Sec:     2619, Lr: 0.000300
2025-05-26 22:35:00,357 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-26 22:37:45,675 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.57, ppl:   4.83, acc:   0.61, generation: 165.3101[sec], evaluation: 0.0000[sec]
2025-05-26 22:37:45,677 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-26 22:37:45,830 - INFO - joeynmt.helpers - delete models/bpe_8k/12500.ckpt
2025-05-26 22:37:45,856 - INFO - joeynmt.training - Example #0
2025-05-26 22:37:45,856 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-26 22:37:45,856 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-26 22:37:45,856 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'ho', 'mostrato', 'queste', 'due', 'l@@', '<unk>', '@', 'ap@@', '<unk>', '@', 'di@@', '<unk>', '@', 'mostra', 'che', 'il', 'ghiaccio', 'che', "l'", 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ant@@', '<unk>', '@', 'ico', ',', 'che', 'il', 'più', 'del', 'primo', 'anno', 'è', 'stato', 'la', 'dimensione', 'dei', 'tre', 'milioni', 'di', 'anni', 'è', 'stato', 'la', 'dimensione', 'dei', 'tre', 'anni', 'è', 'stato', 'la', 'dimensione', 'dei', 'primi', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'dei', 'primi', 'anni', ',', 'ha', 'fatto', 'la', 'dimensione', 'dei', 'primi', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'del', '40', '%', '.', '</s>']
2025-05-26 22:37:45,856 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-26 22:37:45,856 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-26 22:37:45,857 - INFO - joeynmt.training - 	Hypothesis: L' anno ho mostrato queste due l<unk> @ ap<unk> @ di<unk> @ mostra che il ghiaccio che l' ar<unk> @ c<unk> @ ant<unk> @ ico , che il più del primo anno è stato la dimensione dei tre milioni di anni è stato la dimensione dei tre anni è stato la dimensione dei primi milioni di anni è stata la dimensione dei primi anni , ha fatto la dimensione dei primi milioni di anni è stata la dimensione del 40 % .
2025-05-26 22:37:45,857 - INFO - joeynmt.training - Example #1
2025-05-26 22:37:45,857 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-26 22:37:45,857 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-26 22:37:45,857 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', '<unk>', '@', 'sce', 'la', 'questione', 'di', 'questo', 'problema', ',', 'perché', 'non', 'vi', 'mostra', 'la', 'prossima', 'domanda', 'di', 'questo', 'problema', 'non', 'mostra', 'la', 'th@@', '<unk>', '@', 'ick@@', '<unk>', '@', 'zza', 'del', 'ghiaccio', '.', '</s>']
2025-05-26 22:37:45,857 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-26 22:37:45,857 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-26 22:37:45,857 - INFO - joeynmt.training - 	Hypothesis: Ma questo capi<unk> @ sce la questione di questo problema , perché non vi mostra la prossima domanda di questo problema non mostra la th<unk> @ ick<unk> @ zza del ghiaccio .
2025-05-26 22:37:45,857 - INFO - joeynmt.training - Example #2
2025-05-26 22:37:45,857 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-26 22:37:45,857 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-26 22:37:45,857 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'è', 'il', 'ghiaccio', 'è', 'il', 'ghiaccio', 'è', 'in', 'un', 'senso', ',', 'il', 'p@@', '<unk>', '@', 'etto', 'del', 'sistema', 'globale', '.', '</s>']
2025-05-26 22:37:45,857 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-26 22:37:45,857 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-26 22:37:45,857 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio è il ghiaccio è il ghiaccio è in un senso , il p<unk> @ etto del sistema globale .
2025-05-26 22:37:45,857 - INFO - joeynmt.training - Example #3
2025-05-26 22:37:45,857 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-26 22:37:45,857 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-26 22:37:45,857 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'tratta', 'di', 'es@@', '<unk>', '@', 'amin@@', '<unk>', '@', 'ati', 'in', 'v@@', '<unk>', '@', 'etro', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-26 22:37:45,857 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-26 22:37:45,857 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-26 22:37:45,857 - INFO - joeynmt.training - 	Hypothesis: Si tratta di es<unk> @ amin<unk> @ ati in v<unk> @ etro e contr<unk> @ atti in estate .
2025-05-26 22:37:45,857 - INFO - joeynmt.training - Example #4
2025-05-26 22:37:45,858 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-26 22:37:45,858 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-26 22:37:45,858 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'os@@', '<unk>', '@', 'itiva', 'che', 'vi', 'mostrerò', 'un', 'rapi@@', '<unk>', '@', 'do', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-26 22:37:45,858 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-26 22:37:45,858 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-26 22:37:45,858 - INFO - joeynmt.training - 	Hypothesis: La prossima os<unk> @ itiva che vi mostrerò un rapi<unk> @ do di quello che è successo negli ultimi 25 anni .
2025-05-26 22:38:13,041 - INFO - joeynmt.training - Epoch   4, Step:    15100, Batch Loss:     1.601530, Batch Acc: 0.584953, Tokens per Sec:     2732, Lr: 0.000300
2025-05-26 22:38:41,413 - INFO - joeynmt.training - Epoch   4, Step:    15200, Batch Loss:     1.523495, Batch Acc: 0.582248, Tokens per Sec:     2636, Lr: 0.000300
2025-05-26 22:39:09,301 - INFO - joeynmt.training - Epoch   4, Step:    15300, Batch Loss:     1.607541, Batch Acc: 0.582351, Tokens per Sec:     2691, Lr: 0.000300
2025-05-26 22:39:37,481 - INFO - joeynmt.training - Epoch   4, Step:    15400, Batch Loss:     1.621099, Batch Acc: 0.580811, Tokens per Sec:     2590, Lr: 0.000300
2025-05-26 22:40:05,751 - INFO - joeynmt.training - Epoch   4, Step:    15500, Batch Loss:     1.588831, Batch Acc: 0.582948, Tokens per Sec:     2589, Lr: 0.000300
2025-05-26 22:40:05,751 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-26 22:42:26,341 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.56, ppl:   4.78, acc:   0.62, generation: 140.5826[sec], evaluation: 0.0000[sec]
2025-05-26 22:42:26,343 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-26 22:42:26,469 - INFO - joeynmt.helpers - delete models/bpe_8k/13000.ckpt
2025-05-26 22:42:26,493 - INFO - joeynmt.training - Example #0
2025-05-26 22:42:26,493 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-26 22:42:26,493 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-26 22:42:26,493 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'ho', 'mostrato', 'queste', 'due', 'l@@', '<unk>', '@', 'te', 'così', 'che', 'dimostr@@', '<unk>', '@', 'ano', 'che', "l'", 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ant@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'la', 'maggior', 'parte', 'dei', 'due', 'milioni', 'di', 'anni', 'è', 'stato', 'la', 'dimensione', 'di', 'circa', '4@@', '<unk>', '@', '8', '%', ',', 'ha', 'ri@@', '<unk>', '@', 'zz@@', '<unk>', '@', 'ato', 'dal', '40', '%', '.', '</s>']
2025-05-26 22:42:26,493 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-26 22:42:26,493 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-26 22:42:26,493 - INFO - joeynmt.training - 	Hypothesis: L' anno ho mostrato queste due l<unk> @ te così che dimostr<unk> @ ano che l' ar<unk> @ c<unk> @ ant<unk> @ ico , che per la maggior parte dei due milioni di anni è stato la dimensione di circa 4<unk> @ 8 % , ha ri<unk> @ zz<unk> @ ato dal 40 % .
2025-05-26 22:42:26,494 - INFO - joeynmt.training - Example #1
2025-05-26 22:42:26,494 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-26 22:42:26,494 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-26 22:42:26,494 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', '<unk>', '@', 'sce', 'la', 'questione', 'del', 'problema', 'della', 'questione', 'di', 'questo', 'problema', 'perché', 'non', 'lo', 'mostr@@', '<unk>', '@', 'ano', 'il', 'poll@@', '<unk>', '@', 'o', '.', '</s>']
2025-05-26 22:42:26,494 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-26 22:42:26,494 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-26 22:42:26,494 - INFO - joeynmt.training - 	Hypothesis: Ma questo capi<unk> @ sce la questione del problema della questione di questo problema perché non lo mostr<unk> @ ano il poll<unk> @ o .
2025-05-26 22:42:26,494 - INFO - joeynmt.training - Example #2
2025-05-26 22:42:26,494 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-26 22:42:26,494 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-26 22:42:26,494 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'è', 'il', 'ghiaccio', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'climatico', 'globale', '.', '</s>']
2025-05-26 22:42:26,494 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-26 22:42:26,494 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-26 22:42:26,494 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio è il ghiaccio è , in un senso , il cuore del sistema climatico globale .
2025-05-26 22:42:26,494 - INFO - joeynmt.training - Example #3
2025-05-26 22:42:26,494 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-26 22:42:26,494 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-26 22:42:26,494 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', '<unk>', '@', 'amin@@', '<unk>', '@', 'ali', 'nel', 'v@@', '<unk>', '@', 'etro', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-26 22:42:26,494 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-26 22:42:26,494 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-26 22:42:26,494 - INFO - joeynmt.training - 	Hypothesis: Si es<unk> @ amin<unk> @ ali nel v<unk> @ etro e contr<unk> @ atti in estate .
2025-05-26 22:42:26,494 - INFO - joeynmt.training - Example #4
2025-05-26 22:42:26,495 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-26 22:42:26,495 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-26 22:42:26,495 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'slide', 'vi', 'mostrerò', 'un', 'passo', 'avanti', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-26 22:42:26,495 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-26 22:42:26,495 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-26 22:42:26,495 - INFO - joeynmt.training - 	Hypothesis: La prossima slide vi mostrerò un passo avanti di quello che è successo negli ultimi 25 anni .
2025-05-26 22:42:54,129 - INFO - joeynmt.training - Epoch   4, Step:    15600, Batch Loss:     1.523158, Batch Acc: 0.585121, Tokens per Sec:     2719, Lr: 0.000300
2025-05-26 22:43:22,023 - INFO - joeynmt.training - Epoch   4, Step:    15700, Batch Loss:     1.824615, Batch Acc: 0.578953, Tokens per Sec:     2584, Lr: 0.000300
2025-05-26 22:43:49,015 - INFO - joeynmt.training - Epoch   4, Step:    15800, Batch Loss:     1.583197, Batch Acc: 0.585472, Tokens per Sec:     2665, Lr: 0.000300
2025-05-26 22:44:17,026 - INFO - joeynmt.training - Epoch   4, Step:    15900, Batch Loss:     1.739589, Batch Acc: 0.584518, Tokens per Sec:     2593, Lr: 0.000300
2025-05-26 22:44:45,171 - INFO - joeynmt.training - Epoch   4, Step:    16000, Batch Loss:     1.783531, Batch Acc: 0.587357, Tokens per Sec:     2578, Lr: 0.000300
2025-05-26 22:44:45,171 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-26 22:47:19,640 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.55, ppl:   4.72, acc:   0.62, generation: 154.4615[sec], evaluation: 0.0000[sec]
2025-05-26 22:47:19,642 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-26 22:47:19,760 - INFO - joeynmt.helpers - delete models/bpe_8k/13500.ckpt
2025-05-26 22:47:19,780 - INFO - joeynmt.training - Example #0
2025-05-26 22:47:19,780 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-26 22:47:19,780 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-26 22:47:19,780 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'due', 'l@@', '<unk>', '@', 'ap@@', '<unk>', '@', 'di@@', '<unk>', '@', 'de', 'così', 'che', "l'", 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ant@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'più', 'delle', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'delle', 'dimensioni', 'di', 'più', 'alto', '4@@', '<unk>', '@', '8', 'stati', ',', 'hanno', 'portato', 'a', '40', '%', 'di', '4@@', '<unk>', '@', '8', 'anni', ',', 'ha', 'ri@@', '<unk>', '@', 'zz@@', '<unk>', '@', 'ato', 'da', '40', '%', '.', '</s>']
2025-05-26 22:47:19,781 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-26 22:47:19,781 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-26 22:47:19,781 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso ho mostrato queste due l<unk> @ ap<unk> @ di<unk> @ de così che l' ar<unk> @ c<unk> @ ant<unk> @ ico , che per più delle tre milioni di anni è stata la dimensione delle dimensioni di più alto 4<unk> @ 8 stati , hanno portato a 40 % di 4<unk> @ 8 anni , ha ri<unk> @ zz<unk> @ ato da 40 % .
2025-05-26 22:47:19,781 - INFO - joeynmt.training - Example #1
2025-05-26 22:47:19,781 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-26 22:47:19,781 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-26 22:47:19,781 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', '<unk>', '@', 'sce', 'la', 'questione', 'della', 'questione', 'del', 'problema', 'non', 'mostra', 'il', 'problema', 'non', 'mostra', 'il', 'problema', 'del', 'ghiaccio', '.', '</s>']
2025-05-26 22:47:19,781 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-26 22:47:19,781 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-26 22:47:19,781 - INFO - joeynmt.training - 	Hypothesis: Ma questo capi<unk> @ sce la questione della questione del problema non mostra il problema non mostra il problema del ghiaccio .
2025-05-26 22:47:19,781 - INFO - joeynmt.training - Example #2
2025-05-26 22:47:19,781 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-26 22:47:19,781 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-26 22:47:19,781 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 't@@', '<unk>', '@', 'ico', 'è', 'il', 'ghiaccio', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'sistema', 'del', 'sistema', 'climatico', 'del', 'sistema', 'climatico', '.', '</s>']
2025-05-26 22:47:19,781 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-26 22:47:19,781 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-26 22:47:19,781 - INFO - joeynmt.training - 	Hypothesis: Il t<unk> @ ico è il ghiaccio è , in un senso , il sistema del sistema climatico del sistema climatico .
2025-05-26 22:47:19,781 - INFO - joeynmt.training - Example #3
2025-05-26 22:47:19,781 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-26 22:47:19,781 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-26 22:47:19,781 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'tratta', 'di', 'es@@', '<unk>', '@', 'prim@@', '<unk>', '@', 'ere', 'in', 'estate', '.', '</s>']
2025-05-26 22:47:19,782 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-26 22:47:19,782 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-26 22:47:19,782 - INFO - joeynmt.training - 	Hypothesis: Si tratta di es<unk> @ prim<unk> @ ere in estate .
2025-05-26 22:47:19,782 - INFO - joeynmt.training - Example #4
2025-05-26 22:47:19,782 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-26 22:47:19,782 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-26 22:47:19,782 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'slide', 'vi', 'mostrerò', 'un', 'rapi@@', '<unk>', '@', 'do', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-26 22:47:19,782 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-26 22:47:19,782 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-26 22:47:19,782 - INFO - joeynmt.training - 	Hypothesis: La prossima slide vi mostrerò un rapi<unk> @ do di quello che è successo negli ultimi 25 anni .
2025-05-26 22:47:47,663 - INFO - joeynmt.training - Epoch   4, Step:    16100, Batch Loss:     1.660569, Batch Acc: 0.583580, Tokens per Sec:     2632, Lr: 0.000300
2025-05-26 22:48:16,694 - INFO - joeynmt.training - Epoch   4, Step:    16200, Batch Loss:     1.653664, Batch Acc: 0.585229, Tokens per Sec:     2484, Lr: 0.000300
2025-05-26 22:48:44,071 - INFO - joeynmt.training - Epoch   4, Step:    16300, Batch Loss:     1.712816, Batch Acc: 0.584524, Tokens per Sec:     2652, Lr: 0.000300
2025-05-26 22:49:12,914 - INFO - joeynmt.training - Epoch   4, Step:    16400, Batch Loss:     1.593537, Batch Acc: 0.587824, Tokens per Sec:     2571, Lr: 0.000300
2025-05-26 22:49:40,578 - INFO - joeynmt.training - Epoch   4, Step:    16500, Batch Loss:     1.562390, Batch Acc: 0.585685, Tokens per Sec:     2682, Lr: 0.000300
2025-05-26 22:49:40,578 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-26 22:51:59,024 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.54, ppl:   4.68, acc:   0.62, generation: 138.4387[sec], evaluation: 0.0000[sec]
2025-05-26 22:51:59,025 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-26 22:51:59,144 - INFO - joeynmt.helpers - delete models/bpe_8k/14000.ckpt
2025-05-26 22:51:59,157 - INFO - joeynmt.training - Example #0
2025-05-26 22:51:59,158 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-26 22:51:59,158 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-26 22:51:59,158 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'due', 'l@@', '<unk>', '@', 'lu@@', '<unk>', '@', 'de', 'così', 'che', 'dimostr@@', '<unk>', '@', 'ano', 'che', "l'", 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ant@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'del', '4@@', '<unk>', '@', '8', '%', '.', '</s>']
2025-05-26 22:51:59,158 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-26 22:51:59,158 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-26 22:51:59,158 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso ho mostrato queste due l<unk> @ lu<unk> @ de così che dimostr<unk> @ ano che l' ar<unk> @ c<unk> @ ant<unk> @ ico , che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione del 4<unk> @ 8 % .
2025-05-26 22:51:59,158 - INFO - joeynmt.training - Example #1
2025-05-26 22:51:59,158 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-26 22:51:59,158 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-26 22:51:59,158 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'problema', 'è', 'la', 'questione', 'di', 'questo', 'problema', 'è', 'che', 'non', 'mostra', 'il', 'problema', 'perché', 'non', 'si', 'mostra', 'il', 'poll@@', '<unk>', '@', 'o', 'del', 'ghiaccio', '.', '</s>']
2025-05-26 22:51:59,158 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-26 22:51:59,158 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-26 22:51:59,158 - INFO - joeynmt.training - 	Hypothesis: Ma questo problema è la questione di questo problema è che non mostra il problema perché non si mostra il poll<unk> @ o del ghiaccio .
2025-05-26 22:51:59,158 - INFO - joeynmt.training - Example #2
2025-05-26 22:51:59,158 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-26 22:51:59,158 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-26 22:51:59,158 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'è', 'un', 'pezzo', 'di', 'ghiaccio', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'globale', '.', '</s>']
2025-05-26 22:51:59,159 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-26 22:51:59,159 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-26 22:51:59,159 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio è un pezzo di ghiaccio è , in un senso , il cuore del sistema globale .
2025-05-26 22:51:59,159 - INFO - joeynmt.training - Example #3
2025-05-26 22:51:59,159 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-26 22:51:59,159 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-26 22:51:59,159 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'tratta', 'di', 'es@@', '<unk>', '@', 'pan@@', '<unk>', '@', 'e', 'in', 'estate', '.', '</s>']
2025-05-26 22:51:59,159 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-26 22:51:59,159 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-26 22:51:59,159 - INFO - joeynmt.training - 	Hypothesis: Si tratta di es<unk> @ pan<unk> @ e in estate .
2025-05-26 22:51:59,159 - INFO - joeynmt.training - Example #4
2025-05-26 22:51:59,159 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-26 22:51:59,159 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-26 22:51:59,159 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'slide', 'vi', 'mostrerò', 'un', 'rapi@@', '<unk>', '@', 'do', 'più', 'veloce', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', '25', 'anni', '.', '</s>']
2025-05-26 22:51:59,159 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-26 22:51:59,159 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-26 22:51:59,159 - INFO - joeynmt.training - 	Hypothesis: La prossima slide vi mostrerò un rapi<unk> @ do più veloce di quello che è successo negli ultimi 25 25 anni .
2025-05-26 22:52:27,061 - INFO - joeynmt.training - Epoch   4, Step:    16600, Batch Loss:     1.507171, Batch Acc: 0.589126, Tokens per Sec:     2664, Lr: 0.000300
2025-05-26 22:52:56,057 - INFO - joeynmt.training - Epoch   4, Step:    16700, Batch Loss:     1.608821, Batch Acc: 0.588920, Tokens per Sec:     2544, Lr: 0.000300
2025-05-26 22:53:24,362 - INFO - joeynmt.training - Epoch   4, Step:    16800, Batch Loss:     1.625020, Batch Acc: 0.588802, Tokens per Sec:     2595, Lr: 0.000300
2025-05-26 22:53:51,612 - INFO - joeynmt.training - Epoch   4, Step:    16900, Batch Loss:     1.670474, Batch Acc: 0.585984, Tokens per Sec:     2626, Lr: 0.000300
2025-05-26 22:54:19,364 - INFO - joeynmt.training - Epoch   4, Step:    17000, Batch Loss:     1.655901, Batch Acc: 0.583459, Tokens per Sec:     2690, Lr: 0.000300
2025-05-26 22:54:19,365 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-26 22:56:28,021 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.53, ppl:   4.63, acc:   0.62, generation: 128.6500[sec], evaluation: 0.0000[sec]
2025-05-26 22:56:28,023 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-26 22:56:28,138 - INFO - joeynmt.helpers - delete models/bpe_8k/14500.ckpt
2025-05-26 22:56:28,149 - INFO - joeynmt.training - Example #0
2025-05-26 22:56:28,149 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-26 22:56:28,149 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-26 22:56:28,149 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'ho', 'mostrato', 'queste', 'due', 'l@@', '<unk>', '@', 'ap@@', '<unk>', '@', 'di@@', '<unk>', '@', 'de', 'così', 'che', 'dimostr@@', '<unk>', '@', 'ano', 'che', 'il', 'ghiaccio', 'che', 'il', 'ghiaccio', 'che', 'per', 'la', 'maggior', 'parte', 'dei', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'più', 'di', 'più', 'di', '4@@', '<unk>', '@', '8', 'stati', ',', 'ha', 'ri@@', '<unk>', '@', 'dotto', 'dal', '40', '%', '.', '</s>']
2025-05-26 22:56:28,149 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-26 22:56:28,149 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-26 22:56:28,149 - INFO - joeynmt.training - 	Hypothesis: L' anno ho mostrato queste due l<unk> @ ap<unk> @ di<unk> @ de così che dimostr<unk> @ ano che il ghiaccio che il ghiaccio che per la maggior parte dei tre milioni di anni è stata la dimensione più di più di 4<unk> @ 8 stati , ha ri<unk> @ dotto dal 40 % .
2025-05-26 22:56:28,149 - INFO - joeynmt.training - Example #1
2025-05-26 22:56:28,149 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-26 22:56:28,149 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-26 22:56:28,149 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'aspetto', 'che', 'la', 'seri@@', '<unk>', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'il', 'poll@@', '<unk>', '@', 'o', 'del', 'ghiaccio', '.', '</s>']
2025-05-26 22:56:28,149 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-26 22:56:28,149 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-26 22:56:28,149 - INFO - joeynmt.training - 	Hypothesis: Ma questo aspetto che la seri<unk> @ ità del problema perché non mostra il poll<unk> @ o del ghiaccio .
2025-05-26 22:56:28,149 - INFO - joeynmt.training - Example #2
2025-05-26 22:56:28,149 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-26 22:56:28,149 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-26 22:56:28,149 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'è', 'un', 'pezzo', 'di', 'ghiaccio', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'globale', '.', '</s>']
2025-05-26 22:56:28,150 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-26 22:56:28,150 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-26 22:56:28,150 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio è un pezzo di ghiaccio è , in un senso , il cuore del sistema globale .
2025-05-26 22:56:28,150 - INFO - joeynmt.training - Example #3
2025-05-26 22:56:28,150 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-26 22:56:28,150 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-26 22:56:28,150 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', '<unk>', '@', 'amin@@', '<unk>', '@', 'ali', 'nel', 'v@@', '<unk>', '@', 'etro', 'e', 'contr@@', '<unk>', '@', 'atto', 'in', 'estate', '.', '</s>']
2025-05-26 22:56:28,150 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-26 22:56:28,150 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-26 22:56:28,150 - INFO - joeynmt.training - 	Hypothesis: Si es<unk> @ amin<unk> @ ali nel v<unk> @ etro e contr<unk> @ atto in estate .
2025-05-26 22:56:28,150 - INFO - joeynmt.training - Example #4
2025-05-26 22:56:28,150 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-26 22:56:28,150 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-26 22:56:28,150 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'slide', 'vi', 'mostro', 'sarà', 'un', 'rapi@@', '<unk>', '@', 'do', 'di', 'cal@@', '<unk>', '@', 'do', 'di', 'più', 'veloce', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-26 22:56:28,150 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-26 22:56:28,150 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-26 22:56:28,150 - INFO - joeynmt.training - 	Hypothesis: La prossima slide vi mostro sarà un rapi<unk> @ do di cal<unk> @ do di più veloce di quello che è successo negli ultimi 25 anni .
2025-05-26 22:56:55,742 - INFO - joeynmt.training - Epoch   4, Step:    17100, Batch Loss:     1.635502, Batch Acc: 0.589587, Tokens per Sec:     2712, Lr: 0.000300
2025-05-26 22:57:23,299 - INFO - joeynmt.training - Epoch   4, Step:    17200, Batch Loss:     1.488351, Batch Acc: 0.592883, Tokens per Sec:     2757, Lr: 0.000300
2025-05-26 22:57:51,854 - INFO - joeynmt.training - Epoch   4, Step:    17300, Batch Loss:     1.772746, Batch Acc: 0.588269, Tokens per Sec:     2614, Lr: 0.000300
2025-05-26 22:58:18,941 - INFO - joeynmt.training - Epoch   4, Step:    17400, Batch Loss:     1.690455, Batch Acc: 0.595156, Tokens per Sec:     2617, Lr: 0.000300
2025-05-26 22:58:46,499 - INFO - joeynmt.training - Epoch   4, Step:    17500, Batch Loss:     1.715194, Batch Acc: 0.593125, Tokens per Sec:     2645, Lr: 0.000300
2025-05-26 22:58:46,500 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-26 23:00:54,806 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.53, ppl:   4.62, acc:   0.62, generation: 128.2993[sec], evaluation: 0.0000[sec]
2025-05-26 23:00:54,807 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-26 23:00:54,931 - INFO - joeynmt.helpers - delete models/bpe_8k/15000.ckpt
2025-05-26 23:00:54,939 - INFO - joeynmt.training - Example #0
2025-05-26 23:00:54,939 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-26 23:00:54,939 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-26 23:00:54,939 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'due', 'due', 'l@@', '<unk>', '@', 'ap@@', '<unk>', '@', 'di@@', '<unk>', '@', 'de', 'così', 'che', 'il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ant@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'più', 'dei', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'delle', 'dimensioni', 'del', '4@@', '<unk>', '@', '8', 'stati', ',', 'ha', 'ri@@', '<unk>', '@', 'zz@@', '<unk>', '@', 'ato', 'da', '40', '%', '.', '</s>']
2025-05-26 23:00:54,939 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-26 23:00:54,939 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-26 23:00:54,939 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso ho mostrato queste due due l<unk> @ ap<unk> @ di<unk> @ de così che il ghiaccio ar<unk> @ c<unk> @ ant<unk> @ ico , che per più dei tre milioni di anni è stata la dimensione delle dimensioni del 4<unk> @ 8 stati , ha ri<unk> @ zz<unk> @ ato da 40 % .
2025-05-26 23:00:54,939 - INFO - joeynmt.training - Example #1
2025-05-26 23:00:54,939 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-26 23:00:54,939 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-26 23:00:54,939 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', '<unk>', '@', 'sce', 'la', 'seri@@', '<unk>', '@', 'ità', 'di', 'questo', 'particolare', 'problema', 'perché', 'non', 'ha', 'mostrato', 'il', 'poll@@', '<unk>', '@', 'o', '.', '</s>']
2025-05-26 23:00:54,940 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-26 23:00:54,940 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-26 23:00:54,940 - INFO - joeynmt.training - 	Hypothesis: Ma questo capi<unk> @ sce la seri<unk> @ ità di questo particolare problema perché non ha mostrato il poll<unk> @ o .
2025-05-26 23:00:54,940 - INFO - joeynmt.training - Example #2
2025-05-26 23:00:54,940 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-26 23:00:54,940 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-26 23:00:54,940 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'è', ',', 'in', 'un', 'ghiaccio', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'climatico', '.', '</s>']
2025-05-26 23:00:54,940 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-26 23:00:54,940 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-26 23:00:54,940 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio è , in un ghiaccio è , in un senso , il cuore del sistema climatico .
2025-05-26 23:00:54,940 - INFO - joeynmt.training - Example #3
2025-05-26 23:00:54,940 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-26 23:00:54,940 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-26 23:00:54,940 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', '<unk>', '@', 'pan@@', '<unk>', '@', 'di', 'v@@', '<unk>', '@', 'etro', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-26 23:00:54,941 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-26 23:00:54,941 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-26 23:00:54,941 - INFO - joeynmt.training - 	Hypothesis: Si es<unk> @ pan<unk> @ di v<unk> @ etro e contr<unk> @ atti in estate .
2025-05-26 23:00:54,941 - INFO - joeynmt.training - Example #4
2025-05-26 23:00:54,941 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-26 23:00:54,941 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-26 23:00:54,941 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'slide', 'vi', 'mostrerò', 'un', 'rapi@@', '<unk>', '@', 'do', 'di', 'cal@@', '<unk>', '@', 'do', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-26 23:00:54,941 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-26 23:00:54,941 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-26 23:00:54,941 - INFO - joeynmt.training - 	Hypothesis: La prossima slide vi mostrerò un rapi<unk> @ do di cal<unk> @ do di quello che è successo negli ultimi 25 anni .
2025-05-26 23:01:23,793 - INFO - joeynmt.training - Epoch   4, Step:    17600, Batch Loss:     1.527296, Batch Acc: 0.589630, Tokens per Sec:     2554, Lr: 0.000300
2025-05-26 23:01:54,991 - INFO - joeynmt.training - Epoch   4, Step:    17700, Batch Loss:     1.574350, Batch Acc: 0.591137, Tokens per Sec:     2389, Lr: 0.000300
2025-05-26 23:02:22,176 - INFO - joeynmt.training - Epoch   4, Step:    17800, Batch Loss:     1.695724, Batch Acc: 0.587131, Tokens per Sec:     2675, Lr: 0.000300
2025-05-26 23:02:50,148 - INFO - joeynmt.training - Epoch   4, Step:    17900, Batch Loss:     1.553879, Batch Acc: 0.589052, Tokens per Sec:     2686, Lr: 0.000300
2025-05-26 23:03:17,729 - INFO - joeynmt.training - Epoch   4, Step:    18000, Batch Loss:     1.588881, Batch Acc: 0.588443, Tokens per Sec:     2634, Lr: 0.000300
2025-05-26 23:03:17,730 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-26 23:05:56,717 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.51, ppl:   4.55, acc:   0.63, generation: 158.9800[sec], evaluation: 0.0000[sec]
2025-05-26 23:05:56,718 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-26 23:05:56,844 - INFO - joeynmt.helpers - delete models/bpe_8k/15500.ckpt
2025-05-26 23:05:56,852 - INFO - joeynmt.training - Example #0
2025-05-26 23:05:56,852 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-26 23:05:56,852 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-26 23:05:56,853 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'due', 'pi@@', '<unk>', '@', 'ene', 'così', 'che', 'dimostr@@', '<unk>', '@', 'ano', 'che', "l'", 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ant@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'la', 'maggior', 'parte', 'delle', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'delle', 'dimensioni', 'di', 'più', 'più', 'alto', '4@@', '<unk>', '@', '8', '%', '.', '</s>']
2025-05-26 23:05:56,853 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-26 23:05:56,853 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-26 23:05:56,853 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso ho mostrato queste due pi<unk> @ ene così che dimostr<unk> @ ano che l' ar<unk> @ c<unk> @ ant<unk> @ ico , che per la maggior parte delle tre milioni di anni è stata la dimensione delle dimensioni di più più alto 4<unk> @ 8 % .
2025-05-26 23:05:56,853 - INFO - joeynmt.training - Example #1
2025-05-26 23:05:56,853 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-26 23:05:56,853 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-26 23:05:56,853 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', '<unk>', '@', 'sce', 'la', 'seri@@', '<unk>', '@', 'età', 'di', 'questo', 'problema', 'perché', 'non', 'mostra', 'il', 'poll@@', '<unk>', '@', 'o', 'del', 'ghiaccio', '.', '</s>']
2025-05-26 23:05:56,853 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-26 23:05:56,853 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-26 23:05:56,853 - INFO - joeynmt.training - 	Hypothesis: Ma questo capi<unk> @ sce la seri<unk> @ età di questo problema perché non mostra il poll<unk> @ o del ghiaccio .
2025-05-26 23:05:56,853 - INFO - joeynmt.training - Example #2
2025-05-26 23:05:56,853 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-26 23:05:56,853 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-26 23:05:56,853 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ant@@', '<unk>', '@', 'ico', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'climatico', '.', '</s>']
2025-05-26 23:05:56,853 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-26 23:05:56,853 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-26 23:05:56,854 - INFO - joeynmt.training - 	Hypothesis: L' ar<unk> @ c<unk> @ ant<unk> @ ico è , in un senso , il cuore del sistema climatico .
2025-05-26 23:05:56,854 - INFO - joeynmt.training - Example #3
2025-05-26 23:05:56,854 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-26 23:05:56,854 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-26 23:05:56,854 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', '<unk>', '@', 'pan@@', '<unk>', '@', 'di', 'v@@', '<unk>', '@', 'etro', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-26 23:05:56,854 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-26 23:05:56,854 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-26 23:05:56,854 - INFO - joeynmt.training - 	Hypothesis: Si es<unk> @ pan<unk> @ di v<unk> @ etro e contr<unk> @ atti in estate .
2025-05-26 23:05:56,854 - INFO - joeynmt.training - Example #4
2025-05-26 23:05:56,854 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-26 23:05:56,854 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-26 23:05:56,854 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'slide', 'vi', 'mostrerò', 'un', 'rapi@@', '<unk>', '@', 'do', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-26 23:05:56,854 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-26 23:05:56,854 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-26 23:05:56,854 - INFO - joeynmt.training - 	Hypothesis: La prossima slide vi mostrerò un rapi<unk> @ do di quello che è successo negli ultimi 25 anni .
2025-05-26 23:06:24,046 - INFO - joeynmt.training - Epoch   4, Step:    18100, Batch Loss:     1.506108, Batch Acc: 0.594372, Tokens per Sec:     2728, Lr: 0.000300
2025-05-26 23:06:51,804 - INFO - joeynmt.training - Epoch   4, Step:    18200, Batch Loss:     1.513447, Batch Acc: 0.593392, Tokens per Sec:     2678, Lr: 0.000300
2025-05-26 23:07:19,310 - INFO - joeynmt.training - Epoch   4, Step:    18300, Batch Loss:     1.579648, Batch Acc: 0.591849, Tokens per Sec:     2608, Lr: 0.000300
2025-05-26 23:07:47,521 - INFO - joeynmt.training - Epoch   4, Step:    18400, Batch Loss:     1.594398, Batch Acc: 0.592574, Tokens per Sec:     2644, Lr: 0.000300
2025-05-26 23:08:15,778 - INFO - joeynmt.training - Epoch   4, Step:    18500, Batch Loss:     1.581471, Batch Acc: 0.589090, Tokens per Sec:     2589, Lr: 0.000300
2025-05-26 23:08:15,779 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-26 23:10:38,492 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.51, ppl:   4.51, acc:   0.63, generation: 142.7063[sec], evaluation: 0.0000[sec]
2025-05-26 23:10:38,494 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-26 23:10:38,614 - INFO - joeynmt.helpers - delete models/bpe_8k/16000.ckpt
2025-05-26 23:10:38,621 - INFO - joeynmt.training - Example #0
2025-05-26 23:10:38,621 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-26 23:10:38,621 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-26 23:10:38,621 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'due', 'di@@', '<unk>', '@', 'ap@@', '<unk>', '@', 'di@@', '<unk>', '@', 'de', 'così', 'che', 'dimostr@@', '<unk>', '@', 'ano', 'che', "l'", 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ant@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'più', 'degli', 'ultimi', 'tre', 'anni', 'è', 'stata', 'la', 'dimensione', 'delle', 'dimensioni', 'delle', 'dimensioni', 'più', 'più', 'alti', '4@@', '<unk>', '@', '8', 'anni', ',', 'ha', 'portato', 'il', '40', '%', '.', '</s>']
2025-05-26 23:10:38,621 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-26 23:10:38,621 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-26 23:10:38,621 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso ho mostrato queste due di<unk> @ ap<unk> @ di<unk> @ de così che dimostr<unk> @ ano che l' ar<unk> @ c<unk> @ ant<unk> @ ico , che per più degli ultimi tre anni è stata la dimensione delle dimensioni delle dimensioni più più alti 4<unk> @ 8 anni , ha portato il 40 % .
2025-05-26 23:10:38,621 - INFO - joeynmt.training - Example #1
2025-05-26 23:10:38,621 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-26 23:10:38,621 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-26 23:10:38,621 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'problema', 'capi@@', '<unk>', '@', 'sce', 'la', 'seri@@', '<unk>', '@', 'età', 'di', 'questo', 'problema', 'perché', 'non', 'mostra', 'il', 'grafico', 'perché', 'non', 'mostra', 'il', 'poll@@', '<unk>', '@', 'o', '.', '</s>']
2025-05-26 23:10:38,621 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-26 23:10:38,622 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-26 23:10:38,622 - INFO - joeynmt.training - 	Hypothesis: Ma questo problema capi<unk> @ sce la seri<unk> @ età di questo problema perché non mostra il grafico perché non mostra il poll<unk> @ o .
2025-05-26 23:10:38,622 - INFO - joeynmt.training - Example #2
2025-05-26 23:10:38,622 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-26 23:10:38,622 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-26 23:10:38,622 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ant@@', '<unk>', '@', 'ico', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'globale', '.', '</s>']
2025-05-26 23:10:38,622 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-26 23:10:38,622 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-26 23:10:38,622 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio ar<unk> @ c<unk> @ ant<unk> @ ico è , in un senso , il cuore del sistema globale .
2025-05-26 23:10:38,622 - INFO - joeynmt.training - Example #3
2025-05-26 23:10:38,622 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-26 23:10:38,622 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-26 23:10:38,622 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', "'", 'es@@', '<unk>', '@', 'pan@@', '<unk>', '@', 'di', 'v@@', '<unk>', '@', 'etro', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-26 23:10:38,622 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-26 23:10:38,622 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-26 23:10:38,622 - INFO - joeynmt.training - 	Hypothesis: E ' es<unk> @ pan<unk> @ di v<unk> @ etro e contr<unk> @ atti in estate .
2025-05-26 23:10:38,622 - INFO - joeynmt.training - Example #4
2025-05-26 23:10:38,623 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-26 23:10:38,623 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-26 23:10:38,623 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'slide', 'vi', 'mostrerò', 'un', 'rapi@@', '<unk>', '@', 'do', 'di', 'più', 'veloce', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-26 23:10:38,623 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-26 23:10:38,623 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-26 23:10:38,623 - INFO - joeynmt.training - 	Hypothesis: La prossima slide vi mostrerò un rapi<unk> @ do di più veloce di quello che è successo negli ultimi 25 anni .
2025-05-26 23:11:07,925 - INFO - joeynmt.training - Epoch   4, Step:    18600, Batch Loss:     1.581228, Batch Acc: 0.593209, Tokens per Sec:     2459, Lr: 0.000300
2025-05-26 23:11:36,506 - INFO - joeynmt.training - Epoch   4, Step:    18700, Batch Loss:     1.627218, Batch Acc: 0.594935, Tokens per Sec:     2705, Lr: 0.000300
2025-05-26 23:12:04,832 - INFO - joeynmt.training - Epoch   4, Step:    18800, Batch Loss:     1.635490, Batch Acc: 0.595823, Tokens per Sec:     2629, Lr: 0.000300
2025-05-26 23:12:32,189 - INFO - joeynmt.training - Epoch   4, Step:    18900, Batch Loss:     1.599454, Batch Acc: 0.594555, Tokens per Sec:     2661, Lr: 0.000300
2025-05-26 23:12:59,323 - INFO - joeynmt.training - Epoch   4, Step:    19000, Batch Loss:     1.647014, Batch Acc: 0.593405, Tokens per Sec:     2699, Lr: 0.000300
2025-05-26 23:12:59,323 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-26 23:15:32,497 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.50, ppl:   4.48, acc:   0.63, generation: 153.1665[sec], evaluation: 0.0000[sec]
2025-05-26 23:15:32,498 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-26 23:15:32,617 - INFO - joeynmt.helpers - delete models/bpe_8k/16500.ckpt
2025-05-26 23:15:32,623 - INFO - joeynmt.training - Example #0
2025-05-26 23:15:32,623 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-26 23:15:32,623 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-26 23:15:32,623 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'due', 'fi@@', '<unk>', '@', 'di', 'così', 'che', 'il', 'ghiaccio', 'di', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ant@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'più', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stato', 'la', 'dimensione', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stato', 'la', 'dimensione', 'delle', 'più', 'alti', 'di', '4@@', '<unk>', '@', '8', ',', 'ha', 'sp@@', '<unk>', '@', 'eso', 'dal', '40', '%', '.', '</s>']
2025-05-26 23:15:32,623 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-26 23:15:32,623 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-26 23:15:32,623 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso ho mostrato queste due fi<unk> @ di così che il ghiaccio di ar<unk> @ c<unk> @ ant<unk> @ ico , che per più degli ultimi tre milioni di anni è stato la dimensione degli ultimi tre milioni di anni è stato la dimensione delle più alti di 4<unk> @ 8 , ha sp<unk> @ eso dal 40 % .
2025-05-26 23:15:32,623 - INFO - joeynmt.training - Example #1
2025-05-26 23:15:32,623 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-26 23:15:32,623 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-26 23:15:32,623 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questa', 'capi@@', '<unk>', '@', 'sce', 'la', 'seri@@', '<unk>', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'il', 'th@@', '<unk>', '@', 'olo', '.', '</s>']
2025-05-26 23:15:32,623 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-26 23:15:32,623 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-26 23:15:32,623 - INFO - joeynmt.training - 	Hypothesis: Ma questa capi<unk> @ sce la seri<unk> @ ità del problema perché non mostra il th<unk> @ olo .
2025-05-26 23:15:32,624 - INFO - joeynmt.training - Example #2
2025-05-26 23:15:32,624 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-26 23:15:32,624 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-26 23:15:32,624 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ant@@', '<unk>', '@', 'ico', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'globale', '.', '</s>']
2025-05-26 23:15:32,624 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-26 23:15:32,624 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-26 23:15:32,624 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio ar<unk> @ c<unk> @ ant<unk> @ ico è , in un senso , il cuore del sistema globale .
2025-05-26 23:15:32,624 - INFO - joeynmt.training - Example #3
2025-05-26 23:15:32,624 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-26 23:15:32,624 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-26 23:15:32,624 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'tratta', 'di', 'es@@', '<unk>', '@', 'amin@@', '<unk>', '@', 'ali', 'in', 'v@@', '<unk>', '@', 'etro', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-26 23:15:32,624 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-26 23:15:32,624 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-26 23:15:32,624 - INFO - joeynmt.training - 	Hypothesis: Si tratta di es<unk> @ amin<unk> @ ali in v<unk> @ etro e contr<unk> @ atti in estate .
2025-05-26 23:15:32,624 - INFO - joeynmt.training - Example #4
2025-05-26 23:15:32,624 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-26 23:15:32,624 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-26 23:15:32,624 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'slide', 'vi', 'mostrerò', 'un', 'rapi@@', '<unk>', '@', 'do', 'di', 'un', 'rapi@@', '<unk>', '@', 'do', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-26 23:15:32,625 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-26 23:15:32,625 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-26 23:15:32,625 - INFO - joeynmt.training - 	Hypothesis: La prossima slide vi mostrerò un rapi<unk> @ do di un rapi<unk> @ do di quello che è successo negli ultimi 25 anni .
2025-05-26 23:15:59,817 - INFO - joeynmt.training - Epoch   4, Step:    19100, Batch Loss:     1.583752, Batch Acc: 0.596079, Tokens per Sec:     2719, Lr: 0.000300
2025-05-26 23:16:26,761 - INFO - joeynmt.training - Epoch   4, Step:    19200, Batch Loss:     1.515879, Batch Acc: 0.594815, Tokens per Sec:     2690, Lr: 0.000300
2025-05-26 23:16:54,194 - INFO - joeynmt.training - Epoch   4, Step:    19300, Batch Loss:     1.643185, Batch Acc: 0.596846, Tokens per Sec:     2656, Lr: 0.000300
2025-05-26 23:17:22,335 - INFO - joeynmt.training - Epoch   4, Step:    19400, Batch Loss:     1.651522, Batch Acc: 0.592304, Tokens per Sec:     2632, Lr: 0.000300
2025-05-26 23:17:51,200 - INFO - joeynmt.training - Epoch   4, Step:    19500, Batch Loss:     1.480807, Batch Acc: 0.591761, Tokens per Sec:     2583, Lr: 0.000300
2025-05-26 23:17:51,201 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-26 23:20:03,699 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.48, ppl:   4.40, acc:   0.63, generation: 132.4909[sec], evaluation: 0.0000[sec]
2025-05-26 23:20:03,700 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-26 23:20:03,821 - INFO - joeynmt.helpers - delete models/bpe_8k/17000.ckpt
2025-05-26 23:20:03,831 - INFO - joeynmt.training - Example #0
2025-05-26 23:20:03,831 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-26 23:20:03,831 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-26 23:20:03,831 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'due', 'lu@@', '<unk>', '@', 'mi', 'che', 'il', 'ghiaccio', 'che', "l'", 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ant@@', '<unk>', '@', 'ico', ',', 'che', 'il', 'ghiaccio', 'di', 'più', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'delle', 'più', 'alto', 'del', '4@@', '<unk>', '@', '8', 'stati', ',', 'ha', 'ri@@', '<unk>', '@', 'p@@', '<unk>', '@', 'ett@@', '<unk>', '@', 'ori', 'del', '40', '%', '.', '</s>']
2025-05-26 23:20:03,831 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-26 23:20:03,831 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-26 23:20:03,831 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso ho mostrato queste due lu<unk> @ mi che il ghiaccio che l' ar<unk> @ c<unk> @ ant<unk> @ ico , che il ghiaccio di più degli ultimi tre milioni di anni è stata la dimensione delle più alto del 4<unk> @ 8 stati , ha ri<unk> @ p<unk> @ ett<unk> @ ori del 40 % .
2025-05-26 23:20:03,831 - INFO - joeynmt.training - Example #1
2025-05-26 23:20:03,831 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-26 23:20:03,831 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-26 23:20:03,831 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', '<unk>', '@', 'sce', 'la', 'seri@@', '<unk>', '@', 'ità', 'di', 'questo', 'problema', 'perché', 'non', 'mostra', 'il', 'th@@', '<unk>', '@', 'ick@@', '<unk>', '@', 'zza', 'del', 'ghiaccio', '.', '</s>']
2025-05-26 23:20:03,831 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-26 23:20:03,831 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-26 23:20:03,831 - INFO - joeynmt.training - 	Hypothesis: Ma questo capi<unk> @ sce la seri<unk> @ ità di questo problema perché non mostra il th<unk> @ ick<unk> @ zza del ghiaccio .
2025-05-26 23:20:03,831 - INFO - joeynmt.training - Example #2
2025-05-26 23:20:03,832 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-26 23:20:03,832 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-26 23:20:03,832 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ant@@', '<unk>', '@', 'ico', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'del', 'clima', 'del', 'clima', '.', '</s>']
2025-05-26 23:20:03,832 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-26 23:20:03,832 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-26 23:20:03,832 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio ar<unk> @ c<unk> @ ant<unk> @ ico è , in un senso , il cuore del sistema del clima del clima .
2025-05-26 23:20:03,832 - INFO - joeynmt.training - Example #3
2025-05-26 23:20:03,832 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-26 23:20:03,832 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-26 23:20:03,832 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', '<unk>', '@', 'pan@@', '<unk>', '@', 'di', 'v@@', '<unk>', '@', 'etro', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-26 23:20:03,832 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-26 23:20:03,832 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-26 23:20:03,832 - INFO - joeynmt.training - 	Hypothesis: Si es<unk> @ pan<unk> @ di v<unk> @ etro e contr<unk> @ atti in estate .
2025-05-26 23:20:03,832 - INFO - joeynmt.training - Example #4
2025-05-26 23:20:03,832 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-26 23:20:03,832 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-26 23:20:03,832 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'slide', 'vi', 'mostrerò', 'un', 'rapi@@', '<unk>', '@', 'do', 'di', 'cal@@', '<unk>', '@', 'do', 'di', 'più', 'veloce', 'di', 'quello', 'che', 'è', 'accad@@', '<unk>', '@', 'uto', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-26 23:20:03,832 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-26 23:20:03,832 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-26 23:20:03,832 - INFO - joeynmt.training - 	Hypothesis: La prossima slide vi mostrerò un rapi<unk> @ do di cal<unk> @ do di più veloce di quello che è accad<unk> @ uto negli ultimi 25 anni .
2025-05-26 23:20:32,194 - INFO - joeynmt.training - Epoch   4, Step:    19600, Batch Loss:     1.712615, Batch Acc: 0.598964, Tokens per Sec:     2662, Lr: 0.000300
2025-05-26 23:20:59,299 - INFO - joeynmt.training - Epoch   4, Step:    19700, Batch Loss:     1.573716, Batch Acc: 0.598034, Tokens per Sec:     2673, Lr: 0.000300
2025-05-26 23:21:26,880 - INFO - joeynmt.training - Epoch   4, Step:    19800, Batch Loss:     1.601519, Batch Acc: 0.596026, Tokens per Sec:     2735, Lr: 0.000300
2025-05-26 23:21:55,162 - INFO - joeynmt.training - Epoch   4, Step:    19900, Batch Loss:     1.800495, Batch Acc: 0.598544, Tokens per Sec:     2700, Lr: 0.000300
2025-05-26 23:22:10,145 - INFO - joeynmt.training - Epoch   4: total training loss 8153.07
2025-05-26 23:22:10,145 - INFO - joeynmt.training - EPOCH 5
2025-05-26 23:22:23,125 - INFO - joeynmt.training - Epoch   5, Step:    20000, Batch Loss:     1.712854, Batch Acc: 0.603091, Tokens per Sec:     2542, Lr: 0.000300
2025-05-26 23:22:23,125 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-26 23:24:53,064 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.48, ppl:   4.41, acc:   0.63, generation: 149.9303[sec], evaluation: 0.0000[sec]
2025-05-26 23:24:53,181 - INFO - joeynmt.helpers - delete models/bpe_8k/17500.ckpt
2025-05-26 23:24:53,198 - INFO - joeynmt.training - Example #0
2025-05-26 23:24:53,198 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-26 23:24:53,198 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-26 23:24:53,198 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'scorso', 'ho', 'mostrato', 'queste', 'due', 'lu@@', '<unk>', '@', 'di', 'così', 'che', 'dimostr@@', '<unk>', '@', 'ano', 'che', 'il', 'ghiaccio', "dell'", 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ant@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'dei', '4@@', '<unk>', '@', '8', 'stati', ',', 'ha', 'ri@@', '<unk>', '@', 'dotto', 'dalla', '40', '%', '.', '</s>']
2025-05-26 23:24:53,198 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-26 23:24:53,198 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-26 23:24:53,198 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso scorso ho mostrato queste due lu<unk> @ di così che dimostr<unk> @ ano che il ghiaccio dell' ar<unk> @ c<unk> @ ant<unk> @ ico , che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione dei 4<unk> @ 8 stati , ha ri<unk> @ dotto dalla 40 % .
2025-05-26 23:24:53,198 - INFO - joeynmt.training - Example #1
2025-05-26 23:24:53,198 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-26 23:24:53,198 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-26 23:24:53,198 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'è', 'la', 'seri@@', '<unk>', '@', 'ità', 'di', 'questo', 'problema', 'perché', 'non', 'mostra', 'la', 'mostra', 'la', 'pena', 'di', 'questo', 'problema', 'perché', 'non', 'mostra', 'la', 'prossima', 'to@@', '<unk>', '@', 'zza', 'del', 'ghiaccio', '.', '</s>']
2025-05-26 23:24:53,199 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-26 23:24:53,199 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-26 23:24:53,199 - INFO - joeynmt.training - 	Hypothesis: Ma questo è la seri<unk> @ ità di questo problema perché non mostra la mostra la pena di questo problema perché non mostra la prossima to<unk> @ zza del ghiaccio .
2025-05-26 23:24:53,199 - INFO - joeynmt.training - Example #2
2025-05-26 23:24:53,199 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-26 23:24:53,199 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-26 23:24:53,199 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'è', 'il', 'ghiaccio', 'è', 'un', 'senso', ',', 'il', 'p@@', '<unk>', '@', 'etto', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'globale', '.', '</s>']
2025-05-26 23:24:53,199 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-26 23:24:53,199 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-26 23:24:53,199 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio è il ghiaccio è un senso , il p<unk> @ etto è , in un senso , il cuore del sistema globale .
2025-05-26 23:24:53,199 - INFO - joeynmt.training - Example #3
2025-05-26 23:24:53,199 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-26 23:24:53,199 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-26 23:24:53,199 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', '<unk>', '@', 'pon@@', '<unk>', '@', 'gono', 'in', 'v@@', '<unk>', '@', 'etro', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-26 23:24:53,199 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-26 23:24:53,199 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-26 23:24:53,199 - INFO - joeynmt.training - 	Hypothesis: Si es<unk> @ pon<unk> @ gono in v<unk> @ etro e contr<unk> @ atti in estate .
2025-05-26 23:24:53,199 - INFO - joeynmt.training - Example #4
2025-05-26 23:24:53,199 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-26 23:24:53,199 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-26 23:24:53,199 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'slide', 'vi', 'mostrerò', 'un', 'rapi@@', '<unk>', '@', 'do', 'di', 'f@@', '<unk>', '@', 'ast@@', '<unk>', '@', 'ante', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-26 23:24:53,199 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-26 23:24:53,200 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-26 23:24:53,200 - INFO - joeynmt.training - 	Hypothesis: La prossima slide vi mostrerò un rapi<unk> @ do di f<unk> @ ast<unk> @ ante di quello che è successo negli ultimi 25 anni .
2025-05-26 23:25:21,351 - INFO - joeynmt.training - Epoch   5, Step:    20100, Batch Loss:     1.412166, Batch Acc: 0.605442, Tokens per Sec:     2639, Lr: 0.000300
2025-05-26 23:25:49,549 - INFO - joeynmt.training - Epoch   5, Step:    20200, Batch Loss:     1.547219, Batch Acc: 0.603623, Tokens per Sec:     2594, Lr: 0.000300
2025-05-26 23:26:18,217 - INFO - joeynmt.training - Epoch   5, Step:    20300, Batch Loss:     1.534911, Batch Acc: 0.603428, Tokens per Sec:     2573, Lr: 0.000300
2025-05-26 23:26:46,112 - INFO - joeynmt.training - Epoch   5, Step:    20400, Batch Loss:     1.622337, Batch Acc: 0.601900, Tokens per Sec:     2569, Lr: 0.000300
2025-05-26 23:27:14,825 - INFO - joeynmt.training - Epoch   5, Step:    20500, Batch Loss:     1.538103, Batch Acc: 0.603961, Tokens per Sec:     2594, Lr: 0.000300
2025-05-26 23:27:14,825 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-26 23:29:23,717 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.48, ppl:   4.37, acc:   0.64, generation: 128.8839[sec], evaluation: 0.0000[sec]
2025-05-26 23:29:23,718 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-26 23:29:23,867 - INFO - joeynmt.helpers - delete models/bpe_8k/18000.ckpt
2025-05-26 23:29:23,888 - INFO - joeynmt.training - Example #0
2025-05-26 23:29:23,889 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-26 23:29:23,889 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-26 23:29:23,889 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'due', 'ap@@', '<unk>', '@', 'os@@', '<unk>', '@', 'sa', 'così', 'che', 'dimostr@@', '<unk>', '@', 'ano', 'che', 'il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ant@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'dei', '4@@', '<unk>', '@', '8', 'stati', ',', 'ha', 'ri@@', '<unk>', '@', 'dotto', 'dal', '40', '%', '.', '</s>']
2025-05-26 23:29:23,889 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-26 23:29:23,889 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-26 23:29:23,889 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso ho mostrato queste due ap<unk> @ os<unk> @ sa così che dimostr<unk> @ ano che il ghiaccio ar<unk> @ c<unk> @ ant<unk> @ ico , che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione dei 4<unk> @ 8 stati , ha ri<unk> @ dotto dal 40 % .
2025-05-26 23:29:23,889 - INFO - joeynmt.training - Example #1
2025-05-26 23:29:23,889 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-26 23:29:23,889 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-26 23:29:23,889 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', '<unk>', '@', 'scono', 'la', 'seri@@', '<unk>', '@', 'ità', 'di', 'questo', 'problema', 'perché', 'non', 'mostra', 'il', 'poll@@', '<unk>', '@', 'o', 'del', 'ghiaccio', '.', '</s>']
2025-05-26 23:29:23,889 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-26 23:29:23,889 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-26 23:29:23,889 - INFO - joeynmt.training - 	Hypothesis: Ma questo capi<unk> @ scono la seri<unk> @ ità di questo problema perché non mostra il poll<unk> @ o del ghiaccio .
2025-05-26 23:29:23,889 - INFO - joeynmt.training - Example #2
2025-05-26 23:29:23,889 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-26 23:29:23,889 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-26 23:29:23,889 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ant@@', '<unk>', '@', 'ico', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'del', 'clima', '.', '</s>']
2025-05-26 23:29:23,890 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-26 23:29:23,890 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-26 23:29:23,890 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio ar<unk> @ c<unk> @ ant<unk> @ ico è , in un senso , il cuore del sistema del clima .
2025-05-26 23:29:23,890 - INFO - joeynmt.training - Example #3
2025-05-26 23:29:23,890 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-26 23:29:23,890 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-26 23:29:23,890 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Gli', 'es@@', '<unk>', '@', 'pan@@', '<unk>', '@', 'di', 'in', 'v@@', '<unk>', '@', 'enti', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-26 23:29:23,890 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-26 23:29:23,890 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-26 23:29:23,890 - INFO - joeynmt.training - 	Hypothesis: Gli es<unk> @ pan<unk> @ di in v<unk> @ enti e contr<unk> @ atti in estate .
2025-05-26 23:29:23,890 - INFO - joeynmt.training - Example #4
2025-05-26 23:29:23,890 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-26 23:29:23,890 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-26 23:29:23,890 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'slide', 'vi', 'mostro', 'sarà', 'un', 'rapi@@', '<unk>', '@', 'do', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-26 23:29:23,890 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-26 23:29:23,890 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-26 23:29:23,890 - INFO - joeynmt.training - 	Hypothesis: La prossima slide vi mostro sarà un rapi<unk> @ do di quello che è successo negli ultimi 25 anni .
2025-05-26 23:29:52,441 - INFO - joeynmt.training - Epoch   5, Step:    20600, Batch Loss:     1.400256, Batch Acc: 0.604594, Tokens per Sec:     2530, Lr: 0.000300
2025-05-26 23:30:19,584 - INFO - joeynmt.training - Epoch   5, Step:    20700, Batch Loss:     1.697074, Batch Acc: 0.603522, Tokens per Sec:     2774, Lr: 0.000300
2025-05-26 23:30:46,624 - INFO - joeynmt.training - Epoch   5, Step:    20800, Batch Loss:     1.524640, Batch Acc: 0.602792, Tokens per Sec:     2686, Lr: 0.000300
2025-05-26 23:31:14,859 - INFO - joeynmt.training - Epoch   5, Step:    20900, Batch Loss:     1.671793, Batch Acc: 0.603318, Tokens per Sec:     2666, Lr: 0.000300
2025-05-26 23:31:42,748 - INFO - joeynmt.training - Epoch   5, Step:    21000, Batch Loss:     1.398628, Batch Acc: 0.605718, Tokens per Sec:     2596, Lr: 0.000300
2025-05-26 23:31:42,748 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-26 23:34:00,748 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.47, ppl:   4.34, acc:   0.64, generation: 137.9927[sec], evaluation: 0.0000[sec]
2025-05-26 23:34:00,749 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-26 23:34:00,866 - INFO - joeynmt.helpers - delete models/bpe_8k/18500.ckpt
2025-05-26 23:34:00,885 - INFO - joeynmt.training - Example #0
2025-05-26 23:34:00,885 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-26 23:34:00,885 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-26 23:34:00,885 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'due', 'lu@@', '<unk>', '@', 'de', 'così', 'che', 'il', 'ghiaccio', 'che', 'il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ant@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stato', 'la', 'dimensione', 'del', '40', '%', '.', '</s>']
2025-05-26 23:34:00,885 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-26 23:34:00,885 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-26 23:34:00,885 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso ho mostrato queste due lu<unk> @ de così che il ghiaccio che il ghiaccio ar<unk> @ c<unk> @ ant<unk> @ ico , che per la maggior parte degli ultimi tre milioni di anni è stato la dimensione del 40 % .
2025-05-26 23:34:00,885 - INFO - joeynmt.training - Example #1
2025-05-26 23:34:00,885 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-26 23:34:00,885 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-26 23:34:00,885 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', '<unk>', '@', 'sce', 'la', 'seri@@', '<unk>', '@', 'ità', 'di', 'questo', 'problema', 'perché', 'non', 'mostra', 'la', 'prossima', 'parte', 'del', 'ghiaccio', '.', '</s>']
2025-05-26 23:34:00,885 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-26 23:34:00,885 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-26 23:34:00,886 - INFO - joeynmt.training - 	Hypothesis: Ma questo capi<unk> @ sce la seri<unk> @ ità di questo problema perché non mostra la prossima parte del ghiaccio .
2025-05-26 23:34:00,886 - INFO - joeynmt.training - Example #2
2025-05-26 23:34:00,886 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-26 23:34:00,886 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-26 23:34:00,886 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'di', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ant@@', '<unk>', '@', 'ico', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'globale', '.', '</s>']
2025-05-26 23:34:00,886 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-26 23:34:00,886 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-26 23:34:00,886 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio di ar<unk> @ c<unk> @ ant<unk> @ ico è , in un senso , il cuore del sistema globale .
2025-05-26 23:34:00,886 - INFO - joeynmt.training - Example #3
2025-05-26 23:34:00,886 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-26 23:34:00,886 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-26 23:34:00,886 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', '<unk>', '@', 'egu@@', '<unk>', '@', 'isce', 'in', 'inver@@', '<unk>', '@', 'no', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-26 23:34:00,886 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-26 23:34:00,886 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-26 23:34:00,886 - INFO - joeynmt.training - 	Hypothesis: Si es<unk> @ egu<unk> @ isce in inver<unk> @ no e contr<unk> @ atti in estate .
2025-05-26 23:34:00,886 - INFO - joeynmt.training - Example #4
2025-05-26 23:34:00,886 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-26 23:34:00,886 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-26 23:34:00,886 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'slide', 'vi', 'mostro', 'che', 'vi', 'mostrerò', 'un', 'rapi@@', '<unk>', '@', 'do', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-26 23:34:00,886 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-26 23:34:00,886 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-26 23:34:00,887 - INFO - joeynmt.training - 	Hypothesis: La prossima slide vi mostro che vi mostrerò un rapi<unk> @ do di quello che è successo negli ultimi 25 anni .
2025-05-26 23:34:28,647 - INFO - joeynmt.training - Epoch   5, Step:    21100, Batch Loss:     1.505659, Batch Acc: 0.605354, Tokens per Sec:     2610, Lr: 0.000300
2025-05-26 23:34:57,172 - INFO - joeynmt.training - Epoch   5, Step:    21200, Batch Loss:     1.401143, Batch Acc: 0.605586, Tokens per Sec:     2587, Lr: 0.000300
2025-05-26 23:35:25,178 - INFO - joeynmt.training - Epoch   5, Step:    21300, Batch Loss:     1.602444, Batch Acc: 0.607487, Tokens per Sec:     2585, Lr: 0.000300
2025-05-26 23:35:53,284 - INFO - joeynmt.training - Epoch   5, Step:    21400, Batch Loss:     1.662114, Batch Acc: 0.607064, Tokens per Sec:     2612, Lr: 0.000300
2025-05-26 23:36:21,462 - INFO - joeynmt.training - Epoch   5, Step:    21500, Batch Loss:     1.587482, Batch Acc: 0.604236, Tokens per Sec:     2544, Lr: 0.000300
2025-05-26 23:36:21,462 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-26 23:38:59,020 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.47, ppl:   4.34, acc:   0.64, generation: 157.5513[sec], evaluation: 0.0000[sec]
2025-05-26 23:38:59,022 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-26 23:38:59,171 - INFO - joeynmt.helpers - delete models/bpe_8k/19000.ckpt
2025-05-26 23:38:59,187 - INFO - joeynmt.training - Example #0
2025-05-26 23:38:59,187 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-26 23:38:59,187 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-26 23:38:59,187 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'due', 'fi@@', '<unk>', '@', 'ori', 'così', 'che', 'dimostr@@', '<unk>', '@', 'ano', 'che', 'il', 'ghiaccio', 'di', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ant@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'molti', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'del', '40', '%', '.', '</s>']
2025-05-26 23:38:59,187 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-26 23:38:59,187 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-26 23:38:59,187 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso ho mostrato queste due fi<unk> @ ori così che dimostr<unk> @ ano che il ghiaccio di ar<unk> @ c<unk> @ ant<unk> @ ico , che per molti degli ultimi tre milioni di anni è stata la dimensione del 40 % .
2025-05-26 23:38:59,187 - INFO - joeynmt.training - Example #1
2025-05-26 23:38:59,188 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-26 23:38:59,188 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-26 23:38:59,188 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', '<unk>', '@', 'sce', 'la', 'seri@@', '<unk>', '@', 'ità', 'di', 'questo', 'problema', 'perché', 'non', 'mostra', 'il', 'poll@@', '<unk>', '@', 'o', 'del', 'ghiaccio', '.', '</s>']
2025-05-26 23:38:59,188 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-26 23:38:59,188 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-26 23:38:59,188 - INFO - joeynmt.training - 	Hypothesis: Ma questo capi<unk> @ sce la seri<unk> @ ità di questo problema perché non mostra il poll<unk> @ o del ghiaccio .
2025-05-26 23:38:59,188 - INFO - joeynmt.training - Example #2
2025-05-26 23:38:59,188 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-26 23:38:59,188 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-26 23:38:59,188 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'di', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ant@@', '<unk>', '@', 'ico', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'globale', '.', '</s>']
2025-05-26 23:38:59,188 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-26 23:38:59,188 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-26 23:38:59,188 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio di ar<unk> @ c<unk> @ ant<unk> @ ico è , in un senso , il cuore del sistema globale .
2025-05-26 23:38:59,188 - INFO - joeynmt.training - Example #3
2025-05-26 23:38:59,188 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-26 23:38:59,188 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-26 23:38:59,188 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'tratta', 'di', 'es@@', '<unk>', '@', 'amin@@', '<unk>', '@', 'are', 'in', 'inver@@', '<unk>', '@', 'no', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-26 23:38:59,188 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-26 23:38:59,188 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-26 23:38:59,188 - INFO - joeynmt.training - 	Hypothesis: Si tratta di es<unk> @ amin<unk> @ are in inver<unk> @ no e contr<unk> @ atti in estate .
2025-05-26 23:38:59,188 - INFO - joeynmt.training - Example #4
2025-05-26 23:38:59,188 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-26 23:38:59,188 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-26 23:38:59,189 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'slide', 'vi', 'mostro', 'un', 'rapi@@', '<unk>', '@', 'do', 'rapi@@', '<unk>', '@', 'do', 'di', 'f@@', '<unk>', '@', 'ast@@', '<unk>', '@', 'ante', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-26 23:38:59,189 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-26 23:38:59,189 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-26 23:38:59,189 - INFO - joeynmt.training - 	Hypothesis: La prossima slide vi mostro un rapi<unk> @ do rapi<unk> @ do di f<unk> @ ast<unk> @ ante di quello che è successo negli ultimi 25 anni .
2025-05-26 23:39:26,951 - INFO - joeynmt.training - Epoch   5, Step:    21600, Batch Loss:     1.621736, Batch Acc: 0.606560, Tokens per Sec:     2660, Lr: 0.000300
2025-05-26 23:39:54,978 - INFO - joeynmt.training - Epoch   5, Step:    21700, Batch Loss:     1.571192, Batch Acc: 0.607944, Tokens per Sec:     2630, Lr: 0.000300
2025-05-26 23:40:22,220 - INFO - joeynmt.training - Epoch   5, Step:    21800, Batch Loss:     1.643114, Batch Acc: 0.604448, Tokens per Sec:     2714, Lr: 0.000300
2025-05-26 23:40:50,322 - INFO - joeynmt.training - Epoch   5, Step:    21900, Batch Loss:     1.743469, Batch Acc: 0.603818, Tokens per Sec:     2608, Lr: 0.000300
2025-05-26 23:41:18,194 - INFO - joeynmt.training - Epoch   5, Step:    22000, Batch Loss:     1.472662, Batch Acc: 0.600889, Tokens per Sec:     2681, Lr: 0.000300
2025-05-26 23:41:18,195 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-26 23:43:49,251 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.46, ppl:   4.32, acc:   0.64, generation: 151.0488[sec], evaluation: 0.0000[sec]
2025-05-26 23:43:49,252 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-26 23:43:49,371 - INFO - joeynmt.helpers - delete models/bpe_8k/20000.ckpt
2025-05-26 23:43:49,387 - INFO - joeynmt.training - Example #0
2025-05-26 23:43:49,387 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-26 23:43:49,387 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-26 23:43:49,387 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'due', 'fi@@', '<unk>', '@', 'di', 'che', 'il', 'ghiaccio', 'di', 'due', 'fi@@', '<unk>', '@', 'chi', 'che', 'il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ant@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'molti', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'dei', '4@@', '<unk>', '@', '8', 'stati', ',', 'ha', 'fatto', 'il', '40', '%', '.', '</s>']
2025-05-26 23:43:49,387 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-26 23:43:49,387 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-26 23:43:49,387 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso ho mostrato queste due fi<unk> @ di che il ghiaccio di due fi<unk> @ chi che il ghiaccio ar<unk> @ c<unk> @ ant<unk> @ ico , che per molti degli ultimi tre milioni di anni è stata la dimensione degli ultimi tre milioni di anni è stata la dimensione dei 4<unk> @ 8 stati , ha fatto il 40 % .
2025-05-26 23:43:49,387 - INFO - joeynmt.training - Example #1
2025-05-26 23:43:49,387 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-26 23:43:49,387 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-26 23:43:49,387 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', '<unk>', '@', 'sce', 'la', 'seri@@', '<unk>', '@', 'ità', 'di', 'questo', 'problema', 'perché', 'non', 'mostrano', 'la', 'th@@', '<unk>', '@', 'ick@@', '<unk>', '@', 'zza', 'del', 'ghiaccio', '.', '</s>']
2025-05-26 23:43:49,388 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-26 23:43:49,388 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-26 23:43:49,388 - INFO - joeynmt.training - 	Hypothesis: Ma questo capi<unk> @ sce la seri<unk> @ ità di questo problema perché non mostrano la th<unk> @ ick<unk> @ zza del ghiaccio .
2025-05-26 23:43:49,388 - INFO - joeynmt.training - Example #2
2025-05-26 23:43:49,388 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-26 23:43:49,388 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-26 23:43:49,388 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ant@@', '<unk>', '@', 'ico', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'del', 'sistema', 'del', 'clima', '.', '</s>']
2025-05-26 23:43:49,388 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-26 23:43:49,388 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-26 23:43:49,388 - INFO - joeynmt.training - 	Hypothesis: L' ar<unk> @ c<unk> @ ant<unk> @ ico è , in un senso , il cuore del sistema del sistema del clima .
2025-05-26 23:43:49,388 - INFO - joeynmt.training - Example #3
2025-05-26 23:43:49,388 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-26 23:43:49,388 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-26 23:43:49,388 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', '<unk>', '@', 'pon@@', '<unk>', '@', 'gono', 'in', 'inver@@', '<unk>', '@', 'no', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-26 23:43:49,388 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-26 23:43:49,388 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-26 23:43:49,388 - INFO - joeynmt.training - 	Hypothesis: Si es<unk> @ pon<unk> @ gono in inver<unk> @ no e contr<unk> @ atti in estate .
2025-05-26 23:43:49,388 - INFO - joeynmt.training - Example #4
2025-05-26 23:43:49,388 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-26 23:43:49,388 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-26 23:43:49,388 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'slide', 'vi', 'mostro', 'che', 'vi', 'sarà', 'un', 'rapi@@', '<unk>', '@', 'do', 'rapidamente', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-26 23:43:49,388 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-26 23:43:49,388 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-26 23:43:49,388 - INFO - joeynmt.training - 	Hypothesis: La prossima slide vi mostro che vi sarà un rapi<unk> @ do rapidamente di quello che è successo negli ultimi 25 anni .
2025-05-26 23:44:17,025 - INFO - joeynmt.training - Epoch   5, Step:    22100, Batch Loss:     1.434517, Batch Acc: 0.607442, Tokens per Sec:     2568, Lr: 0.000300
2025-05-26 23:44:45,574 - INFO - joeynmt.training - Epoch   5, Step:    22200, Batch Loss:     1.493188, Batch Acc: 0.606163, Tokens per Sec:     2652, Lr: 0.000300
2025-05-26 23:45:12,820 - INFO - joeynmt.training - Epoch   5, Step:    22300, Batch Loss:     1.416095, Batch Acc: 0.607656, Tokens per Sec:     2689, Lr: 0.000300
2025-05-26 23:45:41,699 - INFO - joeynmt.training - Epoch   5, Step:    22400, Batch Loss:     1.557879, Batch Acc: 0.607680, Tokens per Sec:     2558, Lr: 0.000300
2025-05-26 23:46:09,956 - INFO - joeynmt.training - Epoch   5, Step:    22500, Batch Loss:     1.511084, Batch Acc: 0.605089, Tokens per Sec:     2605, Lr: 0.000300
2025-05-26 23:46:09,956 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-26 23:48:35,163 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.45, ppl:   4.28, acc:   0.64, generation: 145.1998[sec], evaluation: 0.0000[sec]
2025-05-26 23:48:35,165 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-26 23:48:35,281 - INFO - joeynmt.helpers - delete models/bpe_8k/19500.ckpt
2025-05-26 23:48:35,299 - INFO - joeynmt.training - Example #0
2025-05-26 23:48:35,299 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-26 23:48:35,299 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-26 23:48:35,299 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'due', 'fi@@', '<unk>', '@', 'lu@@', '<unk>', '@', 'di', 'così', 'che', 'dimostr@@', '<unk>', '@', 'are', 'che', 'il', 'ghiaccio', "dell'", 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ant@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'più', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'dei', '4@@', '<unk>', '@', '8', 'stati', ',', 'ha', 'ri@@', '<unk>', '@', 'dotto', 'dal', '40', '%', '.', '</s>']
2025-05-26 23:48:35,299 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-26 23:48:35,299 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-26 23:48:35,299 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso ho mostrato queste due fi<unk> @ lu<unk> @ di così che dimostr<unk> @ are che il ghiaccio dell' ar<unk> @ c<unk> @ ant<unk> @ ico , che per più degli ultimi tre milioni di anni è stata la dimensione dei 4<unk> @ 8 stati , ha ri<unk> @ dotto dal 40 % .
2025-05-26 23:48:35,299 - INFO - joeynmt.training - Example #1
2025-05-26 23:48:35,299 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-26 23:48:35,299 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-26 23:48:35,299 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', '<unk>', '@', 'sce', 'la', 'seri@@', '<unk>', '@', 'età', 'del', 'problema', 'perché', 'non', 'mostra', 'il', 'problema', 'perché', 'non', 'mostr@@', '<unk>', '@', 'ano', 'il', 'poll@@', '<unk>', '@', 'o', 'del', 'ghiaccio', '.', '</s>']
2025-05-26 23:48:35,300 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-26 23:48:35,300 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-26 23:48:35,300 - INFO - joeynmt.training - 	Hypothesis: Ma questo capi<unk> @ sce la seri<unk> @ età del problema perché non mostra il problema perché non mostr<unk> @ ano il poll<unk> @ o del ghiaccio .
2025-05-26 23:48:35,300 - INFO - joeynmt.training - Example #2
2025-05-26 23:48:35,300 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-26 23:48:35,300 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-26 23:48:35,300 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'del', 'sistema', 'globale', '.', '</s>']
2025-05-26 23:48:35,300 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-26 23:48:35,300 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-26 23:48:35,300 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio ar<unk> @ c<unk> @ ico è , in un senso , il cuore del sistema del sistema globale .
2025-05-26 23:48:35,300 - INFO - joeynmt.training - Example #3
2025-05-26 23:48:35,300 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-26 23:48:35,300 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-26 23:48:35,300 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'tratta', 'di', 'inver@@', '<unk>', '@', 'no', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-26 23:48:35,300 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-26 23:48:35,300 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-26 23:48:35,300 - INFO - joeynmt.training - 	Hypothesis: Si tratta di inver<unk> @ no e contr<unk> @ atti in estate .
2025-05-26 23:48:35,300 - INFO - joeynmt.training - Example #4
2025-05-26 23:48:35,300 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-26 23:48:35,300 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-26 23:48:35,300 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'slide', 'vi', 'mostro', 'che', 'vi', 'mostrerò', 'un', 'rapi@@', '<unk>', '@', 'do', 'di', 'rapi@@', '<unk>', '@', 'do', 'di', 'ciò', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-26 23:48:35,300 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-26 23:48:35,300 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-26 23:48:35,301 - INFO - joeynmt.training - 	Hypothesis: La prossima slide vi mostro che vi mostrerò un rapi<unk> @ do di rapi<unk> @ do di ciò che è successo negli ultimi 25 anni .
2025-05-26 23:49:03,481 - INFO - joeynmt.training - Epoch   5, Step:    22600, Batch Loss:     1.584391, Batch Acc: 0.607811, Tokens per Sec:     2626, Lr: 0.000300
2025-05-26 23:49:31,034 - INFO - joeynmt.training - Epoch   5, Step:    22700, Batch Loss:     1.663049, Batch Acc: 0.604588, Tokens per Sec:     2752, Lr: 0.000300
2025-05-26 23:49:57,885 - INFO - joeynmt.training - Epoch   5, Step:    22800, Batch Loss:     1.576699, Batch Acc: 0.607033, Tokens per Sec:     2733, Lr: 0.000300
2025-05-26 23:50:25,896 - INFO - joeynmt.training - Epoch   5, Step:    22900, Batch Loss:     1.624318, Batch Acc: 0.608918, Tokens per Sec:     2573, Lr: 0.000300
2025-05-26 23:50:54,214 - INFO - joeynmt.training - Epoch   5, Step:    23000, Batch Loss:     1.624083, Batch Acc: 0.607561, Tokens per Sec:     2581, Lr: 0.000300
2025-05-26 23:50:54,214 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-26 23:53:24,530 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.44, ppl:   4.23, acc:   0.64, generation: 150.3082[sec], evaluation: 0.0000[sec]
2025-05-26 23:53:24,532 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-26 23:53:24,649 - INFO - joeynmt.helpers - delete models/bpe_8k/20500.ckpt
2025-05-26 23:53:24,664 - INFO - joeynmt.training - Example #0
2025-05-26 23:53:24,664 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-26 23:53:24,664 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-26 23:53:24,664 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'due', 'lu@@', '<unk>', '@', 'ci', 'che', 'dimostr@@', '<unk>', '@', 'ano', 'che', 'il', 'ghiaccio', 'del', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'molti', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'dei', 'più', 'più', 'di', '4@@', '<unk>', '@', '8', 'stati', ',', 'ha', 'ri@@', '<unk>', '@', 'dotto', 'il', '40', '%', '.', '</s>']
2025-05-26 23:53:24,665 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-26 23:53:24,665 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-26 23:53:24,665 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso ho mostrato queste due lu<unk> @ ci che dimostr<unk> @ ano che il ghiaccio del ghiaccio ar<unk> @ c<unk> @ ico , che per molti degli ultimi tre milioni di anni è stata la dimensione dei più più di 4<unk> @ 8 stati , ha ri<unk> @ dotto il 40 % .
2025-05-26 23:53:24,665 - INFO - joeynmt.training - Example #1
2025-05-26 23:53:24,665 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-26 23:53:24,665 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-26 23:53:24,665 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', '<unk>', '@', 'sce', 'la', 'seri@@', '<unk>', '@', 'ità', 'di', 'questo', 'particolare', 'problema', 'perché', 'non', 'mostra', 'il', 'poll@@', '<unk>', '@', 'o', 'del', 'ghiaccio', '.', '</s>']
2025-05-26 23:53:24,665 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-26 23:53:24,665 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-26 23:53:24,665 - INFO - joeynmt.training - 	Hypothesis: Ma questo capi<unk> @ sce la seri<unk> @ ità di questo particolare problema perché non mostra il poll<unk> @ o del ghiaccio .
2025-05-26 23:53:24,665 - INFO - joeynmt.training - Example #2
2025-05-26 23:53:24,665 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-26 23:53:24,665 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-26 23:53:24,665 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'del', 'sistema', 'climatico', '.', '</s>']
2025-05-26 23:53:24,665 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-26 23:53:24,665 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-26 23:53:24,665 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio ar<unk> @ c<unk> @ ico è , in un senso , il cuore del sistema del sistema climatico .
2025-05-26 23:53:24,665 - INFO - joeynmt.training - Example #3
2025-05-26 23:53:24,665 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-26 23:53:24,665 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-26 23:53:24,665 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'tratta', 'di', 'v@@', '<unk>', '@', 'uo@@', '<unk>', '@', 'to', 'in', 'inver@@', '<unk>', '@', 'no', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-26 23:53:24,666 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-26 23:53:24,666 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-26 23:53:24,666 - INFO - joeynmt.training - 	Hypothesis: Si tratta di v<unk> @ uo<unk> @ to in inver<unk> @ no e contr<unk> @ atti in estate .
2025-05-26 23:53:24,666 - INFO - joeynmt.training - Example #4
2025-05-26 23:53:24,666 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-26 23:53:24,666 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-26 23:53:24,666 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'slide', 'vi', 'mostrerò', 'che', 'vi', 'mostrerò', 'un', 'rapi@@', '<unk>', '@', 'do', 'veloce', 'di', 'un', 'rapi@@', '<unk>', '@', 'do', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-26 23:53:24,666 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-26 23:53:24,666 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-26 23:53:24,666 - INFO - joeynmt.training - 	Hypothesis: La prossima slide vi mostrerò che vi mostrerò un rapi<unk> @ do veloce di un rapi<unk> @ do di quello che è successo negli ultimi 25 anni .
2025-05-26 23:53:52,236 - INFO - joeynmt.training - Epoch   5, Step:    23100, Batch Loss:     1.718876, Batch Acc: 0.609367, Tokens per Sec:     2618, Lr: 0.000300
2025-05-26 23:54:20,490 - INFO - joeynmt.training - Epoch   5, Step:    23200, Batch Loss:     1.583382, Batch Acc: 0.602851, Tokens per Sec:     2617, Lr: 0.000300
2025-05-26 23:54:47,862 - INFO - joeynmt.training - Epoch   5, Step:    23300, Batch Loss:     1.517007, Batch Acc: 0.611085, Tokens per Sec:     2693, Lr: 0.000300
2025-05-26 23:55:15,377 - INFO - joeynmt.training - Epoch   5, Step:    23400, Batch Loss:     1.393416, Batch Acc: 0.608624, Tokens per Sec:     2692, Lr: 0.000300
2025-05-26 23:55:43,343 - INFO - joeynmt.training - Epoch   5, Step:    23500, Batch Loss:     1.575982, Batch Acc: 0.609356, Tokens per Sec:     2691, Lr: 0.000300
2025-05-26 23:55:43,343 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-26 23:58:22,586 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.43, ppl:   4.20, acc:   0.64, generation: 159.2351[sec], evaluation: 0.0000[sec]
2025-05-26 23:58:22,588 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-26 23:58:22,709 - INFO - joeynmt.helpers - delete models/bpe_8k/21000.ckpt
2025-05-26 23:58:22,716 - INFO - joeynmt.training - Example #0
2025-05-26 23:58:22,717 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-26 23:58:22,717 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-26 23:58:22,717 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'due', 'di@@', '<unk>', '@', 'ap@@', '<unk>', '@', 'di@@', '<unk>', '@', 'ap@@', '<unk>', '@', 're', 'così', 'che', 'il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ant@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'dei', 'più', 'più', 'basso', 'da', '40', '%', '.', '</s>']
2025-05-26 23:58:22,717 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-26 23:58:22,717 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-26 23:58:22,717 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso ho mostrato queste due di<unk> @ ap<unk> @ di<unk> @ ap<unk> @ re così che il ghiaccio ar<unk> @ c<unk> @ ant<unk> @ ico , che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione dei più più basso da 40 % .
2025-05-26 23:58:22,717 - INFO - joeynmt.training - Example #1
2025-05-26 23:58:22,717 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-26 23:58:22,717 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-26 23:58:22,717 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'aspetto', 'capi@@', '<unk>', '@', 'sce', 'la', 'seri@@', '<unk>', '@', 'ità', 'di', 'questo', 'problema', 'perché', 'non', 'mostra', 'il', 'poll@@', '<unk>', '@', 'o', 'del', 'ghiaccio', '.', '</s>']
2025-05-26 23:58:22,717 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-26 23:58:22,717 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-26 23:58:22,717 - INFO - joeynmt.training - 	Hypothesis: Ma questo aspetto capi<unk> @ sce la seri<unk> @ ità di questo problema perché non mostra il poll<unk> @ o del ghiaccio .
2025-05-26 23:58:22,718 - INFO - joeynmt.training - Example #2
2025-05-26 23:58:22,718 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-26 23:58:22,718 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-26 23:58:22,718 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ant@@', '<unk>', '@', 'ico', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'globale', '.', '</s>']
2025-05-26 23:58:22,718 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-26 23:58:22,718 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-26 23:58:22,718 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio ar<unk> @ c<unk> @ ant<unk> @ ico è , in un senso , il cuore del sistema globale .
2025-05-26 23:58:22,718 - INFO - joeynmt.training - Example #3
2025-05-26 23:58:22,718 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-26 23:58:22,718 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-26 23:58:22,718 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', '<unk>', '@', 'pan@@', '<unk>', '@', 'di', 'in', 'v@@', '<unk>', '@', 'etro', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-26 23:58:22,718 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-26 23:58:22,718 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-26 23:58:22,718 - INFO - joeynmt.training - 	Hypothesis: Si es<unk> @ pan<unk> @ di in v<unk> @ etro e contr<unk> @ atti in estate .
2025-05-26 23:58:22,718 - INFO - joeynmt.training - Example #4
2025-05-26 23:58:22,718 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-26 23:58:22,718 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-26 23:58:22,718 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'slide', 'vi', 'mostrerò', 'un', 'rapi@@', '<unk>', '@', 'do', 'rapidamente', 'di', 'un', 'rapi@@', '<unk>', '@', 'do', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-26 23:58:22,718 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-26 23:58:22,718 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-26 23:58:22,718 - INFO - joeynmt.training - 	Hypothesis: La prossima slide vi mostrerò un rapi<unk> @ do rapidamente di un rapi<unk> @ do di quello che è successo negli ultimi 25 anni .
2025-05-26 23:58:50,591 - INFO - joeynmt.training - Epoch   5, Step:    23600, Batch Loss:     1.579760, Batch Acc: 0.610608, Tokens per Sec:     2616, Lr: 0.000300
2025-05-26 23:59:18,884 - INFO - joeynmt.training - Epoch   5, Step:    23700, Batch Loss:     1.565604, Batch Acc: 0.609343, Tokens per Sec:     2541, Lr: 0.000300
2025-05-26 23:59:46,934 - INFO - joeynmt.training - Epoch   5, Step:    23800, Batch Loss:     1.375477, Batch Acc: 0.611121, Tokens per Sec:     2558, Lr: 0.000300
2025-05-27 00:00:14,992 - INFO - joeynmt.training - Epoch   5, Step:    23900, Batch Loss:     1.405617, Batch Acc: 0.611530, Tokens per Sec:     2671, Lr: 0.000300
2025-05-27 00:00:42,490 - INFO - joeynmt.training - Epoch   5, Step:    24000, Batch Loss:     1.428561, Batch Acc: 0.611031, Tokens per Sec:     2631, Lr: 0.000300
2025-05-27 00:00:42,490 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 00:03:16,414 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.44, ppl:   4.22, acc:   0.64, generation: 153.9162[sec], evaluation: 0.0000[sec]
2025-05-27 00:03:16,532 - INFO - joeynmt.helpers - delete models/bpe_8k/21500.ckpt
2025-05-27 00:03:16,540 - INFO - joeynmt.training - Example #0
2025-05-27 00:03:16,540 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-27 00:03:16,540 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-27 00:03:16,540 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'due', 'l@@', '<unk>', '@', 'ap@@', '<unk>', '@', 're', 'così', 'che', 'dimostr@@', '<unk>', '@', 'ano', 'che', 'il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'del', '40', '%', '.', '</s>']
2025-05-27 00:03:16,541 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-27 00:03:16,541 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-27 00:03:16,541 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso ho mostrato queste due l<unk> @ ap<unk> @ re così che dimostr<unk> @ ano che il ghiaccio ar<unk> @ c<unk> @ ico , che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione del 40 % .
2025-05-27 00:03:16,541 - INFO - joeynmt.training - Example #1
2025-05-27 00:03:16,541 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-27 00:03:16,541 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-27 00:03:16,541 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'compren@@', '<unk>', '@', 'de', 'la', 'seri@@', '<unk>', '@', 'età', 'di', 'questo', 'particolare', 'problema', 'perché', 'non', 'mostra', 'la', 'mostra', 'la', 'th@@', '<unk>', '@', 'ick@@', '<unk>', '@', 'zza', 'del', 'ghiaccio', '.', '</s>']
2025-05-27 00:03:16,541 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-27 00:03:16,541 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-27 00:03:16,541 - INFO - joeynmt.training - 	Hypothesis: Ma questo compren<unk> @ de la seri<unk> @ età di questo particolare problema perché non mostra la mostra la th<unk> @ ick<unk> @ zza del ghiaccio .
2025-05-27 00:03:16,541 - INFO - joeynmt.training - Example #2
2025-05-27 00:03:16,541 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-27 00:03:16,541 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-27 00:03:16,541 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ant@@', '<unk>', '@', 'ico', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'del', 'sistema', 'globale', '.', '</s>']
2025-05-27 00:03:16,541 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-27 00:03:16,541 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-27 00:03:16,541 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio ar<unk> @ c<unk> @ ant<unk> @ ico è , in un senso , il cuore del sistema del sistema globale .
2025-05-27 00:03:16,542 - INFO - joeynmt.training - Example #3
2025-05-27 00:03:16,542 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-27 00:03:16,542 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-27 00:03:16,542 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', 'v@@', '<unk>', '@', 'asi', 'in', 'v@@', '<unk>', '@', 'etro', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-27 00:03:16,542 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-27 00:03:16,542 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-27 00:03:16,542 - INFO - joeynmt.training - 	Hypothesis: I v<unk> @ asi in v<unk> @ etro e contr<unk> @ atti in estate .
2025-05-27 00:03:16,542 - INFO - joeynmt.training - Example #4
2025-05-27 00:03:16,542 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-27 00:03:16,542 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-27 00:03:16,542 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'slide', 'vi', 'mostrerò', 'una', 'rapi@@', '<unk>', '@', 'da', 'rapi@@', '<unk>', '@', 'do', 'di', 'rapi@@', '<unk>', '@', 'do', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-27 00:03:16,542 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-27 00:03:16,542 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-27 00:03:16,542 - INFO - joeynmt.training - 	Hypothesis: La prossima slide vi mostrerò una rapi<unk> @ da rapi<unk> @ do di rapi<unk> @ do di quello che è successo negli ultimi 25 anni .
2025-05-27 00:03:44,093 - INFO - joeynmt.training - Epoch   5, Step:    24100, Batch Loss:     1.514441, Batch Acc: 0.608461, Tokens per Sec:     2608, Lr: 0.000300
2025-05-27 00:04:12,047 - INFO - joeynmt.training - Epoch   5, Step:    24200, Batch Loss:     1.527833, Batch Acc: 0.608660, Tokens per Sec:     2678, Lr: 0.000300
2025-05-27 00:04:41,119 - INFO - joeynmt.training - Epoch   5, Step:    24300, Batch Loss:     1.415028, Batch Acc: 0.609236, Tokens per Sec:     2597, Lr: 0.000300
2025-05-27 00:05:08,401 - INFO - joeynmt.training - Epoch   5, Step:    24400, Batch Loss:     1.533829, Batch Acc: 0.606885, Tokens per Sec:     2660, Lr: 0.000300
2025-05-27 00:05:36,105 - INFO - joeynmt.training - Epoch   5, Step:    24500, Batch Loss:     1.403789, Batch Acc: 0.607932, Tokens per Sec:     2648, Lr: 0.000300
2025-05-27 00:05:36,105 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 00:07:52,704 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.43, ppl:   4.17, acc:   0.64, generation: 136.5916[sec], evaluation: 0.0000[sec]
2025-05-27 00:07:52,705 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 00:07:52,825 - INFO - joeynmt.helpers - delete models/bpe_8k/22000.ckpt
2025-05-27 00:07:52,830 - INFO - joeynmt.training - Example #0
2025-05-27 00:07:52,830 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-27 00:07:52,830 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-27 00:07:52,830 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'due', 'slide', 'così', 'che', 'dimostr@@', '<unk>', '@', 'ano', 'che', 'il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ant@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'del', '40', '%', '.', '</s>']
2025-05-27 00:07:52,830 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-27 00:07:52,830 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-27 00:07:52,830 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso ho mostrato queste due slide così che dimostr<unk> @ ano che il ghiaccio ar<unk> @ c<unk> @ ant<unk> @ ico , che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione degli ultimi tre milioni di anni è stata la dimensione degli ultimi tre milioni di anni è stata la dimensione del 40 % .
2025-05-27 00:07:52,830 - INFO - joeynmt.training - Example #1
2025-05-27 00:07:52,830 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-27 00:07:52,831 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-27 00:07:52,831 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', '<unk>', '@', 'sce', 'la', 'seri@@', '<unk>', '@', 'età', 'di', 'questo', 'particolare', 'problema', 'perché', 'non', 'mostra', 'la', 'prossima', 'velocità', 'del', 'ghiaccio', '.', '</s>']
2025-05-27 00:07:52,831 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-27 00:07:52,831 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-27 00:07:52,831 - INFO - joeynmt.training - 	Hypothesis: Ma questo capi<unk> @ sce la seri<unk> @ età di questo particolare problema perché non mostra la prossima velocità del ghiaccio .
2025-05-27 00:07:52,831 - INFO - joeynmt.training - Example #2
2025-05-27 00:07:52,831 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-27 00:07:52,831 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-27 00:07:52,831 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ant@@', '<unk>', '@', 'ico', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'del', 'sistema', 'climatico', '.', '</s>']
2025-05-27 00:07:52,831 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-27 00:07:52,831 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-27 00:07:52,831 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio ar<unk> @ c<unk> @ ant<unk> @ ico è , in un senso , il cuore del sistema del sistema climatico .
2025-05-27 00:07:52,831 - INFO - joeynmt.training - Example #3
2025-05-27 00:07:52,831 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-27 00:07:52,831 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-27 00:07:52,831 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', 'es@@', '<unk>', '@', 'pan@@', '<unk>', '@', 'di', 'v@@', '<unk>', '@', 'etro', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-27 00:07:52,831 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-27 00:07:52,832 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-27 00:07:52,832 - INFO - joeynmt.training - 	Hypothesis: I es<unk> @ pan<unk> @ di v<unk> @ etro e contr<unk> @ atti in estate .
2025-05-27 00:07:52,832 - INFO - joeynmt.training - Example #4
2025-05-27 00:07:52,832 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-27 00:07:52,832 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-27 00:07:52,832 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'slide', 'che', 'vi', 'mostrerò', 'un', 'rapi@@', '<unk>', '@', 'do', 'rapidamente', ',', 'vi', 'mostrerò', 'un', 'passo', 'avanti', 'di', 'ciò', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-27 00:07:52,832 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-27 00:07:52,832 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-27 00:07:52,832 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerò un rapi<unk> @ do rapidamente , vi mostrerò un passo avanti di ciò che è successo negli ultimi 25 anni .
2025-05-27 00:08:21,407 - INFO - joeynmt.training - Epoch   5, Step:    24600, Batch Loss:     1.504901, Batch Acc: 0.606045, Tokens per Sec:     2595, Lr: 0.000300
2025-05-27 00:08:48,490 - INFO - joeynmt.training - Epoch   5, Step:    24700, Batch Loss:     1.509127, Batch Acc: 0.610898, Tokens per Sec:     2693, Lr: 0.000300
2025-05-27 00:09:18,264 - INFO - joeynmt.training - Epoch   5, Step:    24800, Batch Loss:     1.605564, Batch Acc: 0.610772, Tokens per Sec:     2496, Lr: 0.000300
2025-05-27 00:09:46,628 - INFO - joeynmt.training - Epoch   5, Step:    24900, Batch Loss:     1.441106, Batch Acc: 0.613548, Tokens per Sec:     2525, Lr: 0.000300
2025-05-27 00:10:02,672 - INFO - joeynmt.training - Epoch   5: total training loss 7744.08
2025-05-27 00:10:02,672 - INFO - joeynmt.training - EPOCH 6
2025-05-27 00:10:14,045 - INFO - joeynmt.training - Epoch   6, Step:    25000, Batch Loss:     1.449636, Batch Acc: 0.627022, Tokens per Sec:     2713, Lr: 0.000300
2025-05-27 00:10:14,045 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 00:12:49,680 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.43, ppl:   4.18, acc:   0.64, generation: 155.6268[sec], evaluation: 0.0000[sec]
2025-05-27 00:12:49,796 - INFO - joeynmt.helpers - delete models/bpe_8k/22500.ckpt
2025-05-27 00:12:49,804 - INFO - joeynmt.training - Example #0
2025-05-27 00:12:49,805 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-27 00:12:49,805 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-27 00:12:49,805 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'due', 'l@@', '<unk>', '@', 'ap@@', '<unk>', '@', 'di@@', '<unk>', '@', 'de', 'così', 'che', 'dimostr@@', '<unk>', '@', 'ano', 'che', 'il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stato', 'la', 'dimensione', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stato', 'la', 'dimensione', 'del', '40', '%', '.', '</s>']
2025-05-27 00:12:49,805 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-27 00:12:49,805 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-27 00:12:49,805 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso ho mostrato queste due l<unk> @ ap<unk> @ di<unk> @ de così che dimostr<unk> @ ano che il ghiaccio ar<unk> @ c<unk> @ ico , che per maggior parte degli ultimi tre milioni di anni è stato la dimensione degli ultimi tre milioni di anni è stato la dimensione del 40 % .
2025-05-27 00:12:49,805 - INFO - joeynmt.training - Example #1
2025-05-27 00:12:49,805 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-27 00:12:49,805 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-27 00:12:49,805 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', '<unk>', '@', 'sce', 'la', 'seri@@', '<unk>', '@', 'età', 'di', 'questo', 'problema', 'perché', 'non', 'mostra', 'la', 'tr@@', '<unk>', '@', 'ama', 'del', 'ghiaccio', '.', '</s>']
2025-05-27 00:12:49,805 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-27 00:12:49,805 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-27 00:12:49,805 - INFO - joeynmt.training - 	Hypothesis: Ma questo capi<unk> @ sce la seri<unk> @ età di questo problema perché non mostra la tr<unk> @ ama del ghiaccio .
2025-05-27 00:12:49,805 - INFO - joeynmt.training - Example #2
2025-05-27 00:12:49,805 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-27 00:12:49,805 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-27 00:12:49,806 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ant@@', '<unk>', '@', 'ico', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'del', 'sistema', 'climatico', '.', '</s>']
2025-05-27 00:12:49,806 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-27 00:12:49,806 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-27 00:12:49,806 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio ar<unk> @ c<unk> @ ant<unk> @ ico è , in un senso , il cuore del sistema del sistema climatico .
2025-05-27 00:12:49,806 - INFO - joeynmt.training - Example #3
2025-05-27 00:12:49,806 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-27 00:12:49,806 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-27 00:12:49,806 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Le', 'es@@', '<unk>', '@', 'amin@@', '<unk>', '@', 'ate', 'in', 'v@@', '<unk>', '@', 'etro', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-27 00:12:49,806 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-27 00:12:49,806 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-27 00:12:49,806 - INFO - joeynmt.training - 	Hypothesis: Le es<unk> @ amin<unk> @ ate in v<unk> @ etro e contr<unk> @ atti in estate .
2025-05-27 00:12:49,806 - INFO - joeynmt.training - Example #4
2025-05-27 00:12:49,806 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-27 00:12:49,806 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-27 00:12:49,806 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'slide', 'vi', 'mostrerò', 'che', 'sarà', 'un', 'rapi@@', '<unk>', '@', 'do', 'rapidamente', 'di', 'un', 'rapi@@', '<unk>', '@', 'do', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-27 00:12:49,806 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-27 00:12:49,806 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-27 00:12:49,806 - INFO - joeynmt.training - 	Hypothesis: La prossima slide vi mostrerò che sarà un rapi<unk> @ do rapidamente di un rapi<unk> @ do di quello che è successo negli ultimi 25 anni .
2025-05-27 00:13:18,056 - INFO - joeynmt.training - Epoch   6, Step:    25100, Batch Loss:     1.652163, Batch Acc: 0.616672, Tokens per Sec:     2559, Lr: 0.000300
2025-05-27 00:13:45,495 - INFO - joeynmt.training - Epoch   6, Step:    25200, Batch Loss:     1.451448, Batch Acc: 0.616401, Tokens per Sec:     2665, Lr: 0.000300
2025-05-27 00:14:12,423 - INFO - joeynmt.training - Epoch   6, Step:    25300, Batch Loss:     1.504511, Batch Acc: 0.618419, Tokens per Sec:     2735, Lr: 0.000300
2025-05-27 00:14:40,635 - INFO - joeynmt.training - Epoch   6, Step:    25400, Batch Loss:     1.372996, Batch Acc: 0.618206, Tokens per Sec:     2538, Lr: 0.000300
2025-05-27 00:15:07,903 - INFO - joeynmt.training - Epoch   6, Step:    25500, Batch Loss:     1.527657, Batch Acc: 0.615935, Tokens per Sec:     2711, Lr: 0.000300
2025-05-27 00:15:07,903 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 00:17:19,921 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.42, ppl:   4.12, acc:   0.65, generation: 132.0100[sec], evaluation: 0.0000[sec]
2025-05-27 00:17:19,923 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 00:17:20,040 - INFO - joeynmt.helpers - delete models/bpe_8k/23000.ckpt
2025-05-27 00:17:20,044 - INFO - joeynmt.training - Example #0
2025-05-27 00:17:20,044 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-27 00:17:20,044 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-27 00:17:20,044 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'due', 'di@@', '<unk>', '@', 'ap@@', '<unk>', '@', 'os@@', '<unk>', '@', 'sa', 'così', 'che', 'il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'molti', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'di', 'più', 'di', '4@@', '<unk>', '@', '8', 'stati', ',', 'ha', 'ri@@', '<unk>', '@', 'gi@@', '<unk>', '@', 'sta', 'da', '40', '%', '.', '</s>']
2025-05-27 00:17:20,045 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-27 00:17:20,045 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-27 00:17:20,045 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso ho mostrato queste due di<unk> @ ap<unk> @ os<unk> @ sa così che il ghiaccio ar<unk> @ c<unk> @ ico , che per molti degli ultimi tre milioni di anni è stata la dimensione di più di 4<unk> @ 8 stati , ha ri<unk> @ gi<unk> @ sta da 40 % .
2025-05-27 00:17:20,045 - INFO - joeynmt.training - Example #1
2025-05-27 00:17:20,045 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-27 00:17:20,045 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-27 00:17:20,045 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', '<unk>', '@', 'sce', 'la', 'seri@@', '<unk>', '@', 'età', 'di', 'questo', 'particolare', 'problema', 'perché', 'non', 'mostra', 'il', 'poll@@', '<unk>', '@', 'o', 'del', 'ghiaccio', '.', '</s>']
2025-05-27 00:17:20,045 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-27 00:17:20,045 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-27 00:17:20,045 - INFO - joeynmt.training - 	Hypothesis: Ma questo capi<unk> @ sce la seri<unk> @ età di questo particolare problema perché non mostra il poll<unk> @ o del ghiaccio .
2025-05-27 00:17:20,045 - INFO - joeynmt.training - Example #2
2025-05-27 00:17:20,045 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-27 00:17:20,045 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-27 00:17:20,045 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ant@@', '<unk>', '@', 'ico', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'del', 'sistema', 'climatico', '.', '</s>']
2025-05-27 00:17:20,045 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-27 00:17:20,045 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-27 00:17:20,045 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio ar<unk> @ c<unk> @ ant<unk> @ ico è , in un senso , il cuore del sistema del sistema climatico .
2025-05-27 00:17:20,045 - INFO - joeynmt.training - Example #3
2025-05-27 00:17:20,046 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-27 00:17:20,046 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-27 00:17:20,046 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', '<unk>', '@', 'pon@@', '<unk>', '@', 'gono', 'in', 'v@@', '<unk>', '@', 'uo@@', '<unk>', '@', 'to', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-27 00:17:20,046 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-27 00:17:20,046 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-27 00:17:20,046 - INFO - joeynmt.training - 	Hypothesis: Si es<unk> @ pon<unk> @ gono in v<unk> @ uo<unk> @ to e contr<unk> @ atti in estate .
2025-05-27 00:17:20,046 - INFO - joeynmt.training - Example #4
2025-05-27 00:17:20,046 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-27 00:17:20,046 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-27 00:17:20,046 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'slide', 'vi', 'mostrerò', 'che', 'sarà', 'un', 'rapi@@', '<unk>', '@', 'do', 'rapidamente', 'di', 'più', 'veloce', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-27 00:17:20,046 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-27 00:17:20,046 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-27 00:17:20,046 - INFO - joeynmt.training - 	Hypothesis: La prossima slide vi mostrerò che sarà un rapi<unk> @ do rapidamente di più veloce di quello che è successo negli ultimi 25 anni .
2025-05-27 00:17:48,035 - INFO - joeynmt.training - Epoch   6, Step:    25600, Batch Loss:     1.528945, Batch Acc: 0.617211, Tokens per Sec:     2541, Lr: 0.000300
2025-05-27 00:18:16,541 - INFO - joeynmt.training - Epoch   6, Step:    25700, Batch Loss:     1.359837, Batch Acc: 0.619492, Tokens per Sec:     2602, Lr: 0.000300
2025-05-27 00:18:44,588 - INFO - joeynmt.training - Epoch   6, Step:    25800, Batch Loss:     1.413536, Batch Acc: 0.619044, Tokens per Sec:     2659, Lr: 0.000300
2025-05-27 00:19:12,204 - INFO - joeynmt.training - Epoch   6, Step:    25900, Batch Loss:     1.522524, Batch Acc: 0.617496, Tokens per Sec:     2759, Lr: 0.000300
2025-05-27 00:19:39,696 - INFO - joeynmt.training - Epoch   6, Step:    26000, Batch Loss:     1.545588, Batch Acc: 0.621725, Tokens per Sec:     2785, Lr: 0.000300
2025-05-27 00:19:39,696 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 00:22:14,609 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.42, ppl:   4.14, acc:   0.65, generation: 154.9053[sec], evaluation: 0.0000[sec]
2025-05-27 00:22:14,724 - INFO - joeynmt.helpers - delete models/bpe_8k/24000.ckpt
2025-05-27 00:22:14,728 - INFO - joeynmt.training - Example #0
2025-05-27 00:22:14,728 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-27 00:22:14,728 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-27 00:22:14,728 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'due', 'di@@', '<unk>', '@', 'ap@@', '<unk>', '@', 'os@@', '<unk>', '@', 'sa', 'così', 'che', 'il', 'ghiaccio', 'è', 'il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ant@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'molti', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'del', '4@@', '<unk>', '@', '8', 'stati', ',', 'ha', 'ri@@', '<unk>', '@', 'vol@@', '<unk>', '@', 'ato', 'dal', '40', '%', '.', '</s>']
2025-05-27 00:22:14,729 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-27 00:22:14,729 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-27 00:22:14,729 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso ho mostrato queste due di<unk> @ ap<unk> @ os<unk> @ sa così che il ghiaccio è il ghiaccio ar<unk> @ c<unk> @ ant<unk> @ ico , che per molti degli ultimi tre milioni di anni è stata la dimensione del 4<unk> @ 8 stati , ha ri<unk> @ vol<unk> @ ato dal 40 % .
2025-05-27 00:22:14,729 - INFO - joeynmt.training - Example #1
2025-05-27 00:22:14,729 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-27 00:22:14,729 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-27 00:22:14,729 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', '<unk>', '@', 'sce', 'la', 'seri@@', '<unk>', '@', 'età', 'di', 'questo', 'problema', 'perché', 'non', 'mostra', 'il', 'poll@@', '<unk>', '@', 'o', 'del', 'ghiaccio', '.', '</s>']
2025-05-27 00:22:14,729 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-27 00:22:14,729 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-27 00:22:14,729 - INFO - joeynmt.training - 	Hypothesis: Ma questo capi<unk> @ sce la seri<unk> @ età di questo problema perché non mostra il poll<unk> @ o del ghiaccio .
2025-05-27 00:22:14,729 - INFO - joeynmt.training - Example #2
2025-05-27 00:22:14,729 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-27 00:22:14,729 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-27 00:22:14,729 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ant@@', '<unk>', '@', 'ico', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'climatico', 'climatico', '.', '</s>']
2025-05-27 00:22:14,729 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-27 00:22:14,729 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-27 00:22:14,729 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio ar<unk> @ c<unk> @ ant<unk> @ ico è , in un senso , il cuore del sistema climatico climatico .
2025-05-27 00:22:14,729 - INFO - joeynmt.training - Example #3
2025-05-27 00:22:14,729 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-27 00:22:14,729 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-27 00:22:14,729 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', '<unk>', '@', 'ag@@', '<unk>', '@', 'iscono', 'in', 'v@@', '<unk>', '@', 'etro', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-27 00:22:14,730 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-27 00:22:14,730 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-27 00:22:14,730 - INFO - joeynmt.training - 	Hypothesis: Si es<unk> @ ag<unk> @ iscono in v<unk> @ etro e contr<unk> @ atti in estate .
2025-05-27 00:22:14,730 - INFO - joeynmt.training - Example #4
2025-05-27 00:22:14,730 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-27 00:22:14,730 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-27 00:22:14,730 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'slide', 'vi', 'mostrerò', 'che', 'sarà', 'un', 'rapi@@', '<unk>', '@', 'do', 'rapidamente', 'di', 'più', 'rapidamente', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-27 00:22:14,730 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-27 00:22:14,730 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-27 00:22:14,730 - INFO - joeynmt.training - 	Hypothesis: La prossima slide vi mostrerò che sarà un rapi<unk> @ do rapidamente di più rapidamente di quello che è successo negli ultimi 25 anni .
2025-05-27 00:22:43,813 - INFO - joeynmt.training - Epoch   6, Step:    26100, Batch Loss:     1.571040, Batch Acc: 0.616602, Tokens per Sec:     2537, Lr: 0.000300
2025-05-27 00:23:11,136 - INFO - joeynmt.training - Epoch   6, Step:    26200, Batch Loss:     1.610747, Batch Acc: 0.611092, Tokens per Sec:     2723, Lr: 0.000300
2025-05-27 00:23:38,707 - INFO - joeynmt.training - Epoch   6, Step:    26300, Batch Loss:     1.453492, Batch Acc: 0.617268, Tokens per Sec:     2659, Lr: 0.000300
2025-05-27 00:24:07,790 - INFO - joeynmt.training - Epoch   6, Step:    26400, Batch Loss:     1.533536, Batch Acc: 0.619328, Tokens per Sec:     2574, Lr: 0.000300
2025-05-27 00:24:35,141 - INFO - joeynmt.training - Epoch   6, Step:    26500, Batch Loss:     1.570356, Batch Acc: 0.614061, Tokens per Sec:     2595, Lr: 0.000300
2025-05-27 00:24:35,141 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 00:27:00,558 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.41, ppl:   4.10, acc:   0.65, generation: 145.4091[sec], evaluation: 0.0000[sec]
2025-05-27 00:27:00,559 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 00:27:00,716 - INFO - joeynmt.helpers - delete models/bpe_8k/23500.ckpt
2025-05-27 00:27:00,723 - INFO - joeynmt.training - Example #0
2025-05-27 00:27:00,723 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-27 00:27:00,723 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-27 00:27:00,723 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'due', 'l@@', '<unk>', '@', 'ament@@', '<unk>', '@', 'ali', 'così', 'che', 'dimostr@@', '<unk>', '@', 'ano', 'che', 'il', 'ghiaccio', 'di', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ant@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'del', '40', '%', '.', '</s>']
2025-05-27 00:27:00,723 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-27 00:27:00,723 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-27 00:27:00,723 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso ho mostrato queste due l<unk> @ ament<unk> @ ali così che dimostr<unk> @ ano che il ghiaccio di ghiaccio ar<unk> @ c<unk> @ ant<unk> @ ico , che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione del 40 % .
2025-05-27 00:27:00,723 - INFO - joeynmt.training - Example #1
2025-05-27 00:27:00,723 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-27 00:27:00,723 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-27 00:27:00,723 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', '<unk>', '@', 'sce', 'la', 'seri@@', '<unk>', '@', 'età', 'di', 'questo', 'particolare', 'problema', 'perché', 'non', 'mostra', 'la', 'poll@@', '<unk>', '@', 'ità', 'del', 'ghiaccio', '.', '</s>']
2025-05-27 00:27:00,724 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-27 00:27:00,724 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-27 00:27:00,724 - INFO - joeynmt.training - 	Hypothesis: Ma questo capi<unk> @ sce la seri<unk> @ età di questo particolare problema perché non mostra la poll<unk> @ ità del ghiaccio .
2025-05-27 00:27:00,724 - INFO - joeynmt.training - Example #2
2025-05-27 00:27:00,724 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-27 00:27:00,724 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-27 00:27:00,724 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ant@@', '<unk>', '@', 'ico', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'del', 'sistema', 'globale', '.', '</s>']
2025-05-27 00:27:00,724 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-27 00:27:00,724 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-27 00:27:00,724 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio ar<unk> @ c<unk> @ ant<unk> @ ico è , in un senso , il cuore del sistema del sistema globale .
2025-05-27 00:27:00,724 - INFO - joeynmt.training - Example #3
2025-05-27 00:27:00,724 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-27 00:27:00,724 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-27 00:27:00,724 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', '<unk>', '@', 'pan@@', '<unk>', '@', 'di', 'v@@', '<unk>', '@', 'etro', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-27 00:27:00,724 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-27 00:27:00,724 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-27 00:27:00,724 - INFO - joeynmt.training - 	Hypothesis: Si es<unk> @ pan<unk> @ di v<unk> @ etro e contr<unk> @ atti in estate .
2025-05-27 00:27:00,724 - INFO - joeynmt.training - Example #4
2025-05-27 00:27:00,724 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-27 00:27:00,724 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-27 00:27:00,724 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'slide', 'vi', 'mostro', 'un', 'rapi@@', '<unk>', '@', 'do', 'rapi@@', '<unk>', '@', 'do', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-27 00:27:00,725 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-27 00:27:00,725 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-27 00:27:00,725 - INFO - joeynmt.training - 	Hypothesis: La prossima slide vi mostro un rapi<unk> @ do rapi<unk> @ do di quello che è successo negli ultimi 25 anni .
2025-05-27 00:27:28,907 - INFO - joeynmt.training - Epoch   6, Step:    26600, Batch Loss:     1.472564, Batch Acc: 0.618995, Tokens per Sec:     2659, Lr: 0.000300
2025-05-27 00:27:57,191 - INFO - joeynmt.training - Epoch   6, Step:    26700, Batch Loss:     1.469083, Batch Acc: 0.617548, Tokens per Sec:     2585, Lr: 0.000300
2025-05-27 00:28:26,263 - INFO - joeynmt.training - Epoch   6, Step:    26800, Batch Loss:     1.379894, Batch Acc: 0.616697, Tokens per Sec:     2549, Lr: 0.000300
2025-05-27 00:28:54,836 - INFO - joeynmt.training - Epoch   6, Step:    26900, Batch Loss:     1.441849, Batch Acc: 0.621399, Tokens per Sec:     2627, Lr: 0.000300
2025-05-27 00:29:23,315 - INFO - joeynmt.training - Epoch   6, Step:    27000, Batch Loss:     1.660628, Batch Acc: 0.615355, Tokens per Sec:     2491, Lr: 0.000300
2025-05-27 00:29:23,316 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 00:31:48,990 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.41, ppl:   4.09, acc:   0.65, generation: 145.6671[sec], evaluation: 0.0000[sec]
2025-05-27 00:31:48,992 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 00:31:49,140 - INFO - joeynmt.helpers - delete models/bpe_8k/25000.ckpt
2025-05-27 00:31:49,155 - INFO - joeynmt.training - Example #0
2025-05-27 00:31:49,155 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-27 00:31:49,155 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-27 00:31:49,155 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'due', 'slide', 'così', 'che', 'dimostr@@', '<unk>', '@', 'ano', 'che', 'il', 'ghiaccio', "l'", 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ant@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'del', '4@@', '<unk>', '@', '8', 'stati', ',', 'ha', 'ri@@', '<unk>', '@', 'dotto', 'la', 'dimensione', 'dei', '4@@', '<unk>', '@', '8', 'stati', ',', 'ha', 'ri@@', '<unk>', '@', 'dotto', 'dal', '40', '%', '.', '</s>']
2025-05-27 00:31:49,155 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-27 00:31:49,155 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-27 00:31:49,155 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso ho mostrato queste due slide così che dimostr<unk> @ ano che il ghiaccio l' ar<unk> @ c<unk> @ ant<unk> @ ico , che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione del 4<unk> @ 8 stati , ha ri<unk> @ dotto la dimensione dei 4<unk> @ 8 stati , ha ri<unk> @ dotto dal 40 % .
2025-05-27 00:31:49,155 - INFO - joeynmt.training - Example #1
2025-05-27 00:31:49,155 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-27 00:31:49,155 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-27 00:31:49,155 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', '<unk>', '@', 'sce', 'la', 'seri@@', '<unk>', '@', 'età', 'di', 'questo', 'problema', 'perché', 'non', 'mostra', 'il', 'poll@@', '<unk>', '@', 'o', 'del', 'ghiaccio', '.', '</s>']
2025-05-27 00:31:49,155 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-27 00:31:49,155 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-27 00:31:49,155 - INFO - joeynmt.training - 	Hypothesis: Ma questo capi<unk> @ sce la seri<unk> @ età di questo problema perché non mostra il poll<unk> @ o del ghiaccio .
2025-05-27 00:31:49,155 - INFO - joeynmt.training - Example #2
2025-05-27 00:31:49,155 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-27 00:31:49,155 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-27 00:31:49,155 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ant@@', '<unk>', '@', 'ico', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'del', 'sistema', 'climatico', '.', '</s>']
2025-05-27 00:31:49,156 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-27 00:31:49,156 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-27 00:31:49,156 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio ar<unk> @ c<unk> @ ant<unk> @ ico è , in un senso , il cuore del sistema del sistema climatico .
2025-05-27 00:31:49,156 - INFO - joeynmt.training - Example #3
2025-05-27 00:31:49,156 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-27 00:31:49,156 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-27 00:31:49,156 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'tratta', 'di', 'v@@', '<unk>', '@', 'asi', 'in', 'v@@', '<unk>', '@', 'etro', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-27 00:31:49,156 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-27 00:31:49,156 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-27 00:31:49,156 - INFO - joeynmt.training - 	Hypothesis: Si tratta di v<unk> @ asi in v<unk> @ etro e contr<unk> @ atti in estate .
2025-05-27 00:31:49,156 - INFO - joeynmt.training - Example #4
2025-05-27 00:31:49,156 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-27 00:31:49,156 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-27 00:31:49,156 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'slide', 'vi', 'mostrerò', 'che', 'sarà', 'un', 'rapi@@', '<unk>', '@', 'do', 'di', 'rapi@@', '<unk>', '@', 'do', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-27 00:31:49,156 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-27 00:31:49,156 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-27 00:31:49,156 - INFO - joeynmt.training - 	Hypothesis: La prossima slide vi mostrerò che sarà un rapi<unk> @ do di rapi<unk> @ do di quello che è successo negli ultimi 25 anni .
2025-05-27 00:32:18,331 - INFO - joeynmt.training - Epoch   6, Step:    27100, Batch Loss:     1.536644, Batch Acc: 0.616594, Tokens per Sec:     2477, Lr: 0.000300
2025-05-27 00:32:47,327 - INFO - joeynmt.training - Epoch   6, Step:    27200, Batch Loss:     1.376042, Batch Acc: 0.617610, Tokens per Sec:     2544, Lr: 0.000300
2025-05-27 00:33:16,000 - INFO - joeynmt.training - Epoch   6, Step:    27300, Batch Loss:     1.429741, Batch Acc: 0.618834, Tokens per Sec:     2576, Lr: 0.000300
2025-05-27 00:33:45,213 - INFO - joeynmt.training - Epoch   6, Step:    27400, Batch Loss:     1.505371, Batch Acc: 0.618549, Tokens per Sec:     2620, Lr: 0.000300
2025-05-27 00:34:13,103 - INFO - joeynmt.training - Epoch   6, Step:    27500, Batch Loss:     1.336279, Batch Acc: 0.619280, Tokens per Sec:     2690, Lr: 0.000300
2025-05-27 00:34:13,104 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 00:36:53,214 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.40, ppl:   4.05, acc:   0.65, generation: 160.1031[sec], evaluation: 0.0000[sec]
2025-05-27 00:36:53,216 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 00:36:53,343 - INFO - joeynmt.helpers - delete models/bpe_8k/24500.ckpt
2025-05-27 00:36:53,365 - INFO - joeynmt.training - Example #0
2025-05-27 00:36:53,365 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-27 00:36:53,365 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-27 00:36:53,365 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'scorso', 'ho', 'mostrato', 'queste', 'due', 'di@@', '<unk>', '@', 'ap@@', '<unk>', '@', 'os@@', '<unk>', '@', 'sa', 'così', 'che', 'il', 'ghiaccio', 'di', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ant@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'del', '40', '%', '.', '</s>']
2025-05-27 00:36:53,365 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-27 00:36:53,365 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-27 00:36:53,365 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso scorso ho mostrato queste due di<unk> @ ap<unk> @ os<unk> @ sa così che il ghiaccio di ghiaccio ar<unk> @ c<unk> @ ant<unk> @ ico , che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione del 40 % .
2025-05-27 00:36:53,365 - INFO - joeynmt.training - Example #1
2025-05-27 00:36:53,365 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-27 00:36:53,365 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-27 00:36:53,365 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', '<unk>', '@', 'sce', 'la', 'seri@@', '<unk>', '@', 'età', 'di', 'questo', 'problema', 'perché', 'non', 'mostra', 'la', 'poll@@', '<unk>', '@', 'o', 'del', 'ghiaccio', '.', '</s>']
2025-05-27 00:36:53,366 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-27 00:36:53,366 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-27 00:36:53,366 - INFO - joeynmt.training - 	Hypothesis: Ma questo capi<unk> @ sce la seri<unk> @ età di questo problema perché non mostra la poll<unk> @ o del ghiaccio .
2025-05-27 00:36:53,366 - INFO - joeynmt.training - Example #2
2025-05-27 00:36:53,366 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-27 00:36:53,366 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-27 00:36:53,366 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ant@@', '<unk>', '@', 'ico', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'climatico', '.', '</s>']
2025-05-27 00:36:53,366 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-27 00:36:53,366 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-27 00:36:53,366 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio ar<unk> @ c<unk> @ ant<unk> @ ico è , in un senso , il cuore del sistema climatico .
2025-05-27 00:36:53,366 - INFO - joeynmt.training - Example #3
2025-05-27 00:36:53,366 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-27 00:36:53,366 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-27 00:36:53,366 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', '<unk>', '@', 'pres@@', '<unk>', '@', 'sioni', 'in', 'inver@@', '<unk>', '@', 'no', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-27 00:36:53,366 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-27 00:36:53,366 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-27 00:36:53,366 - INFO - joeynmt.training - 	Hypothesis: Si es<unk> @ pres<unk> @ sioni in inver<unk> @ no e contr<unk> @ atti in estate .
2025-05-27 00:36:53,366 - INFO - joeynmt.training - Example #4
2025-05-27 00:36:53,366 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-27 00:36:53,366 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-27 00:36:53,366 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'slide', 'vi', 'mostrerò', 'che', 'sarà', 'un', 'rapi@@', '<unk>', '@', 'do', 'di', 'più', 'veloce', 'di', 'quello', 'che', 'è', 'accad@@', '<unk>', '@', 'uto', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-27 00:36:53,366 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-27 00:36:53,366 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-27 00:36:53,366 - INFO - joeynmt.training - 	Hypothesis: La prossima slide vi mostrerò che sarà un rapi<unk> @ do di più veloce di quello che è accad<unk> @ uto negli ultimi 25 anni .
2025-05-27 00:37:21,488 - INFO - joeynmt.training - Epoch   6, Step:    27600, Batch Loss:     1.422863, Batch Acc: 0.618705, Tokens per Sec:     2625, Lr: 0.000300
2025-05-27 00:37:50,378 - INFO - joeynmt.training - Epoch   6, Step:    27700, Batch Loss:     1.469764, Batch Acc: 0.615490, Tokens per Sec:     2536, Lr: 0.000300
2025-05-27 00:38:19,408 - INFO - joeynmt.training - Epoch   6, Step:    27800, Batch Loss:     1.422160, Batch Acc: 0.618393, Tokens per Sec:     2616, Lr: 0.000300
2025-05-27 00:38:48,223 - INFO - joeynmt.training - Epoch   6, Step:    27900, Batch Loss:     1.525600, Batch Acc: 0.614221, Tokens per Sec:     2600, Lr: 0.000300
2025-05-27 00:39:16,796 - INFO - joeynmt.training - Epoch   6, Step:    28000, Batch Loss:     1.417978, Batch Acc: 0.617596, Tokens per Sec:     2602, Lr: 0.000300
2025-05-27 00:39:16,797 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 00:41:53,566 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.39, ppl:   4.03, acc:   0.65, generation: 156.7624[sec], evaluation: 0.0000[sec]
2025-05-27 00:41:53,569 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 00:41:53,705 - INFO - joeynmt.helpers - delete models/bpe_8k/26000.ckpt
2025-05-27 00:41:53,719 - INFO - joeynmt.training - Example #0
2025-05-27 00:41:53,719 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-27 00:41:53,719 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-27 00:41:53,719 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'due', 'slide', 'così', 'che', 'dimostr@@', '<unk>', '@', 'ano', 'che', 'il', 'ghiaccio', 'di', 'ghiaccio', 'che', 'il', 'ghiaccio', 'di', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ant@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'la', 'maggior', 'parte', 'delle', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'del', '4@@', '<unk>', '@', '8', 'stati', ',', 'ha', 'ri@@', '<unk>', '@', 'dotto', 'dal', '40', '%', '.', '</s>']
2025-05-27 00:41:53,719 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-27 00:41:53,719 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-27 00:41:53,719 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso ho mostrato queste due slide così che dimostr<unk> @ ano che il ghiaccio di ghiaccio che il ghiaccio di ghiaccio ar<unk> @ c<unk> @ ant<unk> @ ico , che per la maggior parte delle tre milioni di anni è stata la dimensione del 4<unk> @ 8 stati , ha ri<unk> @ dotto dal 40 % .
2025-05-27 00:41:53,719 - INFO - joeynmt.training - Example #1
2025-05-27 00:41:53,719 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-27 00:41:53,719 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-27 00:41:53,719 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', '<unk>', '@', 'sce', 'la', 'seri@@', '<unk>', '@', 'età', 'di', 'questo', 'particolare', 'problema', 'perché', 'non', 'mostra', 'la', 'poll@@', '<unk>', '@', 'o', 'del', 'ghiaccio', '.', '</s>']
2025-05-27 00:41:53,719 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-27 00:41:53,719 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-27 00:41:53,719 - INFO - joeynmt.training - 	Hypothesis: Ma questo capi<unk> @ sce la seri<unk> @ età di questo particolare problema perché non mostra la poll<unk> @ o del ghiaccio .
2025-05-27 00:41:53,719 - INFO - joeynmt.training - Example #2
2025-05-27 00:41:53,719 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-27 00:41:53,719 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-27 00:41:53,719 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ant@@', '<unk>', '@', 'ico', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'globale', '.', '</s>']
2025-05-27 00:41:53,720 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-27 00:41:53,720 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-27 00:41:53,720 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio ar<unk> @ c<unk> @ ant<unk> @ ico è , in un senso , il cuore del sistema globale .
2025-05-27 00:41:53,720 - INFO - joeynmt.training - Example #3
2025-05-27 00:41:53,720 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-27 00:41:53,720 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-27 00:41:53,720 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', '<unk>', '@', 'amin@@', '<unk>', '@', 'ano', 'in', 'v@@', '<unk>', '@', 'etro', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-27 00:41:53,720 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-27 00:41:53,720 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-27 00:41:53,720 - INFO - joeynmt.training - 	Hypothesis: Si es<unk> @ amin<unk> @ ano in v<unk> @ etro e contr<unk> @ atti in estate .
2025-05-27 00:41:53,720 - INFO - joeynmt.training - Example #4
2025-05-27 00:41:53,720 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-27 00:41:53,720 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-27 00:41:53,720 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'slide', 'vi', 'mostrerò', 'un', 'rapi@@', '<unk>', '@', 'do', 'di', 'essere', 'un', 'rapi@@', '<unk>', '@', 'do', 'di', 'più', 'veloce', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-27 00:41:53,720 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-27 00:41:53,720 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-27 00:41:53,720 - INFO - joeynmt.training - 	Hypothesis: La prossima slide vi mostrerò un rapi<unk> @ do di essere un rapi<unk> @ do di più veloce di quello che è successo negli ultimi 25 anni .
2025-05-27 00:42:22,072 - INFO - joeynmt.training - Epoch   6, Step:    28100, Batch Loss:     1.523918, Batch Acc: 0.616047, Tokens per Sec:     2618, Lr: 0.000300
2025-05-27 00:42:50,796 - INFO - joeynmt.training - Epoch   6, Step:    28200, Batch Loss:     1.498050, Batch Acc: 0.613413, Tokens per Sec:     2540, Lr: 0.000300
2025-05-27 00:43:20,069 - INFO - joeynmt.training - Epoch   6, Step:    28300, Batch Loss:     1.528421, Batch Acc: 0.618621, Tokens per Sec:     2519, Lr: 0.000300
2025-05-27 00:43:47,959 - INFO - joeynmt.training - Epoch   6, Step:    28400, Batch Loss:     1.317438, Batch Acc: 0.617908, Tokens per Sec:     2594, Lr: 0.000300
2025-05-27 00:44:17,025 - INFO - joeynmt.training - Epoch   6, Step:    28500, Batch Loss:     1.439334, Batch Acc: 0.619304, Tokens per Sec:     2500, Lr: 0.000300
2025-05-27 00:44:17,025 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 00:46:41,668 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.39, ppl:   4.02, acc:   0.65, generation: 144.6352[sec], evaluation: 0.0000[sec]
2025-05-27 00:46:41,670 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 00:46:41,786 - INFO - joeynmt.helpers - delete models/bpe_8k/25500.ckpt
2025-05-27 00:46:41,812 - INFO - joeynmt.training - Example #0
2025-05-27 00:46:41,812 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-27 00:46:41,812 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-27 00:46:41,812 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'due', 'mu@@', '<unk>', '@', 'l@@', '<unk>', '@', 'ap@@', '<unk>', '@', 'os@@', '<unk>', '@', 'sa', 'così', 'che', 'il', 'ghiaccio', 'di', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ant@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'la', 'maggior', 'parte', 'dei', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'del', '4@@', '<unk>', '@', '8', 'stati', ',', 'ha', 'ri@@', '<unk>', '@', 'dotto', 'dal', '40', '%', '.', '</s>']
2025-05-27 00:46:41,812 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-27 00:46:41,812 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-27 00:46:41,812 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso ho mostrato queste due mu<unk> @ l<unk> @ ap<unk> @ os<unk> @ sa così che il ghiaccio di ghiaccio ar<unk> @ c<unk> @ ant<unk> @ ico , che per la maggior parte dei tre milioni di anni è stata la dimensione del 4<unk> @ 8 stati , ha ri<unk> @ dotto dal 40 % .
2025-05-27 00:46:41,812 - INFO - joeynmt.training - Example #1
2025-05-27 00:46:41,812 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-27 00:46:41,812 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-27 00:46:41,812 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'è', 'la', 'seri@@', '<unk>', '@', 'età', 'di', 'questo', 'particolare', 'problema', 'perché', 'non', 'mostra', 'il', 'poll@@', '<unk>', '@', 'o', 'del', 'ghiaccio', '.', '</s>']
2025-05-27 00:46:41,813 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-27 00:46:41,813 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-27 00:46:41,813 - INFO - joeynmt.training - 	Hypothesis: Ma questo è la seri<unk> @ età di questo particolare problema perché non mostra il poll<unk> @ o del ghiaccio .
2025-05-27 00:46:41,813 - INFO - joeynmt.training - Example #2
2025-05-27 00:46:41,813 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-27 00:46:41,813 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-27 00:46:41,813 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ant@@', '<unk>', '@', 'ico', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'globale', '.', '</s>']
2025-05-27 00:46:41,813 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-27 00:46:41,813 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-27 00:46:41,813 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio ar<unk> @ c<unk> @ ant<unk> @ ico è , in un senso , il cuore del sistema globale .
2025-05-27 00:46:41,813 - INFO - joeynmt.training - Example #3
2025-05-27 00:46:41,813 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-27 00:46:41,813 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-27 00:46:41,813 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'tratta', 'di', 'un', 'v@@', '<unk>', '@', 'etro', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-27 00:46:41,813 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-27 00:46:41,813 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-27 00:46:41,813 - INFO - joeynmt.training - 	Hypothesis: Si tratta di un v<unk> @ etro e contr<unk> @ atti in estate .
2025-05-27 00:46:41,813 - INFO - joeynmt.training - Example #4
2025-05-27 00:46:41,813 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-27 00:46:41,813 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-27 00:46:41,813 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'slide', 'vi', 'mostro', 'un', 'rapi@@', '<unk>', '@', 'do', 'veloce', 'di', 'un', 'passo', 'avanti', 'di', 'più', 'veloce', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-27 00:46:41,813 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-27 00:46:41,813 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-27 00:46:41,813 - INFO - joeynmt.training - 	Hypothesis: La prossima slide vi mostro un rapi<unk> @ do veloce di un passo avanti di più veloce di quello che è successo negli ultimi 25 anni .
2025-05-27 00:47:11,415 - INFO - joeynmt.training - Epoch   6, Step:    28600, Batch Loss:     1.518221, Batch Acc: 0.617350, Tokens per Sec:     2520, Lr: 0.000300
2025-05-27 00:47:40,303 - INFO - joeynmt.training - Epoch   6, Step:    28700, Batch Loss:     1.632108, Batch Acc: 0.619848, Tokens per Sec:     2561, Lr: 0.000300
2025-05-27 00:48:08,153 - INFO - joeynmt.training - Epoch   6, Step:    28800, Batch Loss:     1.480343, Batch Acc: 0.621522, Tokens per Sec:     2670, Lr: 0.000300
2025-05-27 00:48:37,345 - INFO - joeynmt.training - Epoch   6, Step:    28900, Batch Loss:     1.650952, Batch Acc: 0.621902, Tokens per Sec:     2531, Lr: 0.000300
2025-05-27 00:49:06,030 - INFO - joeynmt.training - Epoch   6, Step:    29000, Batch Loss:     1.791366, Batch Acc: 0.618985, Tokens per Sec:     2537, Lr: 0.000300
2025-05-27 00:49:06,031 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 00:51:27,223 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.39, ppl:   4.01, acc:   0.65, generation: 141.1852[sec], evaluation: 0.0000[sec]
2025-05-27 00:51:27,224 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 00:51:27,342 - INFO - joeynmt.helpers - delete models/bpe_8k/26500.ckpt
2025-05-27 00:51:27,352 - INFO - joeynmt.training - Example #0
2025-05-27 00:51:27,353 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-27 00:51:27,353 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-27 00:51:27,353 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'due', 'l@@', '<unk>', '@', 'ap@@', '<unk>', '@', 'l@@', '<unk>', '@', 'os@@', '<unk>', '@', 'sa', 'che', 'il', 'ghiaccio', 'di', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ant@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'dei', '4@@', '<unk>', '@', '8', 'stati', ',', 'ha', 'ri@@', '<unk>', '@', 'dotto', 'dal', '40', '%', '.', '</s>']
2025-05-27 00:51:27,353 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-27 00:51:27,353 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-27 00:51:27,353 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso ho mostrato queste due l<unk> @ ap<unk> @ l<unk> @ os<unk> @ sa che il ghiaccio di ar<unk> @ c<unk> @ ant<unk> @ ico , che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione dei 4<unk> @ 8 stati , ha ri<unk> @ dotto dal 40 % .
2025-05-27 00:51:27,353 - INFO - joeynmt.training - Example #1
2025-05-27 00:51:27,353 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-27 00:51:27,353 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-27 00:51:27,353 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', '<unk>', '@', 'sce', 'la', 'seri@@', '<unk>', '@', 'età', 'di', 'questo', 'problema', 'perché', 'non', 'mostra', 'la', 'poll@@', '<unk>', '@', 'o', 'del', 'ghiaccio', '.', '</s>']
2025-05-27 00:51:27,353 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-27 00:51:27,353 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-27 00:51:27,353 - INFO - joeynmt.training - 	Hypothesis: Ma questo capi<unk> @ sce la seri<unk> @ età di questo problema perché non mostra la poll<unk> @ o del ghiaccio .
2025-05-27 00:51:27,353 - INFO - joeynmt.training - Example #2
2025-05-27 00:51:27,353 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-27 00:51:27,353 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-27 00:51:27,353 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'di', 'ghiaccio', 'di', 'ghiaccio', 'di', 'ca@@', '<unk>', '@', 'p', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'climatico', '.', '</s>']
2025-05-27 00:51:27,353 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-27 00:51:27,353 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-27 00:51:27,354 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio di ghiaccio di ghiaccio di ca<unk> @ p è , in un senso , il cuore del sistema climatico .
2025-05-27 00:51:27,354 - INFO - joeynmt.training - Example #3
2025-05-27 00:51:27,354 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-27 00:51:27,354 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-27 00:51:27,354 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', '<unk>', '@', 'ag@@', '<unk>', '@', 'isce', 'in', 'inver@@', '<unk>', '@', 'no', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-27 00:51:27,354 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-27 00:51:27,354 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-27 00:51:27,354 - INFO - joeynmt.training - 	Hypothesis: Si es<unk> @ ag<unk> @ isce in inver<unk> @ no e contr<unk> @ atti in estate .
2025-05-27 00:51:27,354 - INFO - joeynmt.training - Example #4
2025-05-27 00:51:27,354 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-27 00:51:27,354 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-27 00:51:27,354 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'slide', 'vi', 'mostro', 'che', 'sarà', 'un', 'passo', 'rapidamente', ',', 'vi', 'mostrerò', 'un', 'passo', 'rapidamente', ',', 'e', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-27 00:51:27,354 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-27 00:51:27,354 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-27 00:51:27,354 - INFO - joeynmt.training - 	Hypothesis: La slide vi mostro che sarà un passo rapidamente , vi mostrerò un passo rapidamente , e di quello che è successo negli ultimi 25 anni .
2025-05-27 00:51:56,544 - INFO - joeynmt.training - Epoch   6, Step:    29100, Batch Loss:     1.488644, Batch Acc: 0.615078, Tokens per Sec:     2464, Lr: 0.000300
2025-05-27 00:52:25,160 - INFO - joeynmt.training - Epoch   6, Step:    29200, Batch Loss:     1.550281, Batch Acc: 0.616578, Tokens per Sec:     2523, Lr: 0.000300
2025-05-27 00:52:54,250 - INFO - joeynmt.training - Epoch   6, Step:    29300, Batch Loss:     1.391244, Batch Acc: 0.621635, Tokens per Sec:     2642, Lr: 0.000300
2025-05-27 00:53:23,249 - INFO - joeynmt.training - Epoch   6, Step:    29400, Batch Loss:     1.337067, Batch Acc: 0.617991, Tokens per Sec:     2553, Lr: 0.000300
2025-05-27 00:53:51,395 - INFO - joeynmt.training - Epoch   6, Step:    29500, Batch Loss:     1.419333, Batch Acc: 0.617539, Tokens per Sec:     2670, Lr: 0.000300
2025-05-27 00:53:51,395 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 00:56:42,995 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.38, ppl:   3.98, acc:   0.65, generation: 171.5925[sec], evaluation: 0.0000[sec]
2025-05-27 00:56:42,997 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 00:56:43,120 - INFO - joeynmt.helpers - delete models/bpe_8k/27000.ckpt
2025-05-27 00:56:43,136 - INFO - joeynmt.training - Example #0
2025-05-27 00:56:43,136 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-27 00:56:43,136 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-27 00:56:43,136 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'due', 'l@@', '<unk>', '@', 'ap@@', '<unk>', '@', 'di@@', '<unk>', '@', 'mostra', 'che', 'il', 'ghiaccio', 'di', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ant@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'la', 'maggior', 'parte', 'dei', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'del', '4@@', '<unk>', '@', '8', 'stati', ',', 'ha', 'ri@@', '<unk>', '@', 'dotto', 'dal', '40', '%', '.', '</s>']
2025-05-27 00:56:43,136 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-27 00:56:43,136 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-27 00:56:43,136 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso ho mostrato queste due l<unk> @ ap<unk> @ di<unk> @ mostra che il ghiaccio di ghiaccio ar<unk> @ c<unk> @ ant<unk> @ ico , che per la maggior parte dei tre milioni di anni è stata la dimensione del 4<unk> @ 8 stati , ha ri<unk> @ dotto dal 40 % .
2025-05-27 00:56:43,136 - INFO - joeynmt.training - Example #1
2025-05-27 00:56:43,136 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-27 00:56:43,136 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-27 00:56:43,136 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', '<unk>', '@', 'sce', 'la', 'seri@@', '<unk>', '@', 'età', 'di', 'questo', 'problema', 'perché', 'non', 'mostra', 'la', 'poll@@', '<unk>', '@', 'o', 'del', 'ghiaccio', '.', '</s>']
2025-05-27 00:56:43,136 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-27 00:56:43,137 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-27 00:56:43,137 - INFO - joeynmt.training - 	Hypothesis: Ma questo capi<unk> @ sce la seri<unk> @ età di questo problema perché non mostra la poll<unk> @ o del ghiaccio .
2025-05-27 00:56:43,137 - INFO - joeynmt.training - Example #2
2025-05-27 00:56:43,137 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-27 00:56:43,137 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-27 00:56:43,137 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ica', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'climatico', '.', '</s>']
2025-05-27 00:56:43,137 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-27 00:56:43,137 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-27 00:56:43,137 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio ar<unk> @ c<unk> @ ica è , in un senso , il cuore del sistema climatico .
2025-05-27 00:56:43,137 - INFO - joeynmt.training - Example #3
2025-05-27 00:56:43,137 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-27 00:56:43,137 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-27 00:56:43,137 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', '<unk>', '@', 'amin@@', '<unk>', '@', 'ano', 'in', 'vinc@@', '<unk>', '@', 'itore', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-27 00:56:43,137 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-27 00:56:43,137 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-27 00:56:43,137 - INFO - joeynmt.training - 	Hypothesis: Si es<unk> @ amin<unk> @ ano in vinc<unk> @ itore e contr<unk> @ atti in estate .
2025-05-27 00:56:43,137 - INFO - joeynmt.training - Example #4
2025-05-27 00:56:43,137 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-27 00:56:43,137 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-27 00:56:43,137 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'slide', 'vi', 'mostro', 'che', 'sarà', 'un', 'passo', 'rapidamente', 'di', 'più', 'rapidamente', 'di', 'un', 'passo', 'rapidamente', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-27 00:56:43,137 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-27 00:56:43,137 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-27 00:56:43,137 - INFO - joeynmt.training - 	Hypothesis: La prossima slide vi mostro che sarà un passo rapidamente di più rapidamente di un passo rapidamente di quello che è successo negli ultimi 25 anni .
2025-05-27 00:57:11,230 - INFO - joeynmt.training - Epoch   6, Step:    29600, Batch Loss:     1.579945, Batch Acc: 0.619809, Tokens per Sec:     2593, Lr: 0.000300
2025-05-27 00:57:39,661 - INFO - joeynmt.training - Epoch   6, Step:    29700, Batch Loss:     1.422955, Batch Acc: 0.620498, Tokens per Sec:     2531, Lr: 0.000300
2025-05-27 00:58:08,376 - INFO - joeynmt.training - Epoch   6, Step:    29800, Batch Loss:     1.478762, Batch Acc: 0.614133, Tokens per Sec:     2492, Lr: 0.000300
2025-05-27 00:58:37,628 - INFO - joeynmt.training - Epoch   6, Step:    29900, Batch Loss:     1.436677, Batch Acc: 0.619668, Tokens per Sec:     2556, Lr: 0.000300
2025-05-27 00:58:48,970 - INFO - joeynmt.training - Epoch   6: total training loss 7419.40
2025-05-27 00:58:48,970 - INFO - joeynmt.training - EPOCH 7
2025-05-27 00:59:07,231 - INFO - joeynmt.training - Epoch   7, Step:    30000, Batch Loss:     1.382202, Batch Acc: 0.629336, Tokens per Sec:     2606, Lr: 0.000300
2025-05-27 00:59:07,231 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 01:01:46,070 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.38, ppl:   3.98, acc:   0.65, generation: 158.8307[sec], evaluation: 0.0000[sec]
2025-05-27 01:01:46,071 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 01:01:46,189 - INFO - joeynmt.helpers - delete models/bpe_8k/27500.ckpt
2025-05-27 01:01:46,194 - INFO - joeynmt.training - Example #0
2025-05-27 01:01:46,194 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-27 01:01:46,194 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-27 01:01:46,194 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'due', 'slide', 'così', 'che', 'dimostr@@', '<unk>', '@', 'ano', 'che', 'il', 'ghiaccio', 'che', 'il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ant@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'più', 'bassa', 'del', '4@@', '<unk>', '@', '8', 'stati', ',', 'ha', 'ri@@', '<unk>', '@', 'dotto', 'dal', '40', '%', '.', '</s>']
2025-05-27 01:01:46,194 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-27 01:01:46,194 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-27 01:01:46,194 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso ho mostrato queste due slide così che dimostr<unk> @ ano che il ghiaccio che il ghiaccio ar<unk> @ c<unk> @ ant<unk> @ ico , che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione più bassa del 4<unk> @ 8 stati , ha ri<unk> @ dotto dal 40 % .
2025-05-27 01:01:46,194 - INFO - joeynmt.training - Example #1
2025-05-27 01:01:46,194 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-27 01:01:46,194 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-27 01:01:46,194 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', '<unk>', '@', 'sce', 'la', 'seri@@', '<unk>', '@', 'età', 'di', 'questo', 'particolare', 'problema', 'perché', 'non', 'mostra', 'la', 'poll@@', '<unk>', '@', 'ità', 'del', 'ghiaccio', '.', '</s>']
2025-05-27 01:01:46,194 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-27 01:01:46,195 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-27 01:01:46,195 - INFO - joeynmt.training - 	Hypothesis: Ma questo capi<unk> @ sce la seri<unk> @ età di questo particolare problema perché non mostra la poll<unk> @ ità del ghiaccio .
2025-05-27 01:01:46,195 - INFO - joeynmt.training - Example #2
2025-05-27 01:01:46,195 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-27 01:01:46,195 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-27 01:01:46,195 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'globale', '.', '</s>']
2025-05-27 01:01:46,195 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-27 01:01:46,195 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-27 01:01:46,195 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio ar<unk> @ c<unk> @ ico è , in un senso , il cuore del sistema globale .
2025-05-27 01:01:46,195 - INFO - joeynmt.training - Example #3
2025-05-27 01:01:46,195 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-27 01:01:46,195 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-27 01:01:46,195 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'tratta', 'di', 'vinc@@', '<unk>', '@', 'ere', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-27 01:01:46,195 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-27 01:01:46,195 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-27 01:01:46,195 - INFO - joeynmt.training - 	Hypothesis: Si tratta di vinc<unk> @ ere e contr<unk> @ atti in estate .
2025-05-27 01:01:46,195 - INFO - joeynmt.training - Example #4
2025-05-27 01:01:46,195 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-27 01:01:46,195 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-27 01:01:46,195 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'slide', 'vi', 'mostrerò', 'che', 'sarà', 'un', 'rapi@@', '<unk>', '@', 'do', 'di', 'rapi@@', '<unk>', '@', 'do', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-27 01:01:46,195 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-27 01:01:46,195 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-27 01:01:46,195 - INFO - joeynmt.training - 	Hypothesis: La prossima slide vi mostrerò che sarà un rapi<unk> @ do di rapi<unk> @ do di quello che è successo negli ultimi 25 anni .
2025-05-27 01:02:14,535 - INFO - joeynmt.training - Epoch   7, Step:    30100, Batch Loss:     1.336130, Batch Acc: 0.632037, Tokens per Sec:     2549, Lr: 0.000300
2025-05-27 01:02:43,675 - INFO - joeynmt.training - Epoch   7, Step:    30200, Batch Loss:     1.591948, Batch Acc: 0.625875, Tokens per Sec:     2481, Lr: 0.000300
2025-05-27 01:03:12,815 - INFO - joeynmt.training - Epoch   7, Step:    30300, Batch Loss:     1.429557, Batch Acc: 0.623311, Tokens per Sec:     2563, Lr: 0.000300
2025-05-27 01:03:40,774 - INFO - joeynmt.training - Epoch   7, Step:    30400, Batch Loss:     1.294570, Batch Acc: 0.625338, Tokens per Sec:     2657, Lr: 0.000300
2025-05-27 01:04:09,846 - INFO - joeynmt.training - Epoch   7, Step:    30500, Batch Loss:     1.471460, Batch Acc: 0.631027, Tokens per Sec:     2512, Lr: 0.000300
2025-05-27 01:04:09,846 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 01:06:47,793 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.38, ppl:   3.99, acc:   0.65, generation: 157.9384[sec], evaluation: 0.0000[sec]
2025-05-27 01:06:47,907 - INFO - joeynmt.helpers - delete models/bpe_8k/28000.ckpt
2025-05-27 01:06:47,911 - INFO - joeynmt.training - Example #0
2025-05-27 01:06:47,911 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-27 01:06:47,912 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-27 01:06:47,912 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'due', 'slide', 'così', 'che', 'dimostr@@', '<unk>', '@', 'ano', 'che', 'il', 'ghiaccio', "dell'", 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ett@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'molti', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'del', '4@@', '<unk>', '@', '8', 'stati', ',', 'ha', 'ri@@', '<unk>', '@', 'dotto', 'dal', '40', '%', '.', '</s>']
2025-05-27 01:06:47,912 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-27 01:06:47,912 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-27 01:06:47,912 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso ho mostrato queste due slide così che dimostr<unk> @ ano che il ghiaccio dell' ar<unk> @ c<unk> @ ett<unk> @ ico , che per molti degli ultimi tre milioni di anni è stata la dimensione del 4<unk> @ 8 stati , ha ri<unk> @ dotto dal 40 % .
2025-05-27 01:06:47,912 - INFO - joeynmt.training - Example #1
2025-05-27 01:06:47,912 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-27 01:06:47,912 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-27 01:06:47,912 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', '<unk>', '@', 'sce', 'la', 'seri@@', '<unk>', '@', 'età', 'di', 'questo', 'problema', 'perché', 'non', 'mostra', 'il', 'poll@@', '<unk>', '@', 'o', 'del', 'ghiaccio', '.', '</s>']
2025-05-27 01:06:47,912 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-27 01:06:47,912 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-27 01:06:47,912 - INFO - joeynmt.training - 	Hypothesis: Ma questo capi<unk> @ sce la seri<unk> @ età di questo problema perché non mostra il poll<unk> @ o del ghiaccio .
2025-05-27 01:06:47,912 - INFO - joeynmt.training - Example #2
2025-05-27 01:06:47,912 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-27 01:06:47,912 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-27 01:06:47,912 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', "dell'", 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ant@@', '<unk>', '@', 'ico', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'globale', '.', '</s>']
2025-05-27 01:06:47,912 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-27 01:06:47,912 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-27 01:06:47,912 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio dell' ar<unk> @ c<unk> @ ant<unk> @ ico è , in un senso , il cuore del sistema globale .
2025-05-27 01:06:47,912 - INFO - joeynmt.training - Example #3
2025-05-27 01:06:47,912 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-27 01:06:47,913 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-27 01:06:47,913 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', '<unk>', '@', 'ag@@', '<unk>', '@', 'ine', 'in', 'v@@', '<unk>', '@', 'etro', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-27 01:06:47,913 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-27 01:06:47,913 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-27 01:06:47,913 - INFO - joeynmt.training - 	Hypothesis: Si es<unk> @ ag<unk> @ ine in v<unk> @ etro e contr<unk> @ atti in estate .
2025-05-27 01:06:47,913 - INFO - joeynmt.training - Example #4
2025-05-27 01:06:47,913 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-27 01:06:47,913 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-27 01:06:47,913 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'slide', 'vi', 'mostrerò', 'essere', 'un', 'passo', 'rapi@@', '<unk>', '@', 'do', 'di', 'ciò', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-27 01:06:47,913 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-27 01:06:47,913 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-27 01:06:47,913 - INFO - joeynmt.training - 	Hypothesis: La prossima slide vi mostrerò essere un passo rapi<unk> @ do di ciò che è successo negli ultimi 25 anni .
2025-05-27 01:07:16,604 - INFO - joeynmt.training - Epoch   7, Step:    30600, Batch Loss:     1.395248, Batch Acc: 0.630311, Tokens per Sec:     2579, Lr: 0.000300
2025-05-27 01:07:45,809 - INFO - joeynmt.training - Epoch   7, Step:    30700, Batch Loss:     1.505859, Batch Acc: 0.627032, Tokens per Sec:     2531, Lr: 0.000300
2025-05-27 01:08:14,774 - INFO - joeynmt.training - Epoch   7, Step:    30800, Batch Loss:     1.603452, Batch Acc: 0.623263, Tokens per Sec:     2500, Lr: 0.000300
2025-05-27 01:08:43,371 - INFO - joeynmt.training - Epoch   7, Step:    30900, Batch Loss:     1.395015, Batch Acc: 0.624769, Tokens per Sec:     2476, Lr: 0.000300
2025-05-27 01:09:11,740 - INFO - joeynmt.training - Epoch   7, Step:    31000, Batch Loss:     1.521005, Batch Acc: 0.623476, Tokens per Sec:     2611, Lr: 0.000300
2025-05-27 01:09:11,740 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 01:11:53,206 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.38, ppl:   3.98, acc:   0.65, generation: 161.4578[sec], evaluation: 0.0000[sec]
2025-05-27 01:11:53,323 - INFO - joeynmt.helpers - delete models/bpe_8k/28500.ckpt
2025-05-27 01:11:53,332 - INFO - joeynmt.training - Example #0
2025-05-27 01:11:53,332 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-27 01:11:53,332 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-27 01:11:53,332 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'due', 'slide', 'così', 'che', 'dimostr@@', '<unk>', '@', 'ano', 'che', 'il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ant@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'del', '4@@', '<unk>', '@', '8', 'stati', ',', 'ha', 'ri@@', '<unk>', '@', 'dotto', 'dal', '40', '%', '.', '</s>']
2025-05-27 01:11:53,332 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-27 01:11:53,333 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-27 01:11:53,333 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso ho mostrato queste due slide così che dimostr<unk> @ ano che il ghiaccio ar<unk> @ c<unk> @ ant<unk> @ ico , che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione del 4<unk> @ 8 stati , ha ri<unk> @ dotto dal 40 % .
2025-05-27 01:11:53,333 - INFO - joeynmt.training - Example #1
2025-05-27 01:11:53,333 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-27 01:11:53,333 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-27 01:11:53,333 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', '<unk>', '@', 'sce', 'la', 'seri@@', '<unk>', '@', 'età', 'di', 'questo', 'problema', 'perché', 'non', 'mostra', 'il', 'poll@@', '<unk>', '@', 'o', 'del', 'ghiaccio', '.', '</s>']
2025-05-27 01:11:53,333 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-27 01:11:53,333 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-27 01:11:53,333 - INFO - joeynmt.training - 	Hypothesis: Ma questo capi<unk> @ sce la seri<unk> @ età di questo problema perché non mostra il poll<unk> @ o del ghiaccio .
2025-05-27 01:11:53,333 - INFO - joeynmt.training - Example #2
2025-05-27 01:11:53,333 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-27 01:11:53,333 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-27 01:11:53,333 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ant@@', '<unk>', '@', 'ico', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'climatico', '.', '</s>']
2025-05-27 01:11:53,333 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-27 01:11:53,333 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-27 01:11:53,333 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio ar<unk> @ c<unk> @ ant<unk> @ ico è , in un senso , il cuore del sistema climatico .
2025-05-27 01:11:53,333 - INFO - joeynmt.training - Example #3
2025-05-27 01:11:53,333 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-27 01:11:53,333 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-27 01:11:53,333 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', '<unk>', '@', 'pan@@', '<unk>', '@', 'di', 'v@@', '<unk>', '@', 'etro', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-27 01:11:53,333 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-27 01:11:53,333 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-27 01:11:53,333 - INFO - joeynmt.training - 	Hypothesis: Si es<unk> @ pan<unk> @ di v<unk> @ etro e contr<unk> @ atti in estate .
2025-05-27 01:11:53,333 - INFO - joeynmt.training - Example #4
2025-05-27 01:11:53,334 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-27 01:11:53,334 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-27 01:11:53,334 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'slide', 'vi', 'mostrerò', 'che', 'sarà', 'un', 'passo', 'rapidamente', 'di', 'un', 'passo', 'rapidamente', 'di', 'un', 'passo', 'rapidamente', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-27 01:11:53,334 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-27 01:11:53,334 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-27 01:11:53,334 - INFO - joeynmt.training - 	Hypothesis: La slide vi mostrerò che sarà un passo rapidamente di un passo rapidamente di un passo rapidamente di quello che è successo negli ultimi 25 anni .
2025-05-27 01:12:22,092 - INFO - joeynmt.training - Epoch   7, Step:    31100, Batch Loss:     1.321310, Batch Acc: 0.625456, Tokens per Sec:     2564, Lr: 0.000300
2025-05-27 01:12:50,925 - INFO - joeynmt.training - Epoch   7, Step:    31200, Batch Loss:     1.422075, Batch Acc: 0.626033, Tokens per Sec:     2548, Lr: 0.000300
2025-05-27 01:13:20,082 - INFO - joeynmt.training - Epoch   7, Step:    31300, Batch Loss:     1.347997, Batch Acc: 0.628483, Tokens per Sec:     2553, Lr: 0.000300
2025-05-27 01:13:49,511 - INFO - joeynmt.training - Epoch   7, Step:    31400, Batch Loss:     1.386492, Batch Acc: 0.622699, Tokens per Sec:     2548, Lr: 0.000300
2025-05-27 01:14:18,241 - INFO - joeynmt.training - Epoch   7, Step:    31500, Batch Loss:     1.459106, Batch Acc: 0.624813, Tokens per Sec:     2578, Lr: 0.000300
2025-05-27 01:14:18,241 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 01:16:50,480 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.38, ppl:   3.96, acc:   0.65, generation: 152.2300[sec], evaluation: 0.0000[sec]
2025-05-27 01:16:50,482 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 01:16:50,616 - INFO - joeynmt.helpers - delete models/bpe_8k/29000.ckpt
2025-05-27 01:16:50,620 - INFO - joeynmt.training - Example #0
2025-05-27 01:16:50,620 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-27 01:16:50,620 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-27 01:16:50,620 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'due', 'slide', 'così', 'che', 'dimostr@@', '<unk>', '@', 'ano', 'che', 'dimostr@@', '<unk>', '@', 'ano', 'che', 'il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'più', 'infer@@', '<unk>', '@', 'i@@', '<unk>', '@', 'ore', 'del', '4@@', '<unk>', '@', '8', 'stati', ',', 'ha', 'ri@@', '<unk>', '@', 'dotto', 'dal', '40', '%', '.', '</s>']
2025-05-27 01:16:50,620 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-27 01:16:50,620 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-27 01:16:50,620 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso ho mostrato queste due slide così che dimostr<unk> @ ano che dimostr<unk> @ ano che il ghiaccio ar<unk> @ c<unk> @ ico , che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione più infer<unk> @ i<unk> @ ore del 4<unk> @ 8 stati , ha ri<unk> @ dotto dal 40 % .
2025-05-27 01:16:50,620 - INFO - joeynmt.training - Example #1
2025-05-27 01:16:50,621 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-27 01:16:50,621 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-27 01:16:50,621 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', '<unk>', '@', 'sce', 'la', 'seri@@', '<unk>', '@', 'età', 'di', 'questo', 'problema', 'perché', 'non', 'mostra', 'la', 'th@@', '<unk>', '@', 'ick@@', '<unk>', '@', 'ità', 'del', 'ghiaccio', '.', '</s>']
2025-05-27 01:16:50,621 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-27 01:16:50,621 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-27 01:16:50,621 - INFO - joeynmt.training - 	Hypothesis: Ma questo capi<unk> @ sce la seri<unk> @ età di questo problema perché non mostra la th<unk> @ ick<unk> @ ità del ghiaccio .
2025-05-27 01:16:50,621 - INFO - joeynmt.training - Example #2
2025-05-27 01:16:50,621 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-27 01:16:50,621 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-27 01:16:50,621 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'globale', '.', '</s>']
2025-05-27 01:16:50,621 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-27 01:16:50,621 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-27 01:16:50,621 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio ar<unk> @ c<unk> @ ico è , in un senso , il cuore del sistema globale .
2025-05-27 01:16:50,621 - INFO - joeynmt.training - Example #3
2025-05-27 01:16:50,621 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-27 01:16:50,621 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-27 01:16:50,621 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', '<unk>', '@', 'pon@@', '<unk>', '@', 'gono', 'in', 'vinc@@', '<unk>', '@', 'ente', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-27 01:16:50,622 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-27 01:16:50,622 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-27 01:16:50,622 - INFO - joeynmt.training - 	Hypothesis: Si es<unk> @ pon<unk> @ gono in vinc<unk> @ ente e contr<unk> @ atti in estate .
2025-05-27 01:16:50,622 - INFO - joeynmt.training - Example #4
2025-05-27 01:16:50,622 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-27 01:16:50,622 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-27 01:16:50,622 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'slide', 'slide', 'vi', 'mostrerò', 'che', 'sarà', 'un', 'passo', 'rapidamente', 'di', 'più', 'rapidamente', 'di', 'quello', 'che', 'è', 'accad@@', '<unk>', '@', 'uto', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-27 01:16:50,622 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-27 01:16:50,622 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-27 01:16:50,622 - INFO - joeynmt.training - 	Hypothesis: La slide slide vi mostrerò che sarà un passo rapidamente di più rapidamente di quello che è accad<unk> @ uto negli ultimi 25 anni .
2025-05-27 01:17:24,159 - INFO - joeynmt.training - Epoch   7, Step:    31600, Batch Loss:     1.417987, Batch Acc: 0.627424, Tokens per Sec:     2179, Lr: 0.000300
2025-05-27 01:17:57,741 - INFO - joeynmt.training - Epoch   7, Step:    31700, Batch Loss:     1.369360, Batch Acc: 0.619731, Tokens per Sec:     2194, Lr: 0.000300
2025-05-27 01:18:29,226 - INFO - joeynmt.training - Epoch   7, Step:    31800, Batch Loss:     1.458276, Batch Acc: 0.625404, Tokens per Sec:     2360, Lr: 0.000300
2025-05-27 01:18:59,835 - INFO - joeynmt.training - Epoch   7, Step:    31900, Batch Loss:     1.502793, Batch Acc: 0.631701, Tokens per Sec:     2397, Lr: 0.000300
2025-05-27 01:19:27,315 - INFO - joeynmt.training - Epoch   7, Step:    32000, Batch Loss:     1.514696, Batch Acc: 0.622282, Tokens per Sec:     2709, Lr: 0.000300
2025-05-27 01:19:27,315 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 01:21:52,484 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.37, ppl:   3.94, acc:   0.66, generation: 145.1614[sec], evaluation: 0.0000[sec]
2025-05-27 01:21:52,487 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 01:21:52,605 - INFO - joeynmt.helpers - delete models/bpe_8k/30500.ckpt
2025-05-27 01:21:52,608 - INFO - joeynmt.training - Example #0
2025-05-27 01:21:52,608 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-27 01:21:52,608 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-27 01:21:52,608 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'due', 'slide', 'così', 'che', 'dimostr@@', '<unk>', '@', 'ano', 'che', 'il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'più', 'bassa', 'dei', '4@@', '<unk>', '@', '8', 'stati', ',', 'ha', 'ri@@', '<unk>', '@', 'dotto', 'dal', '40', '%', '.', '</s>']
2025-05-27 01:21:52,608 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-27 01:21:52,608 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-27 01:21:52,608 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso ho mostrato queste due slide così che dimostr<unk> @ ano che il ghiaccio ar<unk> @ c<unk> @ ico ar<unk> @ c<unk> @ ico , che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione più bassa dei 4<unk> @ 8 stati , ha ri<unk> @ dotto dal 40 % .
2025-05-27 01:21:52,608 - INFO - joeynmt.training - Example #1
2025-05-27 01:21:52,608 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-27 01:21:52,608 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-27 01:21:52,608 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', '<unk>', '@', 'sce', 'la', 'seri@@', '<unk>', '@', 'età', 'di', 'questo', 'problema', 'perché', 'non', 'mostra', 'la', 'poll@@', '<unk>', '@', 'o', 'del', 'ghiaccio', '.', '</s>']
2025-05-27 01:21:52,608 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-27 01:21:52,608 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-27 01:21:52,608 - INFO - joeynmt.training - 	Hypothesis: Ma questo capi<unk> @ sce la seri<unk> @ età di questo problema perché non mostra la poll<unk> @ o del ghiaccio .
2025-05-27 01:21:52,609 - INFO - joeynmt.training - Example #2
2025-05-27 01:21:52,609 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-27 01:21:52,609 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-27 01:21:52,609 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'globale', '.', '</s>']
2025-05-27 01:21:52,609 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-27 01:21:52,609 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-27 01:21:52,609 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio ar<unk> @ c<unk> @ ico è , in un senso , il cuore del sistema globale .
2025-05-27 01:21:52,609 - INFO - joeynmt.training - Example #3
2025-05-27 01:21:52,609 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-27 01:21:52,609 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-27 01:21:52,609 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'tratta', 'di', 'v@@', '<unk>', '@', 'etro', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-27 01:21:52,609 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-27 01:21:52,609 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-27 01:21:52,609 - INFO - joeynmt.training - 	Hypothesis: Si tratta di v<unk> @ etro e contr<unk> @ atti in estate .
2025-05-27 01:21:52,609 - INFO - joeynmt.training - Example #4
2025-05-27 01:21:52,609 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-27 01:21:52,609 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-27 01:21:52,609 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'slide', 'slide', 'vi', 'mostrerò', 'che', 'sarà', 'un', 'rapi@@', '<unk>', '@', 'do', 'di', 'rapi@@', '<unk>', '@', 'do', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-27 01:21:52,609 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-27 01:21:52,609 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-27 01:21:52,609 - INFO - joeynmt.training - 	Hypothesis: La slide slide vi mostrerò che sarà un rapi<unk> @ do di rapi<unk> @ do di quello che è successo negli ultimi 25 anni .
2025-05-27 01:22:21,027 - INFO - joeynmt.training - Epoch   7, Step:    32100, Batch Loss:     1.536220, Batch Acc: 0.621595, Tokens per Sec:     2570, Lr: 0.000300
2025-05-27 01:22:50,450 - INFO - joeynmt.training - Epoch   7, Step:    32200, Batch Loss:     1.581047, Batch Acc: 0.626892, Tokens per Sec:     2490, Lr: 0.000300
2025-05-27 01:23:18,528 - INFO - joeynmt.training - Epoch   7, Step:    32300, Batch Loss:     1.313670, Batch Acc: 0.625273, Tokens per Sec:     2643, Lr: 0.000300
2025-05-27 01:23:46,489 - INFO - joeynmt.training - Epoch   7, Step:    32400, Batch Loss:     1.494619, Batch Acc: 0.622259, Tokens per Sec:     2595, Lr: 0.000300
2025-05-27 01:24:14,590 - INFO - joeynmt.training - Epoch   7, Step:    32500, Batch Loss:     1.629271, Batch Acc: 0.624186, Tokens per Sec:     2606, Lr: 0.000300
2025-05-27 01:24:14,590 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 01:26:43,561 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.37, ppl:   3.93, acc:   0.66, generation: 148.9633[sec], evaluation: 0.0000[sec]
2025-05-27 01:26:43,563 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 01:26:43,710 - INFO - joeynmt.helpers - delete models/bpe_8k/29500.ckpt
2025-05-27 01:26:43,720 - INFO - joeynmt.training - Example #0
2025-05-27 01:26:43,720 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-27 01:26:43,720 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-27 01:26:43,720 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'due', 'slide', 'così', 'che', 'dimostr@@', '<unk>', '@', 'ano', 'che', 'il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ant@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'molti', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'dei', 'più', 'più', 'più', 'bassa', 'dei', '4@@', '<unk>', '@', '8', 'stati', ',', 'ha', 'il', '40', '%', '.', '</s>']
2025-05-27 01:26:43,720 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-27 01:26:43,720 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-27 01:26:43,720 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso ho mostrato queste due slide così che dimostr<unk> @ ano che il ghiaccio ar<unk> @ c<unk> @ ant<unk> @ ico , che per molti degli ultimi tre milioni di anni è stata la dimensione dei più più più bassa dei 4<unk> @ 8 stati , ha il 40 % .
2025-05-27 01:26:43,720 - INFO - joeynmt.training - Example #1
2025-05-27 01:26:43,720 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-27 01:26:43,720 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-27 01:26:43,720 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'compren@@', '<unk>', '@', 'de', 'la', 'seri@@', '<unk>', '@', 'età', 'di', 'questo', 'problema', 'perché', 'non', 'mostra', 'il', 'poll@@', '<unk>', '@', 'o', 'del', 'ghiaccio', '.', '</s>']
2025-05-27 01:26:43,721 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-27 01:26:43,721 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-27 01:26:43,721 - INFO - joeynmt.training - 	Hypothesis: Ma questo compren<unk> @ de la seri<unk> @ età di questo problema perché non mostra il poll<unk> @ o del ghiaccio .
2025-05-27 01:26:43,721 - INFO - joeynmt.training - Example #2
2025-05-27 01:26:43,721 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-27 01:26:43,721 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-27 01:26:43,721 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ant@@', '<unk>', '@', 'ico', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'climatico', '.', '</s>']
2025-05-27 01:26:43,721 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-27 01:26:43,721 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-27 01:26:43,721 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio ar<unk> @ c<unk> @ ant<unk> @ ico è , in un certo senso , il cuore del sistema climatico .
2025-05-27 01:26:43,721 - INFO - joeynmt.training - Example #3
2025-05-27 01:26:43,721 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-27 01:26:43,721 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-27 01:26:43,721 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', '<unk>', '@', 'pan@@', '<unk>', '@', 'di', 'v@@', '<unk>', '@', 'etro', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-27 01:26:43,721 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-27 01:26:43,721 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-27 01:26:43,721 - INFO - joeynmt.training - 	Hypothesis: Si es<unk> @ pan<unk> @ di v<unk> @ etro e contr<unk> @ atti in estate .
2025-05-27 01:26:43,721 - INFO - joeynmt.training - Example #4
2025-05-27 01:26:43,721 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-27 01:26:43,721 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-27 01:26:43,721 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'slide', 'che', 'vi', 'mostro', 'che', 'sar@@', '<unk>', '@', 'ò', 'un', 'rapi@@', '<unk>', '@', 'do', 'rapi@@', '<unk>', '@', 'do', 'di', 'cosa', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-27 01:26:43,722 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-27 01:26:43,722 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-27 01:26:43,722 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostro che sar<unk> @ ò un rapi<unk> @ do rapi<unk> @ do di cosa è successo negli ultimi 25 anni .
2025-05-27 01:27:12,019 - INFO - joeynmt.training - Epoch   7, Step:    32600, Batch Loss:     1.358838, Batch Acc: 0.624333, Tokens per Sec:     2595, Lr: 0.000300
2025-05-27 01:27:40,054 - INFO - joeynmt.training - Epoch   7, Step:    32700, Batch Loss:     1.325214, Batch Acc: 0.620788, Tokens per Sec:     2676, Lr: 0.000300
2025-05-27 01:28:07,980 - INFO - joeynmt.training - Epoch   7, Step:    32800, Batch Loss:     1.540596, Batch Acc: 0.628683, Tokens per Sec:     2703, Lr: 0.000300
2025-05-27 01:28:36,000 - INFO - joeynmt.training - Epoch   7, Step:    32900, Batch Loss:     1.336673, Batch Acc: 0.623305, Tokens per Sec:     2660, Lr: 0.000300
2025-05-27 01:29:03,341 - INFO - joeynmt.training - Epoch   7, Step:    33000, Batch Loss:     1.511627, Batch Acc: 0.624930, Tokens per Sec:     2602, Lr: 0.000300
2025-05-27 01:29:03,341 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 01:31:47,681 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.37, ppl:   3.92, acc:   0.66, generation: 164.3322[sec], evaluation: 0.0000[sec]
2025-05-27 01:31:47,683 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 01:31:47,855 - INFO - joeynmt.helpers - delete models/bpe_8k/31000.ckpt
2025-05-27 01:31:47,888 - INFO - joeynmt.training - Example #0
2025-05-27 01:31:47,888 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-27 01:31:47,888 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-27 01:31:47,888 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'due', 'di@@', '<unk>', '@', 'ap@@', '<unk>', '@', 'di@@', '<unk>', '@', 'mostra', 'che', 'il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'dei', 'più', 'più', 'alti', '4@@', '<unk>', '@', '8', 'stati', ',', 'ha', 'ri@@', '<unk>', '@', 'dotto', 'la', 'dimensione', 'dei', '4@@', '<unk>', '@', '8', 'stati', ',', 'ha', 'ri@@', '<unk>', '@', 'mp@@', '<unk>', '@', 'i@@', '<unk>', '@', 'ore', 'del', '40', '%', '.', '</s>']
2025-05-27 01:31:47,889 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-27 01:31:47,889 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-27 01:31:47,889 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso ho mostrato queste due di<unk> @ ap<unk> @ di<unk> @ mostra che il ghiaccio ar<unk> @ c<unk> @ ico , che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione dei più più alti 4<unk> @ 8 stati , ha ri<unk> @ dotto la dimensione dei 4<unk> @ 8 stati , ha ri<unk> @ mp<unk> @ i<unk> @ ore del 40 % .
2025-05-27 01:31:47,889 - INFO - joeynmt.training - Example #1
2025-05-27 01:31:47,889 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-27 01:31:47,889 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-27 01:31:47,889 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'compren@@', '<unk>', '@', 'de', 'la', 'seri@@', '<unk>', '@', 'età', 'di', 'questo', 'problema', 'perché', 'non', 'mostra', 'la', 'poll@@', '<unk>', '@', 'o', 'del', 'ghiaccio', '.', '</s>']
2025-05-27 01:31:47,889 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-27 01:31:47,889 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-27 01:31:47,889 - INFO - joeynmt.training - 	Hypothesis: Ma questo compren<unk> @ de la seri<unk> @ età di questo problema perché non mostra la poll<unk> @ o del ghiaccio .
2025-05-27 01:31:47,889 - INFO - joeynmt.training - Example #2
2025-05-27 01:31:47,889 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-27 01:31:47,889 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-27 01:31:47,889 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'climatico', '.', '</s>']
2025-05-27 01:31:47,889 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-27 01:31:47,889 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-27 01:31:47,889 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio ar<unk> @ c<unk> @ ico è , in un senso , il cuore del sistema climatico .
2025-05-27 01:31:47,889 - INFO - joeynmt.training - Example #3
2025-05-27 01:31:47,889 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-27 01:31:47,889 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-27 01:31:47,889 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', '<unk>', '@', 'amin@@', '<unk>', '@', 'ano', 'in', 'inver@@', '<unk>', '@', 'no', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-27 01:31:47,890 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-27 01:31:47,890 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-27 01:31:47,890 - INFO - joeynmt.training - 	Hypothesis: Si es<unk> @ amin<unk> @ ano in inver<unk> @ no e contr<unk> @ atti in estate .
2025-05-27 01:31:47,890 - INFO - joeynmt.training - Example #4
2025-05-27 01:31:47,890 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-27 01:31:47,890 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-27 01:31:47,890 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'slide', 'vi', 'mostrerò', 'che', 'vi', 'mostrerò', 'sarà', 'un', 'rapi@@', '<unk>', '@', 'do', 'di', 'più', 'rapidamente', 'di', 'quello', 'che', 'è', 'accad@@', '<unk>', '@', 'uto', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-27 01:31:47,890 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-27 01:31:47,890 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-27 01:31:47,890 - INFO - joeynmt.training - 	Hypothesis: La prossima slide vi mostrerò che vi mostrerò sarà un rapi<unk> @ do di più rapidamente di quello che è accad<unk> @ uto negli ultimi 25 anni .
2025-05-27 01:32:16,414 - INFO - joeynmt.training - Epoch   7, Step:    33100, Batch Loss:     1.506865, Batch Acc: 0.627041, Tokens per Sec:     2633, Lr: 0.000300
2025-05-27 01:32:43,272 - INFO - joeynmt.training - Epoch   7, Step:    33200, Batch Loss:     1.535498, Batch Acc: 0.624286, Tokens per Sec:     2725, Lr: 0.000300
2025-05-27 01:33:10,788 - INFO - joeynmt.training - Epoch   7, Step:    33300, Batch Loss:     1.379472, Batch Acc: 0.624630, Tokens per Sec:     2649, Lr: 0.000300
2025-05-27 01:33:38,584 - INFO - joeynmt.training - Epoch   7, Step:    33400, Batch Loss:     1.503511, Batch Acc: 0.623952, Tokens per Sec:     2584, Lr: 0.000300
2025-05-27 01:34:06,518 - INFO - joeynmt.training - Epoch   7, Step:    33500, Batch Loss:     1.333923, Batch Acc: 0.624683, Tokens per Sec:     2552, Lr: 0.000300
2025-05-27 01:34:06,519 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 01:36:34,817 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.37, ppl:   3.92, acc:   0.66, generation: 148.2908[sec], evaluation: 0.0000[sec]
2025-05-27 01:36:34,819 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 01:36:34,961 - INFO - joeynmt.helpers - delete models/bpe_8k/30000.ckpt
2025-05-27 01:36:34,994 - INFO - joeynmt.training - Example #0
2025-05-27 01:36:34,994 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-27 01:36:34,994 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-27 01:36:34,994 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'due', 'di@@', '<unk>', '@', 'ap@@', '<unk>', '@', 'os@@', '<unk>', '@', 'sa', 'così', 'che', 'il', 'ghiaccio', 'di', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'del', '4@@', '<unk>', '@', '8', 'stati', ',', 'ha', 'ri@@', '<unk>', '@', 'dotto', 'dal', '40', '%', '.', '</s>']
2025-05-27 01:36:34,994 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-27 01:36:34,994 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-27 01:36:34,994 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso ho mostrato queste due di<unk> @ ap<unk> @ os<unk> @ sa così che il ghiaccio di ar<unk> @ c<unk> @ ico , che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione del 4<unk> @ 8 stati , ha ri<unk> @ dotto dal 40 % .
2025-05-27 01:36:34,994 - INFO - joeynmt.training - Example #1
2025-05-27 01:36:34,995 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-27 01:36:34,995 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-27 01:36:34,995 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', '<unk>', '@', 'sce', 'la', 'seri@@', '<unk>', '@', 'età', 'di', 'questo', 'particolare', 'problema', 'perché', 'non', 'mostra', 'la', 'th@@', '<unk>', '@', 'ick@@', '<unk>', '@', 'ità', 'del', 'ghiaccio', '.', '</s>']
2025-05-27 01:36:34,995 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-27 01:36:34,995 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-27 01:36:34,995 - INFO - joeynmt.training - 	Hypothesis: Ma questo capi<unk> @ sce la seri<unk> @ età di questo particolare problema perché non mostra la th<unk> @ ick<unk> @ ità del ghiaccio .
2025-05-27 01:36:34,995 - INFO - joeynmt.training - Example #2
2025-05-27 01:36:34,995 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-27 01:36:34,995 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-27 01:36:34,995 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'di', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'climatico', '.', '</s>']
2025-05-27 01:36:34,995 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-27 01:36:34,995 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-27 01:36:34,995 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio di ar<unk> @ c<unk> @ ico è , in un senso , il cuore del sistema climatico .
2025-05-27 01:36:34,995 - INFO - joeynmt.training - Example #3
2025-05-27 01:36:34,995 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-27 01:36:34,995 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-27 01:36:34,995 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', '<unk>', '@', 'amin@@', '<unk>', '@', 'a', 'v@@', '<unk>', '@', 'etro', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-27 01:36:34,995 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-27 01:36:34,995 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-27 01:36:34,995 - INFO - joeynmt.training - 	Hypothesis: Si es<unk> @ amin<unk> @ a v<unk> @ etro e contr<unk> @ atti in estate .
2025-05-27 01:36:34,995 - INFO - joeynmt.training - Example #4
2025-05-27 01:36:34,995 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-27 01:36:34,995 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-27 01:36:34,995 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'slide', 'vi', 'mostro', 'che', 'sarà', 'un', 'passo', 'avanti', 'avanti', 'di', 'quello', 'che', 'è', 'accad@@', '<unk>', '@', 'uto', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-27 01:36:34,996 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-27 01:36:34,996 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-27 01:36:34,996 - INFO - joeynmt.training - 	Hypothesis: La prossima slide vi mostro che sarà un passo avanti avanti di quello che è accad<unk> @ uto negli ultimi 25 anni .
2025-05-27 01:37:03,330 - INFO - joeynmt.training - Epoch   7, Step:    33600, Batch Loss:     1.510327, Batch Acc: 0.626281, Tokens per Sec:     2542, Lr: 0.000300
2025-05-27 01:37:31,305 - INFO - joeynmt.training - Epoch   7, Step:    33700, Batch Loss:     1.517973, Batch Acc: 0.626898, Tokens per Sec:     2597, Lr: 0.000300
2025-05-27 01:37:58,445 - INFO - joeynmt.training - Epoch   7, Step:    33800, Batch Loss:     1.521037, Batch Acc: 0.624339, Tokens per Sec:     2706, Lr: 0.000300
2025-05-27 01:38:26,644 - INFO - joeynmt.training - Epoch   7, Step:    33900, Batch Loss:     1.444475, Batch Acc: 0.627021, Tokens per Sec:     2700, Lr: 0.000300
2025-05-27 01:38:54,640 - INFO - joeynmt.training - Epoch   7, Step:    34000, Batch Loss:     1.509951, Batch Acc: 0.628701, Tokens per Sec:     2609, Lr: 0.000300
2025-05-27 01:38:54,640 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 01:41:22,321 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.36, ppl:   3.90, acc:   0.66, generation: 147.6733[sec], evaluation: 0.0000[sec]
2025-05-27 01:41:22,332 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 01:41:22,474 - INFO - joeynmt.helpers - delete models/bpe_8k/31500.ckpt
2025-05-27 01:41:22,489 - INFO - joeynmt.training - Example #0
2025-05-27 01:41:22,489 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-27 01:41:22,489 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-27 01:41:22,489 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'due', 'slide', 'così', 'che', 'dimostr@@', '<unk>', '@', 'ano', 'che', 'il', 'ghiaccio', "l'", 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', 'che', 'il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'molti', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'dei', '4@@', '<unk>', '@', '8', 'stati', ',', 'ha', 'ri@@', '<unk>', '@', 'dotto', 'il', '40', '%', '.', '</s>']
2025-05-27 01:41:22,490 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-27 01:41:22,490 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-27 01:41:22,490 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso ho mostrato queste due slide così che dimostr<unk> @ ano che il ghiaccio l' ar<unk> @ c<unk> @ ico che il ghiaccio ar<unk> @ c<unk> @ ico , che per molti degli ultimi tre milioni di anni è stata la dimensione dei 4<unk> @ 8 stati , ha ri<unk> @ dotto il 40 % .
2025-05-27 01:41:22,490 - INFO - joeynmt.training - Example #1
2025-05-27 01:41:22,490 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-27 01:41:22,490 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-27 01:41:22,490 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', '<unk>', '@', 'sce', 'la', 'seri@@', '<unk>', '@', 'età', 'di', 'questo', 'problema', 'perché', 'non', 'mostra', 'la', 'th@@', '<unk>', '@', 'ick@@', '<unk>', '@', 'zza', 'del', 'ghiaccio', '.', '</s>']
2025-05-27 01:41:22,490 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-27 01:41:22,490 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-27 01:41:22,490 - INFO - joeynmt.training - 	Hypothesis: Ma questo capi<unk> @ sce la seri<unk> @ età di questo problema perché non mostra la th<unk> @ ick<unk> @ zza del ghiaccio .
2025-05-27 01:41:22,490 - INFO - joeynmt.training - Example #2
2025-05-27 01:41:22,490 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-27 01:41:22,490 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-27 01:41:22,490 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'climatico', '.', '</s>']
2025-05-27 01:41:22,490 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-27 01:41:22,490 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-27 01:41:22,490 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio ar<unk> @ c<unk> @ ico è , in un senso , il cuore del sistema climatico .
2025-05-27 01:41:22,490 - INFO - joeynmt.training - Example #3
2025-05-27 01:41:22,490 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-27 01:41:22,490 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-27 01:41:22,490 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'tratta', 'di', 'v@@', '<unk>', '@', 'asi', 'nel', 'v@@', '<unk>', '@', 'etro', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-27 01:41:22,491 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-27 01:41:22,491 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-27 01:41:22,491 - INFO - joeynmt.training - 	Hypothesis: Si tratta di v<unk> @ asi nel v<unk> @ etro e contr<unk> @ atti in estate .
2025-05-27 01:41:22,491 - INFO - joeynmt.training - Example #4
2025-05-27 01:41:22,491 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-27 01:41:22,491 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-27 01:41:22,491 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'slide', 'vi', 'mostro', 'che', 'sarà', 'un', 'rapi@@', '<unk>', '@', 'do', 'di', 'più', 'rapidamente', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-27 01:41:22,491 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-27 01:41:22,491 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-27 01:41:22,491 - INFO - joeynmt.training - 	Hypothesis: La prossima slide vi mostro che sarà un rapi<unk> @ do di più rapidamente di quello che è successo negli ultimi 25 anni .
2025-05-27 01:41:50,995 - INFO - joeynmt.training - Epoch   7, Step:    34100, Batch Loss:     1.405622, Batch Acc: 0.626169, Tokens per Sec:     2535, Lr: 0.000300
2025-05-27 01:42:18,854 - INFO - joeynmt.training - Epoch   7, Step:    34200, Batch Loss:     1.441719, Batch Acc: 0.627053, Tokens per Sec:     2661, Lr: 0.000300
2025-05-27 01:42:46,913 - INFO - joeynmt.training - Epoch   7, Step:    34300, Batch Loss:     1.527029, Batch Acc: 0.627968, Tokens per Sec:     2626, Lr: 0.000300
2025-05-27 01:43:15,034 - INFO - joeynmt.training - Epoch   7, Step:    34400, Batch Loss:     1.516481, Batch Acc: 0.628453, Tokens per Sec:     2702, Lr: 0.000300
2025-05-27 01:43:42,856 - INFO - joeynmt.training - Epoch   7, Step:    34500, Batch Loss:     1.317470, Batch Acc: 0.626788, Tokens per Sec:     2629, Lr: 0.000300
2025-05-27 01:43:42,856 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 01:46:05,224 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.35, ppl:   3.88, acc:   0.66, generation: 142.3603[sec], evaluation: 0.0000[sec]
2025-05-27 01:46:05,227 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 01:46:05,347 - INFO - joeynmt.helpers - delete models/bpe_8k/32000.ckpt
2025-05-27 01:46:05,372 - INFO - joeynmt.training - Example #0
2025-05-27 01:46:05,373 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-27 01:46:05,373 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-27 01:46:05,373 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'due', 'di@@', '<unk>', '@', 'ap@@', '<unk>', '@', 'os@@', '<unk>', '@', 'sa', 'così', 'che', 'il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', 'di', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'molti', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'del', '40', '%', '.', '</s>']
2025-05-27 01:46:05,373 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-27 01:46:05,373 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-27 01:46:05,373 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso ho mostrato queste due di<unk> @ ap<unk> @ os<unk> @ sa così che il ghiaccio ar<unk> @ c<unk> @ ico di ghiaccio ar<unk> @ c<unk> @ ico , che per molti degli ultimi tre milioni di anni è stata la dimensione del 40 % .
2025-05-27 01:46:05,373 - INFO - joeynmt.training - Example #1
2025-05-27 01:46:05,373 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-27 01:46:05,373 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-27 01:46:05,373 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', '<unk>', '@', 'sce', 'la', 'seri@@', '<unk>', '@', 'età', 'di', 'questo', 'problema', 'perché', 'non', 'mostra', 'la', 'poll@@', '<unk>', '@', 'o', 'del', 'ghiaccio', '.', '</s>']
2025-05-27 01:46:05,373 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-27 01:46:05,373 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-27 01:46:05,373 - INFO - joeynmt.training - 	Hypothesis: Ma questo capi<unk> @ sce la seri<unk> @ età di questo problema perché non mostra la poll<unk> @ o del ghiaccio .
2025-05-27 01:46:05,373 - INFO - joeynmt.training - Example #2
2025-05-27 01:46:05,373 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-27 01:46:05,373 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-27 01:46:05,373 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'climatico', '.', '</s>']
2025-05-27 01:46:05,373 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-27 01:46:05,373 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-27 01:46:05,373 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio ar<unk> @ c<unk> @ ico è , in un certo senso , il cuore del sistema climatico .
2025-05-27 01:46:05,373 - INFO - joeynmt.training - Example #3
2025-05-27 01:46:05,374 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-27 01:46:05,374 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-27 01:46:05,374 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'espan@@', '<unk>', '@', 'de', 'in', 'v@@', '<unk>', '@', 'ter', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-27 01:46:05,374 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-27 01:46:05,374 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-27 01:46:05,374 - INFO - joeynmt.training - 	Hypothesis: Si espan<unk> @ de in v<unk> @ ter e contr<unk> @ atti in estate .
2025-05-27 01:46:05,374 - INFO - joeynmt.training - Example #4
2025-05-27 01:46:05,374 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-27 01:46:05,374 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-27 01:46:05,374 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'slide', 'che', 'vi', 'mostro', 'essere', 'una', 'rapi@@', '<unk>', '@', 'da', 'da', 'da', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-27 01:46:05,374 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-27 01:46:05,374 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-27 01:46:05,374 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostro essere una rapi<unk> @ da da da quello che è successo negli ultimi 25 anni .
2025-05-27 01:46:33,521 - INFO - joeynmt.training - Epoch   7, Step:    34600, Batch Loss:     1.421849, Batch Acc: 0.623808, Tokens per Sec:     2581, Lr: 0.000300
2025-05-27 01:47:02,304 - INFO - joeynmt.training - Epoch   7, Step:    34700, Batch Loss:     1.446178, Batch Acc: 0.625793, Tokens per Sec:     2649, Lr: 0.000300
2025-05-27 01:47:29,780 - INFO - joeynmt.training - Epoch   7, Step:    34800, Batch Loss:     1.413898, Batch Acc: 0.627269, Tokens per Sec:     2725, Lr: 0.000300
2025-05-27 01:47:58,218 - INFO - joeynmt.training - Epoch   7, Step:    34900, Batch Loss:     1.482844, Batch Acc: 0.629529, Tokens per Sec:     2592, Lr: 0.000300
2025-05-27 01:48:05,582 - INFO - joeynmt.training - Epoch   7: total training loss 7231.70
2025-05-27 01:48:05,582 - INFO - joeynmt.training - EPOCH 8
2025-05-27 01:48:25,753 - INFO - joeynmt.training - Epoch   8, Step:    35000, Batch Loss:     1.387858, Batch Acc: 0.640111, Tokens per Sec:     2694, Lr: 0.000300
2025-05-27 01:48:25,753 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 01:50:45,255 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.35, ppl:   3.88, acc:   0.66, generation: 139.4945[sec], evaluation: 0.0000[sec]
2025-05-27 01:50:45,434 - INFO - joeynmt.helpers - delete models/bpe_8k/32500.ckpt
2025-05-27 01:50:45,452 - INFO - joeynmt.training - Example #0
2025-05-27 01:50:45,453 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-27 01:50:45,453 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-27 01:50:45,453 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'scorso', 'ho', 'mostrato', 'queste', 'due', 'di@@', '<unk>', '@', 'ap@@', '<unk>', '@', 'os@@', '<unk>', '@', 'sa', 'così', 'che', 'il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ant@@', '<unk>', '@', 'ica', ',', 'che', 'per', 'molti', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'del', '40', '%', '.', '</s>']
2025-05-27 01:50:45,453 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-27 01:50:45,453 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-27 01:50:45,453 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso scorso ho mostrato queste due di<unk> @ ap<unk> @ os<unk> @ sa così che il ghiaccio ar<unk> @ c<unk> @ ant<unk> @ ica , che per molti degli ultimi tre milioni di anni è stata la dimensione del 40 % .
2025-05-27 01:50:45,453 - INFO - joeynmt.training - Example #1
2025-05-27 01:50:45,453 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-27 01:50:45,453 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-27 01:50:45,453 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', '<unk>', '@', 'sce', 'la', 'seri@@', '<unk>', '@', 'età', 'di', 'questo', 'particolare', 'problema', 'perché', 'non', 'mostra', 'la', 'poll@@', '<unk>', '@', 'o', 'del', 'ghiaccio', '.', '</s>']
2025-05-27 01:50:45,453 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-27 01:50:45,453 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-27 01:50:45,453 - INFO - joeynmt.training - 	Hypothesis: Ma questo capi<unk> @ sce la seri<unk> @ età di questo particolare problema perché non mostra la poll<unk> @ o del ghiaccio .
2025-05-27 01:50:45,453 - INFO - joeynmt.training - Example #2
2025-05-27 01:50:45,453 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-27 01:50:45,453 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-27 01:50:45,453 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ant@@', '<unk>', '@', 'ica', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'del', 'sistema', 'climatico', '.', '</s>']
2025-05-27 01:50:45,453 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-27 01:50:45,453 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-27 01:50:45,454 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio ar<unk> @ c<unk> @ ant<unk> @ ica è , in un senso , il cuore del sistema del sistema climatico .
2025-05-27 01:50:45,454 - INFO - joeynmt.training - Example #3
2025-05-27 01:50:45,454 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-27 01:50:45,454 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-27 01:50:45,454 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', '<unk>', '@', 'amin@@', '<unk>', '@', 'ano', 'in', 'inver@@', '<unk>', '@', 'no', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-27 01:50:45,454 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-27 01:50:45,454 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-27 01:50:45,454 - INFO - joeynmt.training - 	Hypothesis: Si es<unk> @ amin<unk> @ ano in inver<unk> @ no e contr<unk> @ atti in estate .
2025-05-27 01:50:45,454 - INFO - joeynmt.training - Example #4
2025-05-27 01:50:45,454 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-27 01:50:45,454 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-27 01:50:45,454 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'slide', 'successi@@', '<unk>', '@', 'va', 'vi', 'mostro', 'sarà', 'una', 'rapi@@', '<unk>', '@', 'da', 'da', 'più', 'veloce', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-27 01:50:45,454 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-27 01:50:45,454 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-27 01:50:45,454 - INFO - joeynmt.training - 	Hypothesis: La slide successi<unk> @ va vi mostro sarà una rapi<unk> @ da da più veloce di quello che è successo negli ultimi 25 anni .
2025-05-27 01:51:14,251 - INFO - joeynmt.training - Epoch   8, Step:    35100, Batch Loss:     1.460187, Batch Acc: 0.634907, Tokens per Sec:     2528, Lr: 0.000300
2025-05-27 01:51:41,300 - INFO - joeynmt.training - Epoch   8, Step:    35200, Batch Loss:     1.413014, Batch Acc: 0.632577, Tokens per Sec:     2705, Lr: 0.000300
2025-05-27 01:52:09,340 - INFO - joeynmt.training - Epoch   8, Step:    35300, Batch Loss:     1.363099, Batch Acc: 0.632917, Tokens per Sec:     2671, Lr: 0.000300
2025-05-27 01:52:36,901 - INFO - joeynmt.training - Epoch   8, Step:    35400, Batch Loss:     1.427367, Batch Acc: 0.630035, Tokens per Sec:     2603, Lr: 0.000300
2025-05-27 01:53:04,478 - INFO - joeynmt.training - Epoch   8, Step:    35500, Batch Loss:     1.389948, Batch Acc: 0.636932, Tokens per Sec:     2678, Lr: 0.000300
2025-05-27 01:53:04,478 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 01:55:45,362 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.36, ppl:   3.88, acc:   0.66, generation: 160.8763[sec], evaluation: 0.0000[sec]
2025-05-27 01:55:45,484 - INFO - joeynmt.helpers - delete models/bpe_8k/33000.ckpt
2025-05-27 01:55:45,508 - INFO - joeynmt.training - Example #0
2025-05-27 01:55:45,508 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-27 01:55:45,508 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-27 01:55:45,508 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'due', 'di@@', '<unk>', '@', 'ap@@', '<unk>', '@', 'os@@', '<unk>', '@', 'sa', 'così', 'che', 'dimostr@@', '<unk>', '@', 'ano', 'che', 'il', 'ghiaccio', 'di', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ett@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'del', '40', '%', '.', '</s>']
2025-05-27 01:55:45,509 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-27 01:55:45,509 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-27 01:55:45,509 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso ho mostrato queste due di<unk> @ ap<unk> @ os<unk> @ sa così che dimostr<unk> @ ano che il ghiaccio di ghiaccio ar<unk> @ c<unk> @ ett<unk> @ ico , che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione del 40 % .
2025-05-27 01:55:45,509 - INFO - joeynmt.training - Example #1
2025-05-27 01:55:45,509 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-27 01:55:45,509 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-27 01:55:45,509 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'è', 'che', 'la', 'seri@@', '<unk>', '@', 'età', 'di', 'questo', 'particolare', 'problema', 'perché', 'non', 'mostra', 'la', 'poll@@', '<unk>', '@', 'ità', 'del', 'ghiaccio', '.', '</s>']
2025-05-27 01:55:45,509 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-27 01:55:45,509 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-27 01:55:45,509 - INFO - joeynmt.training - 	Hypothesis: Ma questo è che la seri<unk> @ età di questo particolare problema perché non mostra la poll<unk> @ ità del ghiaccio .
2025-05-27 01:55:45,509 - INFO - joeynmt.training - Example #2
2025-05-27 01:55:45,509 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-27 01:55:45,509 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-27 01:55:45,509 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ett@@', '<unk>', '@', 'ico', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'del', 'sistema', 'climat@@', '<unk>', '@', 'ico', '.', '</s>']
2025-05-27 01:55:45,509 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-27 01:55:45,509 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-27 01:55:45,509 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio ar<unk> @ c<unk> @ ett<unk> @ ico è , in un senso , il cuore del sistema del sistema climat<unk> @ ico .
2025-05-27 01:55:45,509 - INFO - joeynmt.training - Example #3
2025-05-27 01:55:45,509 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-27 01:55:45,509 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-27 01:55:45,509 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', '<unk>', '@', 'pon@@', '<unk>', '@', 'gono', 'in', 'inver@@', '<unk>', '@', 'no', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-27 01:55:45,510 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-27 01:55:45,510 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-27 01:55:45,510 - INFO - joeynmt.training - 	Hypothesis: Si es<unk> @ pon<unk> @ gono in inver<unk> @ no e contr<unk> @ atti in estate .
2025-05-27 01:55:45,510 - INFO - joeynmt.training - Example #4
2025-05-27 01:55:45,510 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-27 01:55:45,510 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-27 01:55:45,510 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'di@@', '<unk>', '@', 'ap@@', '<unk>', '@', 'os@@', '<unk>', '@', 'itiva', 'che', 'vi', 'mostrerò', 'sarà', 'una', 'rapi@@', '<unk>', '@', 'da', 'da', 'più', 'veloce', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-27 01:55:45,510 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-27 01:55:45,510 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-27 01:55:45,510 - INFO - joeynmt.training - 	Hypothesis: La prossima di<unk> @ ap<unk> @ os<unk> @ itiva che vi mostrerò sarà una rapi<unk> @ da da più veloce di quello che è successo negli ultimi 25 anni .
2025-05-27 01:56:13,142 - INFO - joeynmt.training - Epoch   8, Step:    35600, Batch Loss:     1.296672, Batch Acc: 0.632769, Tokens per Sec:     2622, Lr: 0.000300
2025-05-27 02:13:14,211 - INFO - joeynmt.training - Epoch   8, Step:    35700, Batch Loss:     1.614034, Batch Acc: 0.630640, Tokens per Sec:       71, Lr: 0.000300
2025-05-27 02:14:45,779 - INFO - joeynmt.training - Epoch   8, Step:    35800, Batch Loss:     1.505561, Batch Acc: 0.635727, Tokens per Sec:      812, Lr: 0.000300
2025-05-27 02:15:12,955 - INFO - joeynmt.training - Epoch   8, Step:    35900, Batch Loss:     1.468009, Batch Acc: 0.633131, Tokens per Sec:     2704, Lr: 0.000300
2025-05-27 02:15:41,793 - INFO - joeynmt.training - Epoch   8, Step:    36000, Batch Loss:     1.521008, Batch Acc: 0.634250, Tokens per Sec:     2504, Lr: 0.000300
2025-05-27 02:15:41,793 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 02:18:05,754 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.35, ppl:   3.87, acc:   0.66, generation: 143.9532[sec], evaluation: 0.0000[sec]
2025-05-27 02:18:05,756 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 02:18:05,873 - INFO - joeynmt.helpers - delete models/bpe_8k/33500.ckpt
2025-05-27 02:18:05,899 - INFO - joeynmt.training - Example #0
2025-05-27 02:18:05,900 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-27 02:18:05,900 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-27 02:18:05,900 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'due', 'di@@', '<unk>', '@', 'ap@@', '<unk>', '@', 'os@@', '<unk>', '@', 'ci@@', '<unk>', '@', 'ol@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'dei', 'più', 'di', '4@@', '<unk>', '@', '8', 'stati', ',', 'ha', 'ri@@', '<unk>', '@', 'dotto', 'il', '40', '%', '.', '</s>']
2025-05-27 02:18:05,900 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-27 02:18:05,900 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-27 02:18:05,900 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso ho mostrato queste due di<unk> @ ap<unk> @ os<unk> @ ci<unk> @ ol<unk> @ ico , che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione dei più di 4<unk> @ 8 stati , ha ri<unk> @ dotto il 40 % .
2025-05-27 02:18:05,900 - INFO - joeynmt.training - Example #1
2025-05-27 02:18:05,900 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-27 02:18:05,900 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-27 02:18:05,900 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', '<unk>', '@', 'sce', 'la', 'seri@@', '<unk>', '@', 'età', 'di', 'questo', 'problema', 'perché', 'non', 'mostra', 'la', 'poll@@', '<unk>', '@', 'o', 'del', 'ghiaccio', '.', '</s>']
2025-05-27 02:18:05,900 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-27 02:18:05,900 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-27 02:18:05,900 - INFO - joeynmt.training - 	Hypothesis: Ma questo capi<unk> @ sce la seri<unk> @ età di questo problema perché non mostra la poll<unk> @ o del ghiaccio .
2025-05-27 02:18:05,900 - INFO - joeynmt.training - Example #2
2025-05-27 02:18:05,900 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-27 02:18:05,900 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-27 02:18:05,900 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'climatico', '.', '</s>']
2025-05-27 02:18:05,901 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-27 02:18:05,901 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-27 02:18:05,901 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio ar<unk> @ c<unk> @ ico è , in un senso , il cuore del sistema climatico .
2025-05-27 02:18:05,901 - INFO - joeynmt.training - Example #3
2025-05-27 02:18:05,901 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-27 02:18:05,901 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-27 02:18:05,901 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', '<unk>', '@', 'pan@@', '<unk>', '@', 'di', 'v@@', '<unk>', '@', 'etro', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-27 02:18:05,901 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-27 02:18:05,901 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-27 02:18:05,901 - INFO - joeynmt.training - 	Hypothesis: Si es<unk> @ pan<unk> @ di v<unk> @ etro e contr<unk> @ atti in estate .
2025-05-27 02:18:05,901 - INFO - joeynmt.training - Example #4
2025-05-27 02:18:05,901 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-27 02:18:05,901 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-27 02:18:05,901 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'di@@', '<unk>', '@', 'ap@@', '<unk>', '@', 'os@@', '<unk>', '@', 'itiva', 'che', 'vi', 'mostro', 'un', 'rapi@@', '<unk>', '@', 'do', 'veloce', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-27 02:18:05,901 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-27 02:18:05,901 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-27 02:18:05,901 - INFO - joeynmt.training - 	Hypothesis: La di<unk> @ ap<unk> @ os<unk> @ itiva che vi mostro un rapi<unk> @ do veloce di quello che è successo negli ultimi 25 anni .
2025-05-27 02:18:34,422 - INFO - joeynmt.training - Epoch   8, Step:    36100, Batch Loss:     1.380212, Batch Acc: 0.634047, Tokens per Sec:     2481, Lr: 0.000300
2025-05-27 02:19:01,693 - INFO - joeynmt.training - Epoch   8, Step:    36200, Batch Loss:     1.523580, Batch Acc: 0.631713, Tokens per Sec:     2761, Lr: 0.000300
2025-05-27 02:19:28,772 - INFO - joeynmt.training - Epoch   8, Step:    36300, Batch Loss:     1.374745, Batch Acc: 0.630402, Tokens per Sec:     2739, Lr: 0.000300
2025-05-27 02:19:57,850 - INFO - joeynmt.training - Epoch   8, Step:    36400, Batch Loss:     1.425136, Batch Acc: 0.633282, Tokens per Sec:     2533, Lr: 0.000300
2025-05-27 02:20:25,414 - INFO - joeynmt.training - Epoch   8, Step:    36500, Batch Loss:     1.321122, Batch Acc: 0.631047, Tokens per Sec:     2650, Lr: 0.000300
2025-05-27 02:20:25,414 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 02:23:05,292 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.35, ppl:   3.87, acc:   0.66, generation: 159.8700[sec], evaluation: 0.0000[sec]
2025-05-27 02:23:05,295 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 02:23:05,413 - INFO - joeynmt.helpers - delete models/bpe_8k/34000.ckpt
2025-05-27 02:23:05,444 - INFO - joeynmt.training - Example #0
2025-05-27 02:23:05,445 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-27 02:23:05,445 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-27 02:23:05,445 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'due', 'l@@', '<unk>', '@', 'os@@', '<unk>', '@', 'sa', 'così', 'che', 'dimostr@@', '<unk>', '@', 'ano', 'che', 'il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'dei', 'più', 'alti', '4@@', '<unk>', '@', '8', 'stati', ',', 'ha', 'ri@@', '<unk>', '@', 'mp@@', '<unk>', '@', 'i@@', '<unk>', '@', 'va', 'dal', '40', '%', '.', '</s>']
2025-05-27 02:23:05,445 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-27 02:23:05,445 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-27 02:23:05,445 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso ho mostrato queste due l<unk> @ os<unk> @ sa così che dimostr<unk> @ ano che il ghiaccio ar<unk> @ c<unk> @ ico , che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione dei più alti 4<unk> @ 8 stati , ha ri<unk> @ mp<unk> @ i<unk> @ va dal 40 % .
2025-05-27 02:23:05,445 - INFO - joeynmt.training - Example #1
2025-05-27 02:23:05,445 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-27 02:23:05,445 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-27 02:23:05,445 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', '<unk>', '@', 'sce', 'la', 'seri@@', '<unk>', '@', 'età', 'di', 'questo', 'problema', 'perché', 'non', 'mostra', 'la', 'poll@@', '<unk>', '@', 'ità', 'del', 'ghiaccio', '.', '</s>']
2025-05-27 02:23:05,445 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-27 02:23:05,445 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-27 02:23:05,445 - INFO - joeynmt.training - 	Hypothesis: Ma questo capi<unk> @ sce la seri<unk> @ età di questo problema perché non mostra la poll<unk> @ ità del ghiaccio .
2025-05-27 02:23:05,445 - INFO - joeynmt.training - Example #2
2025-05-27 02:23:05,445 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-27 02:23:05,445 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-27 02:23:05,445 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'climatico', '.', '</s>']
2025-05-27 02:23:05,446 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-27 02:23:05,446 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-27 02:23:05,446 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio è , in un senso , il cuore del sistema climatico .
2025-05-27 02:23:05,446 - INFO - joeynmt.training - Example #3
2025-05-27 02:23:05,446 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-27 02:23:05,446 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-27 02:23:05,446 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E@@', '<unk>', '@', 'ppure', 'es@@', '<unk>', '@', 'pan@@', '<unk>', '@', 'di', 'v@@', '<unk>', '@', 'enti', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-27 02:23:05,446 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-27 02:23:05,446 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-27 02:23:05,446 - INFO - joeynmt.training - 	Hypothesis: E<unk> @ ppure es<unk> @ pan<unk> @ di v<unk> @ enti e contr<unk> @ atti in estate .
2025-05-27 02:23:05,446 - INFO - joeynmt.training - Example #4
2025-05-27 02:23:05,446 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-27 02:23:05,446 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-27 02:23:05,446 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'slide', 'vi', 'mostrerò', 'sarà', 'una', 'rapi@@', '<unk>', '@', 'da', 'rapi@@', '<unk>', '@', 'do', 'di', 'rapi@@', '<unk>', '@', 'do', 'di', 'ciò', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-27 02:23:05,446 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-27 02:23:05,446 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-27 02:23:05,446 - INFO - joeynmt.training - 	Hypothesis: La slide vi mostrerò sarà una rapi<unk> @ da rapi<unk> @ do di rapi<unk> @ do di ciò che è successo negli ultimi 25 anni .
2025-05-27 02:23:33,154 - INFO - joeynmt.training - Epoch   8, Step:    36600, Batch Loss:     1.509400, Batch Acc: 0.632817, Tokens per Sec:     2608, Lr: 0.000300
2025-05-27 02:24:01,264 - INFO - joeynmt.training - Epoch   8, Step:    36700, Batch Loss:     1.351325, Batch Acc: 0.633025, Tokens per Sec:     2659, Lr: 0.000300
2025-05-27 02:24:29,232 - INFO - joeynmt.training - Epoch   8, Step:    36800, Batch Loss:     1.410586, Batch Acc: 0.634285, Tokens per Sec:     2665, Lr: 0.000300
2025-05-27 02:24:57,159 - INFO - joeynmt.training - Epoch   8, Step:    36900, Batch Loss:     1.439801, Batch Acc: 0.630884, Tokens per Sec:     2622, Lr: 0.000300
2025-05-27 02:25:28,886 - INFO - joeynmt.training - Epoch   8, Step:    37000, Batch Loss:     1.345439, Batch Acc: 0.631870, Tokens per Sec:     2304, Lr: 0.000300
2025-05-27 02:25:28,887 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 02:33:45,779 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.35, ppl:   3.84, acc:   0.66, generation: 496.8846[sec], evaluation: 0.0000[sec]
2025-05-27 02:33:45,781 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 02:33:45,900 - INFO - joeynmt.helpers - delete models/bpe_8k/35500.ckpt
2025-05-27 02:33:45,907 - INFO - joeynmt.training - Example #0
2025-05-27 02:33:45,907 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-27 02:33:45,907 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-27 02:33:45,907 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'due', 'di@@', '<unk>', '@', 'ap@@', '<unk>', '@', 'di@@', '<unk>', '@', 'ap@@', '<unk>', '@', 'os@@', '<unk>', '@', 'sa', 'così', 'che', 'il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ant@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'del', '4@@', '<unk>', '@', '8', 'stati', ',', 'ha', 'ri@@', '<unk>', '@', 'dotto', 'dal', '40', '%', '.', '</s>']
2025-05-27 02:33:45,907 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-27 02:33:45,907 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-27 02:33:45,907 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso ho mostrato queste due di<unk> @ ap<unk> @ di<unk> @ ap<unk> @ os<unk> @ sa così che il ghiaccio ar<unk> @ c<unk> @ ant<unk> @ ico , che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione del 4<unk> @ 8 stati , ha ri<unk> @ dotto dal 40 % .
2025-05-27 02:33:45,907 - INFO - joeynmt.training - Example #1
2025-05-27 02:33:45,907 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-27 02:33:45,907 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-27 02:33:45,907 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', '<unk>', '@', 'sce', 'la', 'seri@@', '<unk>', '@', 'età', 'di', 'questo', 'problema', 'perché', 'non', 'mostra', 'la', 'poll@@', '<unk>', '@', 'ità', 'del', 'ghiaccio', '.', '</s>']
2025-05-27 02:33:45,908 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-27 02:33:45,908 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-27 02:33:45,908 - INFO - joeynmt.training - 	Hypothesis: Ma questo capi<unk> @ sce la seri<unk> @ età di questo problema perché non mostra la poll<unk> @ ità del ghiaccio .
2025-05-27 02:33:45,908 - INFO - joeynmt.training - Example #2
2025-05-27 02:33:45,908 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-27 02:33:45,908 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-27 02:33:45,908 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ant@@', '<unk>', '@', 'ico', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'climatico', '.', '</s>']
2025-05-27 02:33:45,908 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-27 02:33:45,908 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-27 02:33:45,908 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio ar<unk> @ c<unk> @ ant<unk> @ ico è , in un senso , il cuore del sistema climatico .
2025-05-27 02:33:45,908 - INFO - joeynmt.training - Example #3
2025-05-27 02:33:45,908 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-27 02:33:45,908 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-27 02:33:45,908 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', '<unk>', '@', 'pon@@', '<unk>', '@', 'gono', 'in', 'inver@@', '<unk>', '@', 'no', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-27 02:33:45,909 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-27 02:33:45,909 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-27 02:33:45,909 - INFO - joeynmt.training - 	Hypothesis: Si es<unk> @ pon<unk> @ gono in inver<unk> @ no e contr<unk> @ atti in estate .
2025-05-27 02:33:45,909 - INFO - joeynmt.training - Example #4
2025-05-27 02:33:45,909 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-27 02:33:45,909 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-27 02:33:45,909 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'slide', 'vi', 'mostro', 'sarà', 'una', 'rapi@@', '<unk>', '@', 'da', 'da', 'da', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-27 02:33:45,909 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-27 02:33:45,909 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-27 02:33:45,909 - INFO - joeynmt.training - 	Hypothesis: La prossima slide vi mostro sarà una rapi<unk> @ da da da quello che è successo negli ultimi 25 anni .
2025-05-27 02:34:13,477 - INFO - joeynmt.training - Epoch   8, Step:    37100, Batch Loss:     1.269995, Batch Acc: 0.629469, Tokens per Sec:     2625, Lr: 0.000300
2025-05-27 02:34:41,069 - INFO - joeynmt.training - Epoch   8, Step:    37200, Batch Loss:     1.440448, Batch Acc: 0.630363, Tokens per Sec:     2722, Lr: 0.000300
2025-05-27 02:35:08,849 - INFO - joeynmt.training - Epoch   8, Step:    37300, Batch Loss:     1.420505, Batch Acc: 0.633411, Tokens per Sec:     2646, Lr: 0.000300
2025-05-27 02:35:40,652 - INFO - joeynmt.training - Epoch   8, Step:    37400, Batch Loss:     1.409896, Batch Acc: 0.635340, Tokens per Sec:     2276, Lr: 0.000300
2025-05-27 02:36:08,938 - INFO - joeynmt.training - Epoch   8, Step:    37500, Batch Loss:     1.443905, Batch Acc: 0.632308, Tokens per Sec:     2622, Lr: 0.000300
2025-05-27 02:36:08,938 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 02:38:41,680 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.34, ppl:   3.83, acc:   0.66, generation: 152.7340[sec], evaluation: 0.0000[sec]
2025-05-27 02:38:41,681 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 02:38:41,795 - INFO - joeynmt.helpers - delete models/bpe_8k/35000.ckpt
2025-05-27 02:38:41,814 - INFO - joeynmt.training - Example #0
2025-05-27 02:38:41,814 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-27 02:38:41,814 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-27 02:38:41,814 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'due', 'l@@', '<unk>', '@', 'enti', 'così', 'che', 'dimostr@@', '<unk>', '@', 'ano', 'che', 'il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ant@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'del', '40', '%', '.', '</s>']
2025-05-27 02:38:41,814 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-27 02:38:41,814 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-27 02:38:41,814 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso ho mostrato queste due l<unk> @ enti così che dimostr<unk> @ ano che il ghiaccio ar<unk> @ c<unk> @ ant<unk> @ ico , che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione del 40 % .
2025-05-27 02:38:41,814 - INFO - joeynmt.training - Example #1
2025-05-27 02:38:41,814 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-27 02:38:41,814 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-27 02:38:41,814 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', '<unk>', '@', 'sce', 'la', 'seri@@', '<unk>', '@', 'età', 'di', 'questo', 'problema', 'perché', 'non', 'mostra', 'la', 'poll@@', '<unk>', '@', 'o', 'del', 'ghiaccio', '.', '</s>']
2025-05-27 02:38:41,814 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-27 02:38:41,814 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-27 02:38:41,814 - INFO - joeynmt.training - 	Hypothesis: Ma questo capi<unk> @ sce la seri<unk> @ età di questo problema perché non mostra la poll<unk> @ o del ghiaccio .
2025-05-27 02:38:41,814 - INFO - joeynmt.training - Example #2
2025-05-27 02:38:41,814 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-27 02:38:41,814 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-27 02:38:41,814 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'di', 'ghiaccio', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'climatico', '.', '</s>']
2025-05-27 02:38:41,815 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-27 02:38:41,815 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-27 02:38:41,815 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio di ghiaccio è , in un certo senso , il cuore del sistema climatico .
2025-05-27 02:38:41,815 - INFO - joeynmt.training - Example #3
2025-05-27 02:38:41,815 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-27 02:38:41,815 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-27 02:38:41,815 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', '<unk>', '@', 'pan@@', '<unk>', '@', 'e', 'in', 'estate', '.', '</s>']
2025-05-27 02:38:41,815 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-27 02:38:41,815 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-27 02:38:41,815 - INFO - joeynmt.training - 	Hypothesis: Si es<unk> @ pan<unk> @ e in estate .
2025-05-27 02:38:41,815 - INFO - joeynmt.training - Example #4
2025-05-27 02:38:41,815 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-27 02:38:41,815 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-27 02:38:41,815 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'di@@', '<unk>', '@', 'ap@@', '<unk>', '@', 'os@@', '<unk>', '@', 'itiva', 'che', 'vi', 'mostrerò', 'sarà', 'una', 'rapi@@', '<unk>', '@', 'da', 'da', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-27 02:38:41,815 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-27 02:38:41,815 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-27 02:38:41,815 - INFO - joeynmt.training - 	Hypothesis: La prossima di<unk> @ ap<unk> @ os<unk> @ itiva che vi mostrerò sarà una rapi<unk> @ da da quello che è successo negli ultimi 25 anni .
2025-05-27 02:39:09,804 - INFO - joeynmt.training - Epoch   8, Step:    37600, Batch Loss:     1.386498, Batch Acc: 0.632456, Tokens per Sec:     2598, Lr: 0.000300
2025-05-27 02:39:37,307 - INFO - joeynmt.training - Epoch   8, Step:    37700, Batch Loss:     1.554613, Batch Acc: 0.632660, Tokens per Sec:     2694, Lr: 0.000300
2025-05-27 02:40:05,305 - INFO - joeynmt.training - Epoch   8, Step:    37800, Batch Loss:     1.316668, Batch Acc: 0.629929, Tokens per Sec:     2539, Lr: 0.000300
2025-05-27 02:40:32,833 - INFO - joeynmt.training - Epoch   8, Step:    37900, Batch Loss:     1.464750, Batch Acc: 0.627371, Tokens per Sec:     2630, Lr: 0.000300
2025-05-27 02:41:00,818 - INFO - joeynmt.training - Epoch   8, Step:    38000, Batch Loss:     1.357601, Batch Acc: 0.631954, Tokens per Sec:     2590, Lr: 0.000300
2025-05-27 02:41:00,818 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 02:43:34,489 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.34, ppl:   3.83, acc:   0.66, generation: 153.6634[sec], evaluation: 0.0000[sec]
2025-05-27 02:43:34,611 - INFO - joeynmt.helpers - delete models/bpe_8k/34500.ckpt
2025-05-27 02:43:34,638 - INFO - joeynmt.training - Example #0
2025-05-27 02:43:34,638 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-27 02:43:34,638 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-27 02:43:34,638 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'due', 'slide', 'così', 'che', 'dimostr@@', '<unk>', '@', 'ano', 'che', 'il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', 'di', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'del', '40', '%', '.', '</s>']
2025-05-27 02:43:34,639 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-27 02:43:34,639 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-27 02:43:34,639 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso ho mostrato queste due slide così che dimostr<unk> @ ano che il ghiaccio ar<unk> @ c<unk> @ ico di ghiaccio ar<unk> @ c<unk> @ ico , che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione del 40 % .
2025-05-27 02:43:34,639 - INFO - joeynmt.training - Example #1
2025-05-27 02:43:34,639 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-27 02:43:34,639 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-27 02:43:34,639 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', '<unk>', '@', 'sce', 'la', 'seri@@', '<unk>', '@', 'età', 'di', 'questo', 'problema', 'perché', 'non', 'mostra', 'la', 'poll@@', '<unk>', '@', 'o', 'del', 'ghiaccio', '.', '</s>']
2025-05-27 02:43:34,639 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-27 02:43:34,639 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-27 02:43:34,639 - INFO - joeynmt.training - 	Hypothesis: Ma questo capi<unk> @ sce la seri<unk> @ età di questo problema perché non mostra la poll<unk> @ o del ghiaccio .
2025-05-27 02:43:34,639 - INFO - joeynmt.training - Example #2
2025-05-27 02:43:34,639 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-27 02:43:34,639 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-27 02:43:34,639 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'climatico', '.', '</s>']
2025-05-27 02:43:34,639 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-27 02:43:34,639 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-27 02:43:34,639 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio ar<unk> @ c<unk> @ ico è , in un senso , il cuore del sistema climatico .
2025-05-27 02:43:34,639 - INFO - joeynmt.training - Example #3
2025-05-27 02:43:34,639 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-27 02:43:34,639 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-27 02:43:34,639 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', '<unk>', '@', 'ce', 'in', 'inver@@', '<unk>', '@', 'no', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-27 02:43:34,640 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-27 02:43:34,640 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-27 02:43:34,640 - INFO - joeynmt.training - 	Hypothesis: Si es<unk> @ ce in inver<unk> @ no e contr<unk> @ atti in estate .
2025-05-27 02:43:34,640 - INFO - joeynmt.training - Example #4
2025-05-27 02:43:34,640 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-27 02:43:34,640 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-27 02:43:34,640 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'slide', 'vi', 'mostrerò', 'sarà', 'un', 'rapi@@', '<unk>', '@', 'do', 'rapi@@', '<unk>', '@', 'do', 'di', 'cosa', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-27 02:43:34,640 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-27 02:43:34,640 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-27 02:43:34,640 - INFO - joeynmt.training - 	Hypothesis: La prossima slide vi mostrerò sarà un rapi<unk> @ do rapi<unk> @ do di cosa è successo negli ultimi 25 anni .
2025-05-27 02:44:02,197 - INFO - joeynmt.training - Epoch   8, Step:    38100, Batch Loss:     1.513245, Batch Acc: 0.629113, Tokens per Sec:     2574, Lr: 0.000300
2025-05-27 02:44:30,058 - INFO - joeynmt.training - Epoch   8, Step:    38200, Batch Loss:     1.632117, Batch Acc: 0.635304, Tokens per Sec:     2578, Lr: 0.000300
2025-05-27 02:44:58,250 - INFO - joeynmt.training - Epoch   8, Step:    38300, Batch Loss:     1.436663, Batch Acc: 0.634475, Tokens per Sec:     2712, Lr: 0.000300
2025-05-27 02:45:26,834 - INFO - joeynmt.training - Epoch   8, Step:    38400, Batch Loss:     1.447615, Batch Acc: 0.627106, Tokens per Sec:     2565, Lr: 0.000300
2025-05-27 02:45:55,192 - INFO - joeynmt.training - Epoch   8, Step:    38500, Batch Loss:     1.431277, Batch Acc: 0.630828, Tokens per Sec:     2699, Lr: 0.000300
2025-05-27 02:45:55,193 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 02:48:18,974 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.34, ppl:   3.81, acc:   0.66, generation: 143.7736[sec], evaluation: 0.0000[sec]
2025-05-27 02:48:18,977 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 02:48:19,090 - INFO - joeynmt.helpers - delete models/bpe_8k/36000.ckpt
2025-05-27 02:48:19,102 - INFO - joeynmt.training - Example #0
2025-05-27 02:48:19,102 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-27 02:48:19,102 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-27 02:48:19,102 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'due', 'slide', 'così', 'che', 'dimostr@@', '<unk>', '@', 'ano', 'che', 'il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'del', '4@@', '<unk>', '@', '8', 'stati', ',', 'ha', 'ri@@', '<unk>', '@', 'dotto', 'dal', '40', '%', '.', '</s>']
2025-05-27 02:48:19,102 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-27 02:48:19,102 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-27 02:48:19,102 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso ho mostrato queste due slide così che dimostr<unk> @ ano che il ghiaccio ar<unk> @ c<unk> @ ico ar<unk> @ c<unk> @ ico , che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione del 4<unk> @ 8 stati , ha ri<unk> @ dotto dal 40 % .
2025-05-27 02:48:19,102 - INFO - joeynmt.training - Example #1
2025-05-27 02:48:19,102 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-27 02:48:19,102 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-27 02:48:19,102 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', '<unk>', '@', 'sce', 'la', 'seri@@', '<unk>', '@', 'età', 'di', 'questo', 'particolare', 'problema', 'perché', 'non', 'mostra', 'la', 'poll@@', '<unk>', '@', 'o', 'del', 'ghiaccio', '.', '</s>']
2025-05-27 02:48:19,102 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-27 02:48:19,102 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-27 02:48:19,102 - INFO - joeynmt.training - 	Hypothesis: Ma questo capi<unk> @ sce la seri<unk> @ età di questo particolare problema perché non mostra la poll<unk> @ o del ghiaccio .
2025-05-27 02:48:19,102 - INFO - joeynmt.training - Example #2
2025-05-27 02:48:19,102 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-27 02:48:19,102 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-27 02:48:19,103 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'del', 'sistema', 'climatico', '.', '</s>']
2025-05-27 02:48:19,103 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-27 02:48:19,103 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-27 02:48:19,103 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio ar<unk> @ c<unk> @ ico è , in un senso , il cuore del sistema del sistema climatico .
2025-05-27 02:48:19,103 - INFO - joeynmt.training - Example #3
2025-05-27 02:48:19,103 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-27 02:48:19,103 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-27 02:48:19,103 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'espan@@', '<unk>', '@', 'de', 'in', 'inver@@', '<unk>', '@', 'no', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-27 02:48:19,103 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-27 02:48:19,103 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-27 02:48:19,103 - INFO - joeynmt.training - 	Hypothesis: Si espan<unk> @ de in inver<unk> @ no e contr<unk> @ atti in estate .
2025-05-27 02:48:19,103 - INFO - joeynmt.training - Example #4
2025-05-27 02:48:19,103 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-27 02:48:19,103 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-27 02:48:19,103 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'slide', 'vi', 'mostro', 'che', 'vi', 'sarà', 'una', 'rapi@@', '<unk>', '@', 'da', 'da', 'più', 'rapidamente', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-27 02:48:19,103 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-27 02:48:19,103 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-27 02:48:19,103 - INFO - joeynmt.training - 	Hypothesis: La prossima slide vi mostro che vi sarà una rapi<unk> @ da da più rapidamente di quello che è successo negli ultimi 25 anni .
2025-05-27 02:48:47,286 - INFO - joeynmt.training - Epoch   8, Step:    38600, Batch Loss:     1.493843, Batch Acc: 0.631100, Tokens per Sec:     2553, Lr: 0.000300
2025-05-27 02:49:15,120 - INFO - joeynmt.training - Epoch   8, Step:    38700, Batch Loss:     1.498608, Batch Acc: 0.630050, Tokens per Sec:     2588, Lr: 0.000300
2025-05-27 02:49:42,644 - INFO - joeynmt.training - Epoch   8, Step:    38800, Batch Loss:     1.329027, Batch Acc: 0.629204, Tokens per Sec:     2644, Lr: 0.000300
2025-05-27 02:50:12,478 - INFO - joeynmt.training - Epoch   8, Step:    38900, Batch Loss:     1.366446, Batch Acc: 0.635184, Tokens per Sec:     2413, Lr: 0.000300
2025-05-27 03:05:52,415 - INFO - joeynmt.training - Epoch   8, Step:    39000, Batch Loss:     1.620170, Batch Acc: 0.633306, Tokens per Sec:       77, Lr: 0.000300
2025-05-27 03:05:52,415 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 03:41:31,828 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.34, ppl:   3.80, acc:   0.66, generation: 2139.4057[sec], evaluation: 0.0000[sec]
2025-05-27 03:41:31,829 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 03:41:31,988 - INFO - joeynmt.helpers - delete models/bpe_8k/36500.ckpt
2025-05-27 03:41:32,011 - INFO - joeynmt.training - Example #0
2025-05-27 03:41:32,012 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-27 03:41:32,012 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-27 03:41:32,012 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'due', 'slide', 'così', 'che', 'dimostr@@', '<unk>', '@', 'ano', 'che', 'il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'del', '40', '%', '.', '</s>']
2025-05-27 03:41:32,012 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-27 03:41:32,012 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-27 03:41:32,012 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso ho mostrato queste due slide così che dimostr<unk> @ ano che il ghiaccio ar<unk> @ c<unk> @ ico ar<unk> @ c<unk> @ ico , che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione del 40 % .
2025-05-27 03:41:32,012 - INFO - joeynmt.training - Example #1
2025-05-27 03:41:32,012 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-27 03:41:32,012 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-27 03:41:32,012 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', '<unk>', '@', 'sce', 'la', 'seri@@', '<unk>', '@', 'età', 'di', 'questo', 'problema', 'perché', 'non', 'mostra', 'la', 'poll@@', '<unk>', '@', 'o', 'del', 'ghiaccio', '.', '</s>']
2025-05-27 03:41:32,012 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-27 03:41:32,012 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-27 03:41:32,012 - INFO - joeynmt.training - 	Hypothesis: Ma questo capi<unk> @ sce la seri<unk> @ età di questo problema perché non mostra la poll<unk> @ o del ghiaccio .
2025-05-27 03:41:32,012 - INFO - joeynmt.training - Example #2
2025-05-27 03:41:32,012 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-27 03:41:32,012 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-27 03:41:32,012 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', 'di', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'climatico', '.', '</s>']
2025-05-27 03:41:32,012 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-27 03:41:32,012 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-27 03:41:32,012 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio ar<unk> @ c<unk> @ ico di ghiaccio ar<unk> @ c<unk> @ ico è , in un senso , il cuore del sistema climatico .
2025-05-27 03:41:32,013 - INFO - joeynmt.training - Example #3
2025-05-27 03:41:32,013 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-27 03:41:32,013 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-27 03:41:32,013 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', '<unk>', '@', 'ag@@', '<unk>', '@', 'ge', 'in', 'inver@@', '<unk>', '@', 'no', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-27 03:41:32,013 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-27 03:41:32,013 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-27 03:41:32,013 - INFO - joeynmt.training - 	Hypothesis: Si es<unk> @ ag<unk> @ ge in inver<unk> @ no e contr<unk> @ atti in estate .
2025-05-27 03:41:32,013 - INFO - joeynmt.training - Example #4
2025-05-27 03:41:32,013 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-27 03:41:32,013 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-27 03:41:32,013 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'slide', 'vi', 'mostro', 'che', 'vi', 'mostrerò', 'sarà', 'una', 'rapi@@', '<unk>', '@', 'da', 'rapi@@', '<unk>', '@', 'do', 'di', 'quello', 'che', 'è', 'accad@@', '<unk>', '@', 'uto', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-27 03:41:32,013 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-27 03:41:32,013 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-27 03:41:32,013 - INFO - joeynmt.training - 	Hypothesis: La slide vi mostro che vi mostrerò sarà una rapi<unk> @ da rapi<unk> @ do di quello che è accad<unk> @ uto negli ultimi 25 anni .
2025-05-27 03:58:18,540 - INFO - joeynmt.training - Epoch   8, Step:    39100, Batch Loss:     1.442181, Batch Acc: 0.628369, Tokens per Sec:       71, Lr: 0.000300
2025-05-27 04:14:00,670 - INFO - joeynmt.training - Epoch   8, Step:    39200, Batch Loss:     1.533101, Batch Acc: 0.634350, Tokens per Sec:       76, Lr: 0.000300
2025-05-27 04:14:27,764 - INFO - joeynmt.training - Epoch   8, Step:    39300, Batch Loss:     1.409544, Batch Acc: 0.628934, Tokens per Sec:     2674, Lr: 0.000300
2025-05-27 04:29:55,890 - INFO - joeynmt.training - Epoch   8, Step:    39400, Batch Loss:     1.547680, Batch Acc: 0.630700, Tokens per Sec:       82, Lr: 0.000300
2025-05-27 04:30:23,444 - INFO - joeynmt.training - Epoch   8, Step:    39500, Batch Loss:     1.351807, Batch Acc: 0.630271, Tokens per Sec:     2706, Lr: 0.000300
2025-05-27 04:30:23,444 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 05:19:29,993 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.34, ppl:   3.82, acc:   0.66, generation: 2946.5412[sec], evaluation: 0.0000[sec]
2025-05-27 05:19:30,118 - INFO - joeynmt.helpers - delete models/bpe_8k/37000.ckpt
2025-05-27 05:19:30,150 - INFO - joeynmt.training - Example #0
2025-05-27 05:19:30,151 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-27 05:19:30,151 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-27 05:19:30,151 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'due', 'slide', 'così', 'che', 'dimostr@@', '<unk>', '@', 'ano', 'che', 'il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'molti', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'del', '40', '%', '.', '</s>']
2025-05-27 05:19:30,151 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-27 05:19:30,151 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-27 05:19:30,151 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso ho mostrato queste due slide così che dimostr<unk> @ ano che il ghiaccio ar<unk> @ c<unk> @ ico ar<unk> @ c<unk> @ ico , che per molti degli ultimi tre milioni di anni è stata la dimensione del 40 % .
2025-05-27 05:19:30,151 - INFO - joeynmt.training - Example #1
2025-05-27 05:19:30,151 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-27 05:19:30,151 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-27 05:19:30,151 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', '<unk>', '@', 'sce', 'la', 'seri@@', '<unk>', '@', 'età', 'di', 'questo', 'problema', 'perché', 'non', 'mostra', 'la', 'poll@@', '<unk>', '@', 'ità', 'del', 'ghiaccio', '.', '</s>']
2025-05-27 05:19:30,151 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-27 05:19:30,151 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-27 05:19:30,151 - INFO - joeynmt.training - 	Hypothesis: Ma questo capi<unk> @ sce la seri<unk> @ età di questo problema perché non mostra la poll<unk> @ ità del ghiaccio .
2025-05-27 05:19:30,151 - INFO - joeynmt.training - Example #2
2025-05-27 05:19:30,151 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-27 05:19:30,151 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-27 05:19:30,151 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'climatico', '.', '</s>']
2025-05-27 05:19:30,151 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-27 05:19:30,151 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-27 05:19:30,152 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio ar<unk> @ c<unk> @ ico è , in un senso , il cuore del sistema climatico .
2025-05-27 05:19:30,152 - INFO - joeynmt.training - Example #3
2025-05-27 05:19:30,152 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-27 05:19:30,152 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-27 05:19:30,152 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', '<unk>', '@', 'ag@@', '<unk>', '@', 'isce', 'in', 'inver@@', '<unk>', '@', 'no', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-27 05:19:30,152 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-27 05:19:30,152 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-27 05:19:30,152 - INFO - joeynmt.training - 	Hypothesis: Si es<unk> @ ag<unk> @ isce in inver<unk> @ no e contr<unk> @ atti in estate .
2025-05-27 05:19:30,152 - INFO - joeynmt.training - Example #4
2025-05-27 05:19:30,152 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-27 05:19:30,152 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-27 05:19:30,152 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'slide', 'vi', 'mostro', 'che', 'sarà', 'una', 'rapi@@', '<unk>', '@', 'da', 'rapi@@', '<unk>', '@', 'do', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-27 05:19:30,152 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-27 05:19:30,152 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-27 05:19:30,152 - INFO - joeynmt.training - 	Hypothesis: La prossima slide vi mostro che sarà una rapi<unk> @ da rapi<unk> @ do di quello che è successo negli ultimi 25 anni .
2025-05-27 05:19:57,779 - INFO - joeynmt.training - Epoch   8, Step:    39600, Batch Loss:     1.458313, Batch Acc: 0.631805, Tokens per Sec:     2600, Lr: 0.000300
2025-05-27 05:37:44,538 - INFO - joeynmt.training - Epoch   8, Step:    39700, Batch Loss:     1.347154, Batch Acc: 0.631891, Tokens per Sec:       69, Lr: 0.000300
2025-05-27 05:54:21,485 - INFO - joeynmt.training - Epoch   8, Step:    39800, Batch Loss:     1.452955, Batch Acc: 0.636008, Tokens per Sec:       73, Lr: 0.000300
2025-05-27 05:54:49,486 - INFO - joeynmt.training - Epoch   8, Step:    39900, Batch Loss:     1.433527, Batch Acc: 0.631442, Tokens per Sec:     2581, Lr: 0.000300
2025-05-27 06:12:23,577 - INFO - joeynmt.training - Epoch   8: total training loss 7118.71
2025-05-27 06:12:23,577 - INFO - joeynmt.training - EPOCH 9
2025-05-27 06:12:37,656 - INFO - joeynmt.training - Epoch   9, Step:    40000, Batch Loss:     1.368191, Batch Acc: 0.639956, Tokens per Sec:     2732, Lr: 0.000300
2025-05-27 06:12:37,656 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 06:15:02,174 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.34, ppl:   3.82, acc:   0.66, generation: 144.5099[sec], evaluation: 0.0000[sec]
2025-05-27 06:15:02,298 - INFO - joeynmt.helpers - delete models/bpe_8k/38000.ckpt
2025-05-27 06:15:02,334 - INFO - joeynmt.training - Example #0
2025-05-27 06:15:02,334 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-27 06:15:02,334 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-27 06:15:02,334 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'scorso', 'scorso', 'ho', 'mostrato', 'queste', 'due', 'slide', 'così', 'che', 'dimostr@@', '<unk>', '@', 'ano', 'che', 'il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'dei', '4@@', '<unk>', '@', '8', 'stati', ',', 'ha', 'ri@@', '<unk>', '@', 'dotto', 'il', '40', '%', '.', '</s>']
2025-05-27 06:15:02,334 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-27 06:15:02,334 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-27 06:15:02,334 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso scorso scorso ho mostrato queste due slide così che dimostr<unk> @ ano che il ghiaccio ar<unk> @ c<unk> @ ico ar<unk> @ c<unk> @ ico , che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione dei 4<unk> @ 8 stati , ha ri<unk> @ dotto il 40 % .
2025-05-27 06:15:02,334 - INFO - joeynmt.training - Example #1
2025-05-27 06:15:02,334 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-27 06:15:02,334 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-27 06:15:02,334 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', '<unk>', '@', 'sce', 'la', 'seri@@', '<unk>', '@', 'età', 'di', 'questo', 'problema', 'perché', 'non', 'mostra', 'la', 'poll@@', '<unk>', '@', 'ità', 'del', 'ghiaccio', '.', '</s>']
2025-05-27 06:15:02,335 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-27 06:15:02,335 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-27 06:15:02,335 - INFO - joeynmt.training - 	Hypothesis: Ma questo capi<unk> @ sce la seri<unk> @ età di questo problema perché non mostra la poll<unk> @ ità del ghiaccio .
2025-05-27 06:15:02,335 - INFO - joeynmt.training - Example #2
2025-05-27 06:15:02,335 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-27 06:15:02,335 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-27 06:15:02,335 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'climatico', '.', '</s>']
2025-05-27 06:15:02,335 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-27 06:15:02,335 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-27 06:15:02,335 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio ar<unk> @ c<unk> @ ico è , in un certo senso , il cuore del sistema climatico .
2025-05-27 06:15:02,335 - INFO - joeynmt.training - Example #3
2025-05-27 06:15:02,335 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-27 06:15:02,335 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-27 06:15:02,335 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', '<unk>', '@', 'pan@@', '<unk>', '@', 'di', 'inver@@', '<unk>', '@', 'no', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-27 06:15:02,335 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-27 06:15:02,335 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-27 06:15:02,335 - INFO - joeynmt.training - 	Hypothesis: Si es<unk> @ pan<unk> @ di inver<unk> @ no e contr<unk> @ atti in estate .
2025-05-27 06:15:02,335 - INFO - joeynmt.training - Example #4
2025-05-27 06:15:02,335 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-27 06:15:02,335 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-27 06:15:02,335 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'slide', 'vi', 'mostrerò', 'sarà', 'una', 'rapi@@', '<unk>', '@', 'do', 'di', 'più', 'rapi@@', '<unk>', '@', 'do', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-27 06:15:02,336 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-27 06:15:02,336 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-27 06:15:02,336 - INFO - joeynmt.training - 	Hypothesis: La prossima slide vi mostrerò sarà una rapi<unk> @ do di più rapi<unk> @ do di quello che è successo negli ultimi 25 anni .
2025-05-27 06:31:26,178 - INFO - joeynmt.training - Epoch   9, Step:    40100, Batch Loss:     1.332962, Batch Acc: 0.636460, Tokens per Sec:       76, Lr: 0.000300
2025-05-27 06:31:54,531 - INFO - joeynmt.training - Epoch   9, Step:    40200, Batch Loss:     1.566092, Batch Acc: 0.637583, Tokens per Sec:     2595, Lr: 0.000300
2025-05-27 06:49:54,587 - INFO - joeynmt.training - Epoch   9, Step:    40300, Batch Loss:     1.620000, Batch Acc: 0.637828, Tokens per Sec:       68, Lr: 0.000300
2025-05-27 06:50:21,534 - INFO - joeynmt.training - Epoch   9, Step:    40400, Batch Loss:     1.363787, Batch Acc: 0.636314, Tokens per Sec:     2738, Lr: 0.000300
2025-05-27 07:06:00,534 - INFO - joeynmt.training - Epoch   9, Step:    40500, Batch Loss:     1.441476, Batch Acc: 0.640645, Tokens per Sec:       77, Lr: 0.000300
2025-05-27 07:06:00,535 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 07:25:01,908 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.34, ppl:   3.81, acc:   0.66, generation: 1141.3651[sec], evaluation: 0.0000[sec]
2025-05-27 07:25:02,102 - INFO - joeynmt.helpers - delete models/bpe_8k/37500.ckpt
2025-05-27 07:25:02,155 - INFO - joeynmt.training - Example #0
2025-05-27 07:25:02,155 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-27 07:25:02,155 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-27 07:25:02,155 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'due', 'di@@', '<unk>', '@', 'os@@', '<unk>', '@', 'sa', 'così', 'che', 'il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'molti', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'dei', '4@@', '<unk>', '@', '8', 'stati', ',', 'ha', 'ri@@', '<unk>', '@', 'dotto', 'il', '40', '%', '.', '</s>']
2025-05-27 07:25:02,155 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-27 07:25:02,155 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-27 07:25:02,155 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso ho mostrato queste due di<unk> @ os<unk> @ sa così che il ghiaccio ar<unk> @ c<unk> @ ico , che per molti degli ultimi tre milioni di anni è stata la dimensione dei 4<unk> @ 8 stati , ha ri<unk> @ dotto il 40 % .
2025-05-27 07:25:02,155 - INFO - joeynmt.training - Example #1
2025-05-27 07:25:02,155 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-27 07:25:02,155 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-27 07:25:02,155 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', '<unk>', '@', 'sce', 'la', 'seri@@', '<unk>', '@', 'età', 'di', 'questo', 'particolare', 'problema', 'perché', 'non', 'mostra', 'la', 'poll@@', '<unk>', '@', 'o', 'del', 'ghiaccio', '.', '</s>']
2025-05-27 07:25:02,155 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-27 07:25:02,155 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-27 07:25:02,155 - INFO - joeynmt.training - 	Hypothesis: Ma questo capi<unk> @ sce la seri<unk> @ età di questo particolare problema perché non mostra la poll<unk> @ o del ghiaccio .
2025-05-27 07:25:02,155 - INFO - joeynmt.training - Example #2
2025-05-27 07:25:02,156 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-27 07:25:02,156 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-27 07:25:02,156 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'del', 'sistema', 'climatico', '.', '</s>']
2025-05-27 07:25:02,156 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-27 07:25:02,156 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-27 07:25:02,156 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio ar<unk> @ c<unk> @ ico è , in un senso , il cuore del sistema del sistema climatico .
2025-05-27 07:25:02,156 - INFO - joeynmt.training - Example #3
2025-05-27 07:25:02,156 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-27 07:25:02,156 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-27 07:25:02,156 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'espan@@', '<unk>', '@', 'de', 'in', 'inver@@', '<unk>', '@', 'no', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-27 07:25:02,156 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-27 07:25:02,156 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-27 07:25:02,156 - INFO - joeynmt.training - 	Hypothesis: Si espan<unk> @ de in inver<unk> @ no e contr<unk> @ atti in estate .
2025-05-27 07:25:02,156 - INFO - joeynmt.training - Example #4
2025-05-27 07:25:02,156 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-27 07:25:02,156 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-27 07:25:02,156 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'slide', 'vi', 'mostrerò', 'sarà', 'una', 'rapi@@', '<unk>', '@', 'da', 'rapi@@', '<unk>', '@', 'da', 'da', 'da', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-27 07:25:02,156 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-27 07:25:02,156 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-27 07:25:02,156 - INFO - joeynmt.training - 	Hypothesis: La slide vi mostrerò sarà una rapi<unk> @ da rapi<unk> @ da da da quello che è successo negli ultimi 25 anni .
2025-05-27 07:25:29,615 - INFO - joeynmt.training - Epoch   9, Step:    40600, Batch Loss:     1.268620, Batch Acc: 0.638665, Tokens per Sec:     2637, Lr: 0.000300
2025-05-27 07:25:57,514 - INFO - joeynmt.training - Epoch   9, Step:    40700, Batch Loss:     1.397349, Batch Acc: 0.647375, Tokens per Sec:     2706, Lr: 0.000300
2025-05-27 07:26:25,497 - INFO - joeynmt.training - Epoch   9, Step:    40800, Batch Loss:     1.386298, Batch Acc: 0.639187, Tokens per Sec:     2666, Lr: 0.000300
2025-05-27 07:26:52,444 - INFO - joeynmt.training - Epoch   9, Step:    40900, Batch Loss:     1.388729, Batch Acc: 0.639604, Tokens per Sec:     2673, Lr: 0.000300
2025-05-27 07:44:52,771 - INFO - joeynmt.training - Epoch   9, Step:    41000, Batch Loss:     1.332688, Batch Acc: 0.637010, Tokens per Sec:       68, Lr: 0.000300
2025-05-27 07:44:52,772 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 08:05:09,448 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.33, ppl:   3.78, acc:   0.66, generation: 1216.6690[sec], evaluation: 0.0000[sec]
2025-05-27 08:05:09,449 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 08:05:09,795 - INFO - joeynmt.helpers - delete models/bpe_8k/39500.ckpt
2025-05-27 08:05:09,827 - INFO - joeynmt.training - Example #0
2025-05-27 08:05:09,827 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-27 08:05:09,827 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-27 08:05:09,828 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'due', 'l@@', '<unk>', '@', 'os@@', '<unk>', '@', 'sa', 'così', 'che', 'il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'molti', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'degli', 'stati', ',', 'ha', 'ri@@', '<unk>', '@', 'vol@@', '<unk>', '@', 'on@@', '<unk>', '@', 'tà', 'del', '40', '%', '.', '</s>']
2025-05-27 08:05:09,828 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-27 08:05:09,828 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-27 08:05:09,828 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso ho mostrato queste due l<unk> @ os<unk> @ sa così che il ghiaccio ar<unk> @ c<unk> @ ico , che per molti degli ultimi tre milioni di anni è stata la dimensione degli ultimi tre milioni di anni è stata la dimensione degli ultimi tre milioni di anni è stata la dimensione degli stati , ha ri<unk> @ vol<unk> @ on<unk> @ tà del 40 % .
2025-05-27 08:05:09,828 - INFO - joeynmt.training - Example #1
2025-05-27 08:05:09,828 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-27 08:05:09,828 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-27 08:05:09,828 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'compren@@', '<unk>', '@', 'de', 'la', 'seri@@', '<unk>', '@', 'età', 'di', 'questo', 'particolare', 'problema', 'perché', 'non', 'mostra', 'la', 'th@@', '<unk>', '@', 'ick@@', '<unk>', '@', 'ità', 'del', 'ghiaccio', '.', '</s>']
2025-05-27 08:05:09,828 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-27 08:05:09,828 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-27 08:05:09,828 - INFO - joeynmt.training - 	Hypothesis: Ma questo compren<unk> @ de la seri<unk> @ età di questo particolare problema perché non mostra la th<unk> @ ick<unk> @ ità del ghiaccio .
2025-05-27 08:05:09,828 - INFO - joeynmt.training - Example #2
2025-05-27 08:05:09,828 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-27 08:05:09,828 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-27 08:05:09,828 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'climatico', '.', '</s>']
2025-05-27 08:05:09,828 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-27 08:05:09,828 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-27 08:05:09,828 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio ar<unk> @ c<unk> @ ico è , in un senso , il cuore del sistema climatico .
2025-05-27 08:05:09,828 - INFO - joeynmt.training - Example #3
2025-05-27 08:05:09,828 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-27 08:05:09,828 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-27 08:05:09,828 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', 'v@@', '<unk>', '@', 'enti', 'in', 'inver@@', '<unk>', '@', 'no', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-27 08:05:09,829 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-27 08:05:09,829 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-27 08:05:09,829 - INFO - joeynmt.training - 	Hypothesis: I v<unk> @ enti in inver<unk> @ no e contr<unk> @ atti in estate .
2025-05-27 08:05:09,829 - INFO - joeynmt.training - Example #4
2025-05-27 08:05:09,829 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-27 08:05:09,829 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-27 08:05:09,829 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'slide', 'vi', 'mostro', 'che', 'sarà', 'un', 'rapi@@', '<unk>', '@', 'do', 'rapi@@', '<unk>', '@', 'do', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-27 08:05:09,829 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-27 08:05:09,829 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-27 08:05:09,829 - INFO - joeynmt.training - 	Hypothesis: La prossima slide vi mostro che sarà un rapi<unk> @ do rapi<unk> @ do di quello che è successo negli ultimi 25 anni .
2025-05-27 08:21:14,349 - INFO - joeynmt.training - Epoch   9, Step:    41100, Batch Loss:     1.351040, Batch Acc: 0.640496, Tokens per Sec:       78, Lr: 0.000300
2025-05-27 08:21:41,690 - INFO - joeynmt.training - Epoch   9, Step:    41200, Batch Loss:     1.388050, Batch Acc: 0.638246, Tokens per Sec:     2639, Lr: 0.000300
2025-05-27 08:38:01,791 - INFO - joeynmt.training - Epoch   9, Step:    41300, Batch Loss:     1.328617, Batch Acc: 0.637279, Tokens per Sec:       74, Lr: 0.000300
2025-05-27 08:38:29,690 - INFO - joeynmt.training - Epoch   9, Step:    41400, Batch Loss:     1.412612, Batch Acc: 0.634352, Tokens per Sec:     2583, Lr: 0.000300
2025-05-27 08:54:33,637 - INFO - joeynmt.training - Epoch   9, Step:    41500, Batch Loss:     1.404488, Batch Acc: 0.637315, Tokens per Sec:       79, Lr: 0.000300
2025-05-27 08:54:33,637 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 09:43:25,178 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.33, ppl:   3.78, acc:   0.66, generation: 2931.5337[sec], evaluation: 0.0000[sec]
2025-05-27 09:43:25,180 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 09:43:25,327 - INFO - joeynmt.helpers - delete models/bpe_8k/40000.ckpt
2025-05-27 09:43:25,417 - INFO - joeynmt.training - Example #0
2025-05-27 09:43:25,418 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-27 09:43:25,418 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-27 09:43:25,418 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'due', 'di@@', '<unk>', '@', 'ap@@', '<unk>', '@', 'os@@', '<unk>', '@', 'sa', 'così', 'che', 'il', 'ghiaccio', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'dei', 'più', 'grandi', '4@@', '<unk>', '@', '8', 'stati', ',', 'ha', 'ri@@', '<unk>', '@', 'dotto', 'il', '40', '%', '.', '</s>']
2025-05-27 09:43:25,418 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-27 09:43:25,418 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-27 09:43:25,418 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso ho mostrato queste due di<unk> @ ap<unk> @ os<unk> @ sa così che il ghiaccio , che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione dei più grandi 4<unk> @ 8 stati , ha ri<unk> @ dotto il 40 % .
2025-05-27 09:43:25,418 - INFO - joeynmt.training - Example #1
2025-05-27 09:43:25,418 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-27 09:43:25,418 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-27 09:43:25,418 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', '<unk>', '@', 'sce', 'la', 'seri@@', '<unk>', '@', 'età', 'di', 'questo', 'problema', 'perché', 'non', 'mostra', 'la', 'quantità', 'di', 'ghiaccio', '.', '</s>']
2025-05-27 09:43:25,418 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-27 09:43:25,418 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-27 09:43:25,418 - INFO - joeynmt.training - 	Hypothesis: Ma questo capi<unk> @ sce la seri<unk> @ età di questo problema perché non mostra la quantità di ghiaccio .
2025-05-27 09:43:25,418 - INFO - joeynmt.training - Example #2
2025-05-27 09:43:25,418 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-27 09:43:25,418 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-27 09:43:25,418 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'climatico', '.', '</s>']
2025-05-27 09:43:25,419 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-27 09:43:25,419 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-27 09:43:25,419 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio ar<unk> @ c<unk> @ ico è , in un senso , il cuore del sistema climatico .
2025-05-27 09:43:25,419 - INFO - joeynmt.training - Example #3
2025-05-27 09:43:25,419 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-27 09:43:25,419 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-27 09:43:25,419 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', '<unk>', '@', 'ag@@', '<unk>', '@', 'ge', 'in', 'inver@@', '<unk>', '@', 'no', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-27 09:43:25,419 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-27 09:43:25,419 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-27 09:43:25,419 - INFO - joeynmt.training - 	Hypothesis: Si es<unk> @ ag<unk> @ ge in inver<unk> @ no e contr<unk> @ atti in estate .
2025-05-27 09:43:25,419 - INFO - joeynmt.training - Example #4
2025-05-27 09:43:25,419 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-27 09:43:25,419 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-27 09:43:25,419 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'slide', 'vi', 'mostrerò', 'una', 'cosa', 'rapi@@', '<unk>', '@', 'da', 'da', 'un', 'rapi@@', '<unk>', '@', 'do', 'rapi@@', '<unk>', '@', 'da', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-27 09:43:25,419 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-27 09:43:25,419 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-27 09:43:25,419 - INFO - joeynmt.training - 	Hypothesis: La slide vi mostrerò una cosa rapi<unk> @ da da un rapi<unk> @ do rapi<unk> @ da di quello che è successo negli ultimi 25 anni .
2025-05-27 09:43:54,061 - INFO - joeynmt.training - Epoch   9, Step:    41600, Batch Loss:     1.288532, Batch Acc: 0.636308, Tokens per Sec:     2605, Lr: 0.000300
2025-05-27 10:00:53,186 - INFO - joeynmt.training - Epoch   9, Step:    41700, Batch Loss:     1.459345, Batch Acc: 0.633512, Tokens per Sec:       71, Lr: 0.000300
2025-05-27 10:01:22,552 - INFO - joeynmt.training - Epoch   9, Step:    41800, Batch Loss:     1.439026, Batch Acc: 0.633321, Tokens per Sec:     2479, Lr: 0.000300
2025-05-27 10:16:52,156 - INFO - joeynmt.training - Epoch   9, Step:    41900, Batch Loss:     1.628582, Batch Acc: 0.633903, Tokens per Sec:       78, Lr: 0.000300
2025-05-27 10:34:00,564 - INFO - joeynmt.training - Epoch   9, Step:    42000, Batch Loss:     1.424929, Batch Acc: 0.639146, Tokens per Sec:       72, Lr: 0.000300
2025-05-27 10:34:00,564 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 10:56:36,169 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.33, ppl:   3.77, acc:   0.66, generation: 1355.5965[sec], evaluation: 0.0000[sec]
2025-05-27 10:56:36,170 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 10:56:36,333 - INFO - joeynmt.helpers - delete models/bpe_8k/38500.ckpt
2025-05-27 10:56:36,414 - INFO - joeynmt.training - Example #0
2025-05-27 10:56:36,414 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-27 10:56:36,414 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-27 10:56:36,414 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'questi', 'due', 'slide', 'così', 'che', 'dimostr@@', '<unk>', '@', 'ano', 'che', 'il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', 'di', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'del', '40', '%', '.', '</s>']
2025-05-27 10:56:36,414 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-27 10:56:36,414 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-27 10:56:36,414 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso ho mostrato questi due slide così che dimostr<unk> @ ano che il ghiaccio ar<unk> @ c<unk> @ ico di ghiaccio ar<unk> @ c<unk> @ ico , che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione del 40 % .
2025-05-27 10:56:36,414 - INFO - joeynmt.training - Example #1
2025-05-27 10:56:36,414 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-27 10:56:36,414 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-27 10:56:36,415 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', '<unk>', '@', 'sce', 'la', 'seri@@', '<unk>', '@', 'età', 'di', 'questo', 'problema', 'perché', 'non', 'mostra', 'la', 'poll@@', '<unk>', '@', 'o', 'del', 'ghiaccio', '.', '</s>']
2025-05-27 10:56:36,415 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-27 10:56:36,415 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-27 10:56:36,415 - INFO - joeynmt.training - 	Hypothesis: Ma questo capi<unk> @ sce la seri<unk> @ età di questo problema perché non mostra la poll<unk> @ o del ghiaccio .
2025-05-27 10:56:36,415 - INFO - joeynmt.training - Example #2
2025-05-27 10:56:36,415 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-27 10:56:36,415 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-27 10:56:36,415 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'climatico', '.', '</s>']
2025-05-27 10:56:36,415 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-27 10:56:36,415 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-27 10:56:36,415 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio ar<unk> @ c<unk> @ ico è , in un certo senso , il cuore del sistema climatico .
2025-05-27 10:56:36,415 - INFO - joeynmt.training - Example #3
2025-05-27 10:56:36,415 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-27 10:56:36,415 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-27 10:56:36,415 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'tratta', 'di', 'vinc@@', '<unk>', '@', 'ere', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-27 10:56:36,415 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-27 10:56:36,415 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-27 10:56:36,415 - INFO - joeynmt.training - 	Hypothesis: Si tratta di vinc<unk> @ ere e contr<unk> @ atti in estate .
2025-05-27 10:56:36,415 - INFO - joeynmt.training - Example #4
2025-05-27 10:56:36,415 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-27 10:56:36,415 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-27 10:56:36,415 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'slide', 'vi', 'mostrerò', 'un', 'f@@', '<unk>', '@', 'ast@@', '<unk>', '@', 'ante', 'rapi@@', '<unk>', '@', 'do', 'di', 'quello', 'che', 'è', 'accad@@', '<unk>', '@', 'uto', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-27 10:56:36,416 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-27 10:56:36,416 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-27 10:56:36,416 - INFO - joeynmt.training - 	Hypothesis: La prossima slide vi mostrerò un f<unk> @ ast<unk> @ ante rapi<unk> @ do di quello che è accad<unk> @ uto negli ultimi 25 anni .
2025-05-27 10:57:05,462 - INFO - joeynmt.training - Epoch   9, Step:    42100, Batch Loss:     1.380774, Batch Acc: 0.634813, Tokens per Sec:     2496, Lr: 0.000300
2025-05-27 10:57:33,432 - INFO - joeynmt.training - Epoch   9, Step:    42200, Batch Loss:     1.418335, Batch Acc: 0.634520, Tokens per Sec:     2596, Lr: 0.000300
2025-05-27 10:58:01,734 - INFO - joeynmt.training - Epoch   9, Step:    42300, Batch Loss:     1.321531, Batch Acc: 0.637847, Tokens per Sec:     2561, Lr: 0.000300
2025-05-27 10:58:30,193 - INFO - joeynmt.training - Epoch   9, Step:    42400, Batch Loss:     1.392714, Batch Acc: 0.634161, Tokens per Sec:     2620, Lr: 0.000300
2025-05-27 10:58:58,478 - INFO - joeynmt.training - Epoch   9, Step:    42500, Batch Loss:     1.468736, Batch Acc: 0.638987, Tokens per Sec:     2656, Lr: 0.000300
2025-05-27 10:58:58,478 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 11:01:15,907 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.32, ppl:   3.73, acc:   0.67, generation: 137.4213[sec], evaluation: 0.0000[sec]
2025-05-27 11:01:15,908 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 11:01:16,042 - INFO - joeynmt.helpers - delete models/bpe_8k/40500.ckpt
2025-05-27 11:01:16,147 - INFO - joeynmt.training - Example #0
2025-05-27 11:01:16,147 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-27 11:01:16,147 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-27 11:01:16,147 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'questi', 'due', 'di@@', '<unk>', '@', 'ap@@', '<unk>', '@', 'os@@', '<unk>', '@', 'sa', 'così', 'che', 'il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'molti', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'del', 'più', 'di', '4@@', '<unk>', '@', '8', 'stati', ',', 'ha', 'ri@@', '<unk>', '@', 'dotto', 'dal', '40', '%', '.', '</s>']
2025-05-27 11:01:16,147 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-27 11:01:16,147 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-27 11:01:16,147 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso ho mostrato questi due di<unk> @ ap<unk> @ os<unk> @ sa così che il ghiaccio ar<unk> @ c<unk> @ ico , che per molti degli ultimi tre milioni di anni è stata la dimensione del più di 4<unk> @ 8 stati , ha ri<unk> @ dotto dal 40 % .
2025-05-27 11:01:16,147 - INFO - joeynmt.training - Example #1
2025-05-27 11:01:16,147 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-27 11:01:16,147 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-27 11:01:16,147 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'compren@@', '<unk>', '@', 'de', 'la', 'seri@@', '<unk>', '@', 'età', 'di', 'questo', 'particolare', 'problema', 'perché', 'non', 'mostra', 'la', 'quantità', 'di', 'ghiaccio', '.', '</s>']
2025-05-27 11:01:16,147 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-27 11:01:16,147 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-27 11:01:16,147 - INFO - joeynmt.training - 	Hypothesis: Ma questo compren<unk> @ de la seri<unk> @ età di questo particolare problema perché non mostra la quantità di ghiaccio .
2025-05-27 11:01:16,147 - INFO - joeynmt.training - Example #2
2025-05-27 11:01:16,148 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-27 11:01:16,148 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-27 11:01:16,148 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'climatico', '.', '</s>']
2025-05-27 11:01:16,148 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-27 11:01:16,148 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-27 11:01:16,148 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio ar<unk> @ c<unk> @ ico è , in un senso , il cuore del sistema climatico .
2025-05-27 11:01:16,148 - INFO - joeynmt.training - Example #3
2025-05-27 11:01:16,148 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-27 11:01:16,148 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-27 11:01:16,148 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', '<unk>', '@', 'pan@@', '<unk>', '@', 'di', 'inver@@', '<unk>', '@', 'no', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-27 11:01:16,148 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-27 11:01:16,148 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-27 11:01:16,148 - INFO - joeynmt.training - 	Hypothesis: Si es<unk> @ pan<unk> @ di inver<unk> @ no e contr<unk> @ atti in estate .
2025-05-27 11:01:16,148 - INFO - joeynmt.training - Example #4
2025-05-27 11:01:16,148 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-27 11:01:16,148 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-27 11:01:16,148 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'slide', 'vi', 'mostro', 'che', 'sarà', 'un', 'f@@', '<unk>', '@', 'ast@@', '<unk>', '@', 'ante', 'rapi@@', '<unk>', '@', 'do', 'di', 'cosa', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-27 11:01:16,148 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-27 11:01:16,148 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-27 11:01:16,148 - INFO - joeynmt.training - 	Hypothesis: La prossima slide vi mostro che sarà un f<unk> @ ast<unk> @ ante rapi<unk> @ do di cosa è successo negli ultimi 25 anni .
2025-05-27 11:01:44,061 - INFO - joeynmt.training - Epoch   9, Step:    42600, Batch Loss:     1.747914, Batch Acc: 0.633993, Tokens per Sec:     2621, Lr: 0.000300
2025-05-27 11:02:12,792 - INFO - joeynmt.training - Epoch   9, Step:    42700, Batch Loss:     1.672150, Batch Acc: 0.635109, Tokens per Sec:     2575, Lr: 0.000300
2025-05-27 11:02:39,890 - INFO - joeynmt.training - Epoch   9, Step:    42800, Batch Loss:     1.301296, Batch Acc: 0.636694, Tokens per Sec:     2742, Lr: 0.000300
2025-05-27 11:03:07,845 - INFO - joeynmt.training - Epoch   9, Step:    42900, Batch Loss:     1.333087, Batch Acc: 0.637299, Tokens per Sec:     2656, Lr: 0.000300
2025-05-27 11:03:36,561 - INFO - joeynmt.training - Epoch   9, Step:    43000, Batch Loss:     1.371933, Batch Acc: 0.638030, Tokens per Sec:     2604, Lr: 0.000300
2025-05-27 11:03:36,561 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 11:06:15,900 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.32, ppl:   3.76, acc:   0.66, generation: 159.3310[sec], evaluation: 0.0000[sec]
2025-05-27 11:06:16,059 - INFO - joeynmt.helpers - delete models/bpe_8k/39000.ckpt
2025-05-27 11:06:16,162 - INFO - joeynmt.training - Example #0
2025-05-27 11:06:16,162 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-27 11:06:16,162 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-27 11:06:16,162 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'due', 'di@@', '<unk>', '@', 'ap@@', '<unk>', '@', 'os@@', '<unk>', '@', 'sa', 'così', 'che', 'il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', 'di', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'molti', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'del', '40', '%', '.', '</s>']
2025-05-27 11:06:16,162 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-27 11:06:16,162 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-27 11:06:16,162 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso ho mostrato queste due di<unk> @ ap<unk> @ os<unk> @ sa così che il ghiaccio ar<unk> @ c<unk> @ ico di ghiaccio ar<unk> @ c<unk> @ ico , che per molti degli ultimi tre milioni di anni è stata la dimensione del 40 % .
2025-05-27 11:06:16,162 - INFO - joeynmt.training - Example #1
2025-05-27 11:06:16,162 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-27 11:06:16,162 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-27 11:06:16,162 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'compren@@', '<unk>', '@', 'de', 'la', 'seri@@', '<unk>', '@', 'età', 'di', 'questo', 'particolare', 'problema', 'perché', 'non', 'mostra', 'la', 'poll@@', '<unk>', '@', 'o', 'del', 'ghiaccio', '.', '</s>']
2025-05-27 11:06:16,162 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-27 11:06:16,162 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-27 11:06:16,162 - INFO - joeynmt.training - 	Hypothesis: Ma questo compren<unk> @ de la seri<unk> @ età di questo particolare problema perché non mostra la poll<unk> @ o del ghiaccio .
2025-05-27 11:06:16,162 - INFO - joeynmt.training - Example #2
2025-05-27 11:06:16,162 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-27 11:06:16,162 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-27 11:06:16,163 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'globale', '.', '</s>']
2025-05-27 11:06:16,163 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-27 11:06:16,163 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-27 11:06:16,163 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio ar<unk> @ c<unk> @ ico è , in un senso , il cuore del sistema globale .
2025-05-27 11:06:16,163 - INFO - joeynmt.training - Example #3
2025-05-27 11:06:16,163 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-27 11:06:16,163 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-27 11:06:16,163 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', '<unk>', '@', 'ag@@', '<unk>', '@', 'ine', 'in', 'inver@@', '<unk>', '@', 'no', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-27 11:06:16,163 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-27 11:06:16,163 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-27 11:06:16,163 - INFO - joeynmt.training - 	Hypothesis: Si es<unk> @ ag<unk> @ ine in inver<unk> @ no e contr<unk> @ atti in estate .
2025-05-27 11:06:16,163 - INFO - joeynmt.training - Example #4
2025-05-27 11:06:16,163 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-27 11:06:16,163 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-27 11:06:16,163 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'slide', 'vi', 'mostrerò', 'che', 'sarà', 'un', 'rapi@@', '<unk>', '@', 'do', 'di', 'più', 'rapi@@', '<unk>', '@', 'do', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-27 11:06:16,163 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-27 11:06:16,163 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-27 11:06:16,163 - INFO - joeynmt.training - 	Hypothesis: La prossima slide vi mostrerò che sarà un rapi<unk> @ do di più rapi<unk> @ do di quello che è successo negli ultimi 25 anni .
2025-05-27 11:06:44,057 - INFO - joeynmt.training - Epoch   9, Step:    43100, Batch Loss:     1.348465, Batch Acc: 0.640478, Tokens per Sec:     2631, Lr: 0.000300
2025-05-27 11:07:12,051 - INFO - joeynmt.training - Epoch   9, Step:    43200, Batch Loss:     1.593445, Batch Acc: 0.635152, Tokens per Sec:     2675, Lr: 0.000300
2025-05-27 11:07:40,865 - INFO - joeynmt.training - Epoch   9, Step:    43300, Batch Loss:     1.330503, Batch Acc: 0.636443, Tokens per Sec:     2553, Lr: 0.000300
2025-05-27 11:08:10,124 - INFO - joeynmt.training - Epoch   9, Step:    43400, Batch Loss:     1.275667, Batch Acc: 0.631103, Tokens per Sec:     2523, Lr: 0.000300
2025-05-27 11:08:37,785 - INFO - joeynmt.training - Epoch   9, Step:    43500, Batch Loss:     1.426443, Batch Acc: 0.633978, Tokens per Sec:     2704, Lr: 0.000300
2025-05-27 11:08:37,785 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 11:11:06,665 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.32, ppl:   3.75, acc:   0.67, generation: 148.8724[sec], evaluation: 0.0000[sec]
2025-05-27 11:11:06,800 - INFO - joeynmt.helpers - delete models/bpe_8k/41000.ckpt
2025-05-27 11:11:06,823 - INFO - joeynmt.training - Example #0
2025-05-27 11:11:06,823 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-27 11:11:06,823 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-27 11:11:06,823 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'due', 'di@@', '<unk>', '@', 'ap@@', '<unk>', '@', 'os@@', '<unk>', '@', 'sa', 'così', 'che', 'il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'di', '4@@', '<unk>', '@', '8', 'stati', ',', 'ha', 'ri@@', '<unk>', '@', 'dotto', 'la', 'dimensione', 'del', '40', '%', '.', '</s>']
2025-05-27 11:11:06,823 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-27 11:11:06,823 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-27 11:11:06,823 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso ho mostrato queste due di<unk> @ ap<unk> @ os<unk> @ sa così che il ghiaccio ar<unk> @ c<unk> @ ico , che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione di 4<unk> @ 8 stati , ha ri<unk> @ dotto la dimensione del 40 % .
2025-05-27 11:11:06,823 - INFO - joeynmt.training - Example #1
2025-05-27 11:11:06,823 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-27 11:11:06,823 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-27 11:11:06,823 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', '<unk>', '@', 'sce', 'la', 'seri@@', '<unk>', '@', 'età', 'di', 'questo', 'particolare', 'problema', 'perché', 'non', 'mostra', 'la', 'poll@@', '<unk>', '@', 'o', 'del', 'ghiaccio', '.', '</s>']
2025-05-27 11:11:06,823 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-27 11:11:06,823 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-27 11:11:06,823 - INFO - joeynmt.training - 	Hypothesis: Ma questo capi<unk> @ sce la seri<unk> @ età di questo particolare problema perché non mostra la poll<unk> @ o del ghiaccio .
2025-05-27 11:11:06,824 - INFO - joeynmt.training - Example #2
2025-05-27 11:11:06,824 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-27 11:11:06,824 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-27 11:11:06,824 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'climatico', '.', '</s>']
2025-05-27 11:11:06,824 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-27 11:11:06,824 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-27 11:11:06,824 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio ar<unk> @ c<unk> @ ico è , in un senso , il cuore del sistema climatico .
2025-05-27 11:11:06,824 - INFO - joeynmt.training - Example #3
2025-05-27 11:11:06,824 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-27 11:11:06,824 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-27 11:11:06,824 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', '<unk>', '@', 'ag@@', '<unk>', '@', 'ine', 'in', 'inver@@', '<unk>', '@', 'no', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-27 11:11:06,824 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-27 11:11:06,824 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-27 11:11:06,824 - INFO - joeynmt.training - 	Hypothesis: Si es<unk> @ ag<unk> @ ine in inver<unk> @ no e contr<unk> @ atti in estate .
2025-05-27 11:11:06,824 - INFO - joeynmt.training - Example #4
2025-05-27 11:11:06,824 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-27 11:11:06,824 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-27 11:11:06,824 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'slide', 'vi', 'mostrerò', 'una', 'rapi@@', '<unk>', '@', 'da', 'da', 'più', 'rapi@@', '<unk>', '@', 'do', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-27 11:11:06,824 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-27 11:11:06,824 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-27 11:11:06,824 - INFO - joeynmt.training - 	Hypothesis: La prossima slide vi mostrerò una rapi<unk> @ da da più rapi<unk> @ do di quello che è successo negli ultimi 25 anni .
2025-05-27 11:11:35,168 - INFO - joeynmt.training - Epoch   9, Step:    43600, Batch Loss:     1.397122, Batch Acc: 0.640379, Tokens per Sec:     2582, Lr: 0.000300
2025-05-27 11:12:02,984 - INFO - joeynmt.training - Epoch   9, Step:    43700, Batch Loss:     1.298673, Batch Acc: 0.636353, Tokens per Sec:     2644, Lr: 0.000300
2025-05-27 11:12:30,826 - INFO - joeynmt.training - Epoch   9, Step:    43800, Batch Loss:     1.423431, Batch Acc: 0.632907, Tokens per Sec:     2684, Lr: 0.000300
2025-05-27 11:12:58,047 - INFO - joeynmt.training - Epoch   9, Step:    43900, Batch Loss:     1.406200, Batch Acc: 0.637913, Tokens per Sec:     2705, Lr: 0.000300
2025-05-27 11:13:25,749 - INFO - joeynmt.training - Epoch   9, Step:    44000, Batch Loss:     1.387640, Batch Acc: 0.639008, Tokens per Sec:     2753, Lr: 0.000300
2025-05-27 11:13:25,749 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 11:15:55,783 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.32, ppl:   3.73, acc:   0.67, generation: 150.0261[sec], evaluation: 0.0000[sec]
2025-05-27 11:15:55,785 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 11:15:55,964 - INFO - joeynmt.helpers - delete models/bpe_8k/41500.ckpt
2025-05-27 11:15:55,977 - INFO - joeynmt.training - Example #0
2025-05-27 11:15:55,977 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-27 11:15:55,977 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-27 11:15:55,978 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'due', 'slide', 'così', 'che', 'dimostr@@', '<unk>', '@', 'ano', 'che', 'il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ica', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'dei', '4@@', '<unk>', '@', '8', 'stati', 'la', 'dimensione', 'più', 'bassa', 'dei', '4@@', '<unk>', '@', '8', 'stati', ',', 'ha', 'il', '40', '%', '.', '</s>']
2025-05-27 11:15:55,978 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-27 11:15:55,978 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-27 11:15:55,978 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso ho mostrato queste due slide così che dimostr<unk> @ ano che il ghiaccio ar<unk> @ c<unk> @ ica , che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione dei 4<unk> @ 8 stati la dimensione più bassa dei 4<unk> @ 8 stati , ha il 40 % .
2025-05-27 11:15:55,978 - INFO - joeynmt.training - Example #1
2025-05-27 11:15:55,978 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-27 11:15:55,978 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-27 11:15:55,978 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', '<unk>', '@', 'sce', 'la', 'seri@@', '<unk>', '@', 'età', 'di', 'questo', 'particolare', 'problema', 'perché', 'non', 'mostra', 'la', 'poll@@', '<unk>', '@', 'o', 'del', 'ghiaccio', '.', '</s>']
2025-05-27 11:15:55,978 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-27 11:15:55,978 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-27 11:15:55,978 - INFO - joeynmt.training - 	Hypothesis: Ma questo capi<unk> @ sce la seri<unk> @ età di questo particolare problema perché non mostra la poll<unk> @ o del ghiaccio .
2025-05-27 11:15:55,978 - INFO - joeynmt.training - Example #2
2025-05-27 11:15:55,978 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-27 11:15:55,978 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-27 11:15:55,978 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'del', 'sistema', 'climatico', '.', '</s>']
2025-05-27 11:15:55,978 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-27 11:15:55,978 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-27 11:15:55,978 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio ar<unk> @ c<unk> @ ico è , in un senso , il cuore del sistema del sistema climatico .
2025-05-27 11:15:55,978 - INFO - joeynmt.training - Example #3
2025-05-27 11:15:55,978 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-27 11:15:55,978 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-27 11:15:55,978 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', '<unk>', '@', 'ag@@', '<unk>', '@', 'ge', 'nel', 'vinc@@', '<unk>', '@', 'itore', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-27 11:15:55,979 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-27 11:15:55,979 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-27 11:15:55,979 - INFO - joeynmt.training - 	Hypothesis: Si es<unk> @ ag<unk> @ ge nel vinc<unk> @ itore e contr<unk> @ atti in estate .
2025-05-27 11:15:55,979 - INFO - joeynmt.training - Example #4
2025-05-27 11:15:55,979 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-27 11:15:55,979 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-27 11:15:55,979 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'slide', 'vi', 'mostrerò', 'sarà', 'un', 'passo', 'veloce', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-27 11:15:55,979 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-27 11:15:55,979 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-27 11:15:55,979 - INFO - joeynmt.training - 	Hypothesis: La prossima slide vi mostrerò sarà un passo veloce di quello che è successo negli ultimi 25 anni .
2025-05-27 11:17:40,877 - INFO - joeynmt.training - Epoch   9, Step:    44100, Batch Loss:     1.221373, Batch Acc: 0.639390, Tokens per Sec:      692, Lr: 0.000300
2025-05-27 11:18:10,593 - INFO - joeynmt.training - Epoch   9, Step:    44200, Batch Loss:     1.281414, Batch Acc: 0.641158, Tokens per Sec:     2419, Lr: 0.000300
2025-05-27 11:18:38,649 - INFO - joeynmt.training - Epoch   9, Step:    44300, Batch Loss:     1.339037, Batch Acc: 0.635632, Tokens per Sec:     2712, Lr: 0.000300
2025-05-27 11:33:20,418 - INFO - joeynmt.training - Epoch   9, Step:    44400, Batch Loss:     1.433215, Batch Acc: 0.634104, Tokens per Sec:       84, Lr: 0.000300
2025-05-27 11:33:48,632 - INFO - joeynmt.training - Epoch   9, Step:    44500, Batch Loss:     1.398904, Batch Acc: 0.635043, Tokens per Sec:     2581, Lr: 0.000300
2025-05-27 11:33:48,632 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 11:36:28,905 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.31, ppl:   3.72, acc:   0.67, generation: 160.2641[sec], evaluation: 0.0000[sec]
2025-05-27 11:36:28,906 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 11:36:29,063 - INFO - joeynmt.helpers - delete models/bpe_8k/42000.ckpt
2025-05-27 11:36:29,070 - INFO - joeynmt.training - Example #0
2025-05-27 11:36:29,070 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-27 11:36:29,070 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-27 11:36:29,070 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'due', 'slide', 'così', 'che', 'dimostr@@', '<unk>', '@', 'ano', 'che', 'il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'del', '40', '%', '.', '</s>']
2025-05-27 11:36:29,070 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-27 11:36:29,070 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-27 11:36:29,070 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso ho mostrato queste due slide così che dimostr<unk> @ ano che il ghiaccio ar<unk> @ c<unk> @ ico ar<unk> @ c<unk> @ ico , che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione del 40 % .
2025-05-27 11:36:29,070 - INFO - joeynmt.training - Example #1
2025-05-27 11:36:29,070 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-27 11:36:29,070 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-27 11:36:29,070 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', '<unk>', '@', 'sce', 'la', 'seri@@', '<unk>', '@', 'età', 'di', 'questo', 'problema', 'perché', 'non', 'mostra', 'la', 'poll@@', '<unk>', '@', 'o', 'del', 'ghiaccio', '.', '</s>']
2025-05-27 11:36:29,071 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-27 11:36:29,071 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-27 11:36:29,071 - INFO - joeynmt.training - 	Hypothesis: Ma questo capi<unk> @ sce la seri<unk> @ età di questo problema perché non mostra la poll<unk> @ o del ghiaccio .
2025-05-27 11:36:29,071 - INFO - joeynmt.training - Example #2
2025-05-27 11:36:29,071 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-27 11:36:29,071 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-27 11:36:29,071 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'climatico', '.', '</s>']
2025-05-27 11:36:29,071 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-27 11:36:29,071 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-27 11:36:29,071 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio ar<unk> @ c<unk> @ ico è , in un senso , il cuore del sistema climatico .
2025-05-27 11:36:29,071 - INFO - joeynmt.training - Example #3
2025-05-27 11:36:29,071 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-27 11:36:29,071 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-27 11:36:29,071 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', 'es@@', '<unk>', '@', 'pan@@', '<unk>', '@', 'di', 'inver@@', '<unk>', '@', 'no', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-27 11:36:29,071 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-27 11:36:29,071 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-27 11:36:29,071 - INFO - joeynmt.training - 	Hypothesis: I es<unk> @ pan<unk> @ di inver<unk> @ no e contr<unk> @ atti in estate .
2025-05-27 11:36:29,071 - INFO - joeynmt.training - Example #4
2025-05-27 11:36:29,071 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-27 11:36:29,071 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-27 11:36:29,071 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'slide', 'vi', 'mostro', 'che', 'sarà', 'un', 'passo', 'avanti', 'di', 'ciò', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-27 11:36:29,072 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-27 11:36:29,072 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-27 11:36:29,072 - INFO - joeynmt.training - 	Hypothesis: La slide vi mostro che sarà un passo avanti di ciò che è successo negli ultimi 25 anni .
2025-05-27 11:36:57,871 - INFO - joeynmt.training - Epoch   9, Step:    44600, Batch Loss:     1.166500, Batch Acc: 0.635405, Tokens per Sec:     2501, Lr: 0.000300
2025-05-27 11:37:26,419 - INFO - joeynmt.training - Epoch   9, Step:    44700, Batch Loss:     1.499639, Batch Acc: 0.639406, Tokens per Sec:     2549, Lr: 0.000300
2025-05-27 11:37:54,806 - INFO - joeynmt.training - Epoch   9, Step:    44800, Batch Loss:     1.272592, Batch Acc: 0.636210, Tokens per Sec:     2577, Lr: 0.000300
2025-05-27 11:38:24,184 - INFO - joeynmt.training - Epoch   9, Step:    44900, Batch Loss:     1.451486, Batch Acc: 0.637782, Tokens per Sec:     2483, Lr: 0.000300
2025-05-27 11:38:34,193 - INFO - joeynmt.training - Epoch   9: total training loss 6944.61
2025-05-27 11:38:34,194 - INFO - joeynmt.training - EPOCH 10
2025-05-27 11:38:52,864 - INFO - joeynmt.training - Epoch  10, Step:    45000, Batch Loss:     1.359847, Batch Acc: 0.639435, Tokens per Sec:     2586, Lr: 0.000300
2025-05-27 11:38:52,865 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 11:41:23,420 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.32, ppl:   3.74, acc:   0.67, generation: 150.5470[sec], evaluation: 0.0000[sec]
2025-05-27 11:41:23,548 - INFO - joeynmt.helpers - delete models/bpe_8k/43000.ckpt
2025-05-27 11:41:23,558 - INFO - joeynmt.training - Example #0
2025-05-27 11:41:23,558 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-27 11:41:23,558 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-27 11:41:23,558 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'due', 'slide', 'così', 'che', 'dimostr@@', '<unk>', '@', 'are', 'che', 'il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'del', '4@@', '<unk>', '@', '8', 'stati', 'la', 'dimensione', 'del', '4@@', '<unk>', '@', '8', 'stati', 'la', 'dimensione', 'del', '40', '%', '.', '</s>']
2025-05-27 11:41:23,558 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-27 11:41:23,558 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-27 11:41:23,558 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso ho mostrato queste due slide così che dimostr<unk> @ are che il ghiaccio ar<unk> @ c<unk> @ ico ar<unk> @ c<unk> @ ico , che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione del 4<unk> @ 8 stati la dimensione del 4<unk> @ 8 stati la dimensione del 40 % .
2025-05-27 11:41:23,558 - INFO - joeynmt.training - Example #1
2025-05-27 11:41:23,558 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-27 11:41:23,558 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-27 11:41:23,558 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', '<unk>', '@', 'sce', 'la', 'seri@@', '<unk>', '@', 'età', 'di', 'questo', 'problema', 'perché', 'non', 'mostra', 'la', 'poll@@', '<unk>', '@', 'o', 'del', 'ghiaccio', '.', '</s>']
2025-05-27 11:41:23,558 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-27 11:41:23,558 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-27 11:41:23,558 - INFO - joeynmt.training - 	Hypothesis: Ma questo capi<unk> @ sce la seri<unk> @ età di questo problema perché non mostra la poll<unk> @ o del ghiaccio .
2025-05-27 11:41:23,558 - INFO - joeynmt.training - Example #2
2025-05-27 11:41:23,558 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-27 11:41:23,558 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-27 11:41:23,558 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'climatico', '.', '</s>']
2025-05-27 11:41:23,559 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-27 11:41:23,559 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-27 11:41:23,559 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio ar<unk> @ c<unk> @ ico è , in un senso , il cuore del sistema climatico .
2025-05-27 11:41:23,559 - INFO - joeynmt.training - Example #3
2025-05-27 11:41:23,559 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-27 11:41:23,559 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-27 11:41:23,559 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', '<unk>', '@', 'ag@@', '<unk>', '@', 'ge', 'in', 'inver@@', '<unk>', '@', 'no', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-27 11:41:23,559 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-27 11:41:23,559 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-27 11:41:23,559 - INFO - joeynmt.training - 	Hypothesis: Si es<unk> @ ag<unk> @ ge in inver<unk> @ no e contr<unk> @ atti in estate .
2025-05-27 11:41:23,559 - INFO - joeynmt.training - Example #4
2025-05-27 11:41:23,559 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-27 11:41:23,559 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-27 11:41:23,559 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'slide', 'vi', 'mostrerò', 'un', 'rapi@@', '<unk>', '@', 'do', 'rapi@@', '<unk>', '@', 'do', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-27 11:41:23,559 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-27 11:41:23,559 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-27 11:41:23,559 - INFO - joeynmt.training - 	Hypothesis: La prossima slide vi mostrerò un rapi<unk> @ do rapi<unk> @ do di quello che è successo negli ultimi 25 anni .
2025-05-27 11:41:51,935 - INFO - joeynmt.training - Epoch  10, Step:    45100, Batch Loss:     1.466031, Batch Acc: 0.647537, Tokens per Sec:     2613, Lr: 0.000300
2025-05-27 11:42:20,416 - INFO - joeynmt.training - Epoch  10, Step:    45200, Batch Loss:     1.249582, Batch Acc: 0.647904, Tokens per Sec:     2582, Lr: 0.000300
2025-05-27 11:42:49,661 - INFO - joeynmt.training - Epoch  10, Step:    45300, Batch Loss:     1.307054, Batch Acc: 0.644684, Tokens per Sec:     2580, Lr: 0.000300
2025-05-27 11:43:18,096 - INFO - joeynmt.training - Epoch  10, Step:    45400, Batch Loss:     1.282936, Batch Acc: 0.646097, Tokens per Sec:     2667, Lr: 0.000300
2025-05-27 11:43:46,705 - INFO - joeynmt.training - Epoch  10, Step:    45500, Batch Loss:     1.418011, Batch Acc: 0.640146, Tokens per Sec:     2608, Lr: 0.000300
2025-05-27 11:43:46,706 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 11:46:17,128 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.32, ppl:   3.75, acc:   0.67, generation: 150.4141[sec], evaluation: 0.0000[sec]
2025-05-27 11:46:17,258 - INFO - joeynmt.helpers - delete models/bpe_8k/43500.ckpt
2025-05-27 11:46:17,265 - INFO - joeynmt.training - Example #0
2025-05-27 11:46:17,265 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-27 11:46:17,265 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-27 11:46:17,265 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'due', 'slide', 'così', 'che', 'dimostr@@', '<unk>', '@', 'ano', 'che', 'il', 'ghiaccio', "l'", 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', 'del', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'molti', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'degli', 'ultimi', '4@@', '<unk>', '@', '8', 'stati', ',', 'ha', 'ri@@', '<unk>', '@', 'dotto', 'da', '40', '%', '.', '</s>']
2025-05-27 11:46:17,265 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-27 11:46:17,265 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-27 11:46:17,266 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso ho mostrato queste due slide così che dimostr<unk> @ ano che il ghiaccio l' ar<unk> @ c<unk> @ ico del ghiaccio ar<unk> @ c<unk> @ ico , che per molti degli ultimi tre milioni di anni è stata la dimensione degli ultimi tre milioni di anni è stata la dimensione degli ultimi tre milioni di anni è stata la dimensione degli ultimi 4<unk> @ 8 stati , ha ri<unk> @ dotto da 40 % .
2025-05-27 11:46:17,266 - INFO - joeynmt.training - Example #1
2025-05-27 11:46:17,266 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-27 11:46:17,266 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-27 11:46:17,266 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'compren@@', '<unk>', '@', 'de', 'la', 'seri@@', '<unk>', '@', 'età', 'di', 'questo', 'particolare', 'problema', 'perché', 'non', 'mostra', 'la', 'poll@@', '<unk>', '@', 'o', 'del', 'ghiaccio', '.', '</s>']
2025-05-27 11:46:17,266 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-27 11:46:17,266 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-27 11:46:17,266 - INFO - joeynmt.training - 	Hypothesis: Ma questo compren<unk> @ de la seri<unk> @ età di questo particolare problema perché non mostra la poll<unk> @ o del ghiaccio .
2025-05-27 11:46:17,266 - INFO - joeynmt.training - Example #2
2025-05-27 11:46:17,266 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-27 11:46:17,266 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-27 11:46:17,266 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'climatico', '.', '</s>']
2025-05-27 11:46:17,266 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-27 11:46:17,266 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-27 11:46:17,266 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio ar<unk> @ c<unk> @ ico è , in un senso , il cuore del sistema climatico .
2025-05-27 11:46:17,266 - INFO - joeynmt.training - Example #3
2025-05-27 11:46:17,266 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-27 11:46:17,266 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-27 11:46:17,266 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', '<unk>', '@', 'ag@@', '<unk>', '@', 'isce', 'in', 'inver@@', '<unk>', '@', 'no', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-27 11:46:17,266 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-27 11:46:17,266 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-27 11:46:17,266 - INFO - joeynmt.training - 	Hypothesis: Si es<unk> @ ag<unk> @ isce in inver<unk> @ no e contr<unk> @ atti in estate .
2025-05-27 11:46:17,266 - INFO - joeynmt.training - Example #4
2025-05-27 11:46:17,267 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-27 11:46:17,267 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-27 11:46:17,267 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'slide', 'vi', 'mostrerò', 'essere', 'un', 'rapi@@', '<unk>', '@', 'do', 'rapidamente', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-27 11:46:17,267 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-27 11:46:17,267 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-27 11:46:17,267 - INFO - joeynmt.training - 	Hypothesis: La prossima slide vi mostrerò essere un rapi<unk> @ do rapidamente di quello che è successo negli ultimi 25 anni .
2025-05-27 11:46:45,618 - INFO - joeynmt.training - Epoch  10, Step:    45600, Batch Loss:     1.409975, Batch Acc: 0.638692, Tokens per Sec:     2469, Lr: 0.000300
2025-05-27 11:47:13,860 - INFO - joeynmt.training - Epoch  10, Step:    45700, Batch Loss:     1.248432, Batch Acc: 0.642218, Tokens per Sec:     2589, Lr: 0.000300
2025-05-27 11:47:41,751 - INFO - joeynmt.training - Epoch  10, Step:    45800, Batch Loss:     1.462941, Batch Acc: 0.646032, Tokens per Sec:     2693, Lr: 0.000300
2025-05-27 11:48:10,031 - INFO - joeynmt.training - Epoch  10, Step:    45900, Batch Loss:     1.485888, Batch Acc: 0.638878, Tokens per Sec:     2609, Lr: 0.000300
2025-05-27 11:48:38,651 - INFO - joeynmt.training - Epoch  10, Step:    46000, Batch Loss:     1.234779, Batch Acc: 0.640843, Tokens per Sec:     2583, Lr: 0.000300
2025-05-27 11:48:38,651 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 11:51:04,985 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.32, ppl:   3.73, acc:   0.67, generation: 146.3263[sec], evaluation: 0.0000[sec]
2025-05-27 11:51:05,101 - INFO - joeynmt.helpers - delete models/bpe_8k/45500.ckpt
2025-05-27 11:51:05,104 - INFO - joeynmt.helpers - delete /Users/liuxduan/Desktop/mt-exercise-4-master/models/bpe_8k/45500.ckpt
2025-05-27 11:51:05,104 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /Users/liuxduan/Desktop/mt-exercise-4-master/models/bpe_8k/45500.ckpt but file does not exist. ([Errno 2] No such file or directory: '/Users/liuxduan/Desktop/mt-exercise-4-master/models/bpe_8k/45500.ckpt')
2025-05-27 11:51:05,104 - INFO - joeynmt.training - Example #0
2025-05-27 11:51:05,105 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-27 11:51:05,105 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-27 11:51:05,105 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'due', 'slide', 'così', 'che', 'dimostr@@', '<unk>', '@', 'ano', 'che', 'il', 'ghiaccio', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ico', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'molti', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'degli', 'ultimi', '4@@', '<unk>', '@', '8', 'stati', ',', 'ha', 'la', 'dimensione', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'degli', 'ultimi', '4@@', '<unk>', '@', '8', 'stati', ',', 'ha', 'la', 'dimensione', 'del', '40', '%', '.', '</s>']
2025-05-27 11:51:05,105 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-27 11:51:05,105 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-27 11:51:05,105 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso ho mostrato queste due slide così che dimostr<unk> @ ano che il ghiaccio ar<unk> @ t<unk> @ ico ar<unk> @ t<unk> @ ico , che per molti degli ultimi tre milioni di anni è stata la dimensione degli ultimi tre milioni di anni è stata la dimensione degli ultimi 4<unk> @ 8 stati , ha la dimensione degli ultimi tre milioni di anni è stata la dimensione degli ultimi 4<unk> @ 8 stati , ha la dimensione del 40 % .
2025-05-27 11:51:05,105 - INFO - joeynmt.training - Example #1
2025-05-27 11:51:05,105 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-27 11:51:05,105 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-27 11:51:05,105 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', '<unk>', '@', 'sce', 'il', 'seri@@', '<unk>', '@', 'età', 'di', 'questo', 'problema', 'perché', 'non', 'mostra', 'la', 'poll@@', '<unk>', '@', 'o', 'del', 'ghiaccio', '.', '</s>']
2025-05-27 11:51:05,105 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-27 11:51:05,105 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-27 11:51:05,105 - INFO - joeynmt.training - 	Hypothesis: Ma questo capi<unk> @ sce il seri<unk> @ età di questo problema perché non mostra la poll<unk> @ o del ghiaccio .
2025-05-27 11:51:05,105 - INFO - joeynmt.training - Example #2
2025-05-27 11:51:05,105 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-27 11:51:05,105 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-27 11:51:05,105 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'climatico', '.', '</s>']
2025-05-27 11:51:05,106 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-27 11:51:05,106 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-27 11:51:05,106 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio ar<unk> @ c<unk> @ ico è , in un certo senso , il cuore del sistema climatico .
2025-05-27 11:51:05,106 - INFO - joeynmt.training - Example #3
2025-05-27 11:51:05,106 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-27 11:51:05,106 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-27 11:51:05,106 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', '<unk>', '@', 'ag@@', '<unk>', '@', 'ge', 'in', 'inver@@', '<unk>', '@', 'no', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-27 11:51:05,106 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-27 11:51:05,106 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-27 11:51:05,106 - INFO - joeynmt.training - 	Hypothesis: Si es<unk> @ ag<unk> @ ge in inver<unk> @ no e contr<unk> @ atti in estate .
2025-05-27 11:51:05,106 - INFO - joeynmt.training - Example #4
2025-05-27 11:51:05,106 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-27 11:51:05,106 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-27 11:51:05,106 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'slide', 'vi', 'mostro', 'che', 'vi', 'mostrerò', 'una', 'rapi@@', '<unk>', '@', 'da', 'da', 'più', 'veloce', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-27 11:51:05,106 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-27 11:51:05,106 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-27 11:51:05,106 - INFO - joeynmt.training - 	Hypothesis: La prossima slide vi mostro che vi mostrerò una rapi<unk> @ da da più veloce di quello che è successo negli ultimi 25 anni .
2025-05-27 11:51:34,631 - INFO - joeynmt.training - Epoch  10, Step:    46100, Batch Loss:     1.399133, Batch Acc: 0.640301, Tokens per Sec:     2489, Lr: 0.000300
2025-05-27 11:52:04,443 - INFO - joeynmt.training - Epoch  10, Step:    46200, Batch Loss:     1.327308, Batch Acc: 0.643611, Tokens per Sec:     2454, Lr: 0.000300
2025-05-27 11:52:32,887 - INFO - joeynmt.training - Epoch  10, Step:    46300, Batch Loss:     1.469215, Batch Acc: 0.640302, Tokens per Sec:     2580, Lr: 0.000300
2025-05-27 11:53:01,368 - INFO - joeynmt.training - Epoch  10, Step:    46400, Batch Loss:     1.443876, Batch Acc: 0.639867, Tokens per Sec:     2569, Lr: 0.000300
2025-05-27 11:53:30,116 - INFO - joeynmt.training - Epoch  10, Step:    46500, Batch Loss:     1.318550, Batch Acc: 0.639935, Tokens per Sec:     2508, Lr: 0.000300
2025-05-27 11:53:30,116 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 11:56:14,376 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.31, ppl:   3.72, acc:   0.67, generation: 164.2516[sec], evaluation: 0.0000[sec]
2025-05-27 11:56:14,377 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 11:56:14,492 - INFO - joeynmt.helpers - delete models/bpe_8k/45000.ckpt
2025-05-27 11:56:14,506 - INFO - joeynmt.training - Example #0
2025-05-27 11:56:14,506 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-27 11:56:14,506 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-27 11:56:14,506 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'due', 'slide', 'così', 'che', 'dimostr@@', '<unk>', '@', 'ano', 'che', 'il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', 'di', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'molti', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'del', '40', '%', '.', '</s>']
2025-05-27 11:56:14,506 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-27 11:56:14,506 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-27 11:56:14,506 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso ho mostrato queste due slide così che dimostr<unk> @ ano che il ghiaccio ar<unk> @ c<unk> @ ico di ghiaccio ar<unk> @ c<unk> @ ico , che per molti degli ultimi tre milioni di anni è stata la dimensione del 40 % .
2025-05-27 11:56:14,506 - INFO - joeynmt.training - Example #1
2025-05-27 11:56:14,506 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-27 11:56:14,506 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-27 11:56:14,506 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', '<unk>', '@', 'sce', 'il', 'seri@@', '<unk>', '@', 'età', 'di', 'questo', 'problema', 'perché', 'non', 'mostra', 'la', 'poll@@', '<unk>', '@', 'o', 'del', 'ghiaccio', '.', '</s>']
2025-05-27 11:56:14,507 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-27 11:56:14,507 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-27 11:56:14,507 - INFO - joeynmt.training - 	Hypothesis: Ma questo capi<unk> @ sce il seri<unk> @ età di questo problema perché non mostra la poll<unk> @ o del ghiaccio .
2025-05-27 11:56:14,507 - INFO - joeynmt.training - Example #2
2025-05-27 11:56:14,507 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-27 11:56:14,507 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-27 11:56:14,507 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'climatico', '.', '</s>']
2025-05-27 11:56:14,507 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-27 11:56:14,507 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-27 11:56:14,507 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio ar<unk> @ c<unk> @ ico è , in un senso , il cuore del sistema climatico .
2025-05-27 11:56:14,507 - INFO - joeynmt.training - Example #3
2025-05-27 11:56:14,507 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-27 11:56:14,507 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-27 11:56:14,507 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', '<unk>', '@', 'ag@@', '<unk>', '@', 'ge', 'in', 'inver@@', '<unk>', '@', 'no', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-27 11:56:14,507 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-27 11:56:14,507 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-27 11:56:14,507 - INFO - joeynmt.training - 	Hypothesis: Si es<unk> @ ag<unk> @ ge in inver<unk> @ no e contr<unk> @ atti in estate .
2025-05-27 11:56:14,507 - INFO - joeynmt.training - Example #4
2025-05-27 11:56:14,507 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-27 11:56:14,507 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-27 11:56:14,507 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'slide', 'vi', 'mostrerò', 'sarà', 'un', 'passo', 'veloce', 'di', 'ciò', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-27 11:56:14,508 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-27 11:56:14,508 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-27 11:56:14,508 - INFO - joeynmt.training - 	Hypothesis: La prossima slide vi mostrerò sarà un passo veloce di ciò che è successo negli ultimi 25 anni .
2025-05-27 11:56:43,006 - INFO - joeynmt.training - Epoch  10, Step:    46600, Batch Loss:     1.396934, Batch Acc: 0.642920, Tokens per Sec:     2615, Lr: 0.000300
2025-05-27 11:57:10,159 - INFO - joeynmt.training - Epoch  10, Step:    46700, Batch Loss:     1.315364, Batch Acc: 0.641257, Tokens per Sec:     2693, Lr: 0.000300
2025-05-27 11:57:37,957 - INFO - joeynmt.training - Epoch  10, Step:    46800, Batch Loss:     1.415496, Batch Acc: 0.641930, Tokens per Sec:     2678, Lr: 0.000300
2025-05-27 11:58:06,430 - INFO - joeynmt.training - Epoch  10, Step:    46900, Batch Loss:     1.221147, Batch Acc: 0.646914, Tokens per Sec:     2649, Lr: 0.000300
2025-05-27 11:58:34,233 - INFO - joeynmt.training - Epoch  10, Step:    47000, Batch Loss:     1.387334, Batch Acc: 0.639326, Tokens per Sec:     2582, Lr: 0.000300
2025-05-27 11:58:34,233 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 12:00:57,005 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.32, ppl:   3.73, acc:   0.67, generation: 142.7645[sec], evaluation: 0.0000[sec]
2025-05-27 12:00:57,148 - INFO - joeynmt.helpers - delete models/bpe_8k/42500.ckpt
2025-05-27 12:00:57,156 - INFO - joeynmt.training - Example #0
2025-05-27 12:00:57,156 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-27 12:00:57,156 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-27 12:00:57,156 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'due', 'di@@', '<unk>', '@', 'ap@@', '<unk>', '@', 'os@@', '<unk>', '@', 'sa', 'così', 'che', 'il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ant@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'molti', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'del', '40', '%', '.', '</s>']
2025-05-27 12:00:57,156 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-27 12:00:57,156 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-27 12:00:57,156 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso ho mostrato queste due di<unk> @ ap<unk> @ os<unk> @ sa così che il ghiaccio ar<unk> @ c<unk> @ ant<unk> @ ico , che per molti degli ultimi tre milioni di anni è stata la dimensione del 40 % .
2025-05-27 12:00:57,157 - INFO - joeynmt.training - Example #1
2025-05-27 12:00:57,157 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-27 12:00:57,157 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-27 12:00:57,157 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', '<unk>', '@', 'sce', 'la', 'seri@@', '<unk>', '@', 'età', 'di', 'questo', 'problema', 'perché', 'non', 'mostra', 'la', 'th@@', '<unk>', '@', 'ick@@', '<unk>', '@', 'ità', 'del', 'ghiaccio', '.', '</s>']
2025-05-27 12:00:57,157 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-27 12:00:57,157 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-27 12:00:57,157 - INFO - joeynmt.training - 	Hypothesis: Ma questo capi<unk> @ sce la seri<unk> @ età di questo problema perché non mostra la th<unk> @ ick<unk> @ ità del ghiaccio .
2025-05-27 12:00:57,157 - INFO - joeynmt.training - Example #2
2025-05-27 12:00:57,157 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-27 12:00:57,157 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-27 12:00:57,157 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ant@@', '<unk>', '@', 'ico', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'del', 'sistema', 'climatico', '.', '</s>']
2025-05-27 12:00:57,157 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-27 12:00:57,157 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-27 12:00:57,157 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio ar<unk> @ c<unk> @ ant<unk> @ ico è , in un senso , il cuore del sistema del sistema climatico .
2025-05-27 12:00:57,157 - INFO - joeynmt.training - Example #3
2025-05-27 12:00:57,157 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-27 12:00:57,157 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-27 12:00:57,157 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', '<unk>', '@', 'pan@@', '<unk>', '@', 'di', 'in', 'inver@@', '<unk>', '@', 'no', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-27 12:00:57,157 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-27 12:00:57,157 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-27 12:00:57,157 - INFO - joeynmt.training - 	Hypothesis: Si es<unk> @ pan<unk> @ di in inver<unk> @ no e contr<unk> @ atti in estate .
2025-05-27 12:00:57,157 - INFO - joeynmt.training - Example #4
2025-05-27 12:00:57,158 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-27 12:00:57,158 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-27 12:00:57,158 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'di@@', '<unk>', '@', 'ap@@', '<unk>', '@', 'os@@', '<unk>', '@', 'itiva', 'vi', 'vi', 'mostrerò', 'un', 'rapi@@', '<unk>', '@', 'do', 'di', 'cosa', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-27 12:00:57,158 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-27 12:00:57,158 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-27 12:00:57,158 - INFO - joeynmt.training - 	Hypothesis: La prossima di<unk> @ ap<unk> @ os<unk> @ itiva vi vi mostrerò un rapi<unk> @ do di cosa è successo negli ultimi 25 anni .
2025-05-27 12:01:26,138 - INFO - joeynmt.training - Epoch  10, Step:    47100, Batch Loss:     1.441373, Batch Acc: 0.642195, Tokens per Sec:     2503, Lr: 0.000300
2025-05-27 12:01:53,776 - INFO - joeynmt.training - Epoch  10, Step:    47200, Batch Loss:     1.295571, Batch Acc: 0.642741, Tokens per Sec:     2727, Lr: 0.000300
2025-05-27 12:02:21,034 - INFO - joeynmt.training - Epoch  10, Step:    47300, Batch Loss:     1.356967, Batch Acc: 0.635466, Tokens per Sec:     2668, Lr: 0.000300
2025-05-27 12:02:48,208 - INFO - joeynmt.training - Epoch  10, Step:    47400, Batch Loss:     1.396436, Batch Acc: 0.640712, Tokens per Sec:     2703, Lr: 0.000300
2025-05-27 12:03:15,475 - INFO - joeynmt.training - Epoch  10, Step:    47500, Batch Loss:     1.344071, Batch Acc: 0.641597, Tokens per Sec:     2687, Lr: 0.000300
2025-05-27 12:03:15,475 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 12:05:27,050 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.31, ppl:   3.70, acc:   0.67, generation: 131.5677[sec], evaluation: 0.0000[sec]
2025-05-27 12:05:27,051 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 12:05:27,165 - INFO - joeynmt.helpers - delete models/bpe_8k/47000.ckpt
2025-05-27 12:05:27,168 - INFO - joeynmt.training - Example #0
2025-05-27 12:05:27,168 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-27 12:05:27,168 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-27 12:05:27,168 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'due', 'slide', 'così', 'che', 'dimostr@@', '<unk>', '@', 'ano', 'che', 'il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 't@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'molti', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'dei', 'più', 'basso', '4@@', '<unk>', '@', '8', 'stati', ',', 'ha', 'ri@@', '<unk>', '@', 'dotto', 'la', 'dimensione', 'dei', '4@@', '<unk>', '@', '8', 'stati', ',', 'ha', 'ri@@', '<unk>', '@', 'mos@@', '<unk>', '@', 'so', 'dal', '40', '%', '.', '</s>']
2025-05-27 12:05:27,168 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-27 12:05:27,168 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-27 12:05:27,168 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso ho mostrato queste due slide così che dimostr<unk> @ ano che il ghiaccio ar<unk> @ c<unk> @ t<unk> @ ico , che per molti degli ultimi tre milioni di anni è stata la dimensione dei più basso 4<unk> @ 8 stati , ha ri<unk> @ dotto la dimensione dei 4<unk> @ 8 stati , ha ri<unk> @ mos<unk> @ so dal 40 % .
2025-05-27 12:05:27,168 - INFO - joeynmt.training - Example #1
2025-05-27 12:05:27,168 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-27 12:05:27,168 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-27 12:05:27,168 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questa', 'capi@@', '<unk>', '@', 'sce', 'la', 'seri@@', '<unk>', '@', 'età', 'di', 'questo', 'problema', 'perché', 'non', 'mostra', 'la', 'poll@@', '<unk>', '@', 'ità', 'del', 'ghiaccio', '.', '</s>']
2025-05-27 12:05:27,169 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-27 12:05:27,169 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-27 12:05:27,169 - INFO - joeynmt.training - 	Hypothesis: Ma questa capi<unk> @ sce la seri<unk> @ età di questo problema perché non mostra la poll<unk> @ ità del ghiaccio .
2025-05-27 12:05:27,169 - INFO - joeynmt.training - Example #2
2025-05-27 12:05:27,169 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-27 12:05:27,169 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-27 12:05:27,169 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 't@@', '<unk>', '@', 'ico', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'climatico', '.', '</s>']
2025-05-27 12:05:27,169 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-27 12:05:27,169 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-27 12:05:27,169 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio ar<unk> @ c<unk> @ t<unk> @ ico è , in un senso , il cuore del sistema climatico .
2025-05-27 12:05:27,169 - INFO - joeynmt.training - Example #3
2025-05-27 12:05:27,169 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-27 12:05:27,169 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-27 12:05:27,169 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'tratta', 'di', 'v@@', '<unk>', '@', 'uo@@', '<unk>', '@', 'to', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-27 12:05:27,169 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-27 12:05:27,169 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-27 12:05:27,169 - INFO - joeynmt.training - 	Hypothesis: Si tratta di v<unk> @ uo<unk> @ to e contr<unk> @ atti in estate .
2025-05-27 12:05:27,169 - INFO - joeynmt.training - Example #4
2025-05-27 12:05:27,169 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-27 12:05:27,169 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-27 12:05:27,169 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'slide', 'vi', 'mostrerò', 'sarà', 'un', 'passo', 'avanti', 'avanti', 'avanti', 'di', 'ciò', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-27 12:05:27,170 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-27 12:05:27,170 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-27 12:05:27,170 - INFO - joeynmt.training - 	Hypothesis: La prossima slide vi mostrerò sarà un passo avanti avanti avanti di ciò che è successo negli ultimi 25 anni .
2025-05-27 12:05:55,724 - INFO - joeynmt.training - Epoch  10, Step:    47600, Batch Loss:     1.371084, Batch Acc: 0.636215, Tokens per Sec:     2541, Lr: 0.000300
2025-05-27 12:06:23,597 - INFO - joeynmt.training - Epoch  10, Step:    47700, Batch Loss:     1.340667, Batch Acc: 0.636036, Tokens per Sec:     2658, Lr: 0.000300
2025-05-27 12:06:51,699 - INFO - joeynmt.training - Epoch  10, Step:    47800, Batch Loss:     1.358614, Batch Acc: 0.640901, Tokens per Sec:     2654, Lr: 0.000300
2025-05-27 12:07:20,037 - INFO - joeynmt.training - Epoch  10, Step:    47900, Batch Loss:     1.337845, Batch Acc: 0.640286, Tokens per Sec:     2587, Lr: 0.000300
2025-05-27 12:07:49,839 - INFO - joeynmt.training - Epoch  10, Step:    48000, Batch Loss:     1.303957, Batch Acc: 0.637505, Tokens per Sec:     2438, Lr: 0.000300
2025-05-27 12:07:49,840 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 12:10:17,601 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.31, ppl:   3.69, acc:   0.67, generation: 147.7536[sec], evaluation: 0.0000[sec]
2025-05-27 12:10:17,603 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 12:10:17,739 - INFO - joeynmt.helpers - delete models/bpe_8k/44000.ckpt
2025-05-27 12:10:17,751 - INFO - joeynmt.training - Example #0
2025-05-27 12:10:17,752 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-27 12:10:17,752 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-27 12:10:17,752 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'due', 'di@@', '<unk>', '@', 'ap@@', '<unk>', '@', 'os@@', '<unk>', '@', 'sa', 'così', 'che', 'il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'dei', '4@@', '<unk>', '@', '8', 'stati', ',', 'ha', 'ri@@', '<unk>', '@', 'dotto', 'il', '40', '%', '.', '</s>']
2025-05-27 12:10:17,752 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-27 12:10:17,752 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-27 12:10:17,752 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso ho mostrato queste due di<unk> @ ap<unk> @ os<unk> @ sa così che il ghiaccio ar<unk> @ c<unk> @ ico ar<unk> @ c<unk> @ ico , che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione dei 4<unk> @ 8 stati , ha ri<unk> @ dotto il 40 % .
2025-05-27 12:10:17,752 - INFO - joeynmt.training - Example #1
2025-05-27 12:10:17,752 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-27 12:10:17,752 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-27 12:10:17,752 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', '<unk>', '@', 'sce', 'la', 'seri@@', '<unk>', '@', 'età', 'di', 'questo', 'problema', 'perché', 'non', 'mostra', 'la', 'poll@@', '<unk>', '@', 'ità', 'del', 'ghiaccio', '.', '</s>']
2025-05-27 12:10:17,752 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-27 12:10:17,752 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-27 12:10:17,752 - INFO - joeynmt.training - 	Hypothesis: Ma questo capi<unk> @ sce la seri<unk> @ età di questo problema perché non mostra la poll<unk> @ ità del ghiaccio .
2025-05-27 12:10:17,752 - INFO - joeynmt.training - Example #2
2025-05-27 12:10:17,752 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-27 12:10:17,752 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-27 12:10:17,752 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'climatico', '.', '</s>']
2025-05-27 12:10:17,753 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-27 12:10:17,753 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-27 12:10:17,753 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio ar<unk> @ c<unk> @ ico è , in un senso , il cuore del sistema climatico .
2025-05-27 12:10:17,753 - INFO - joeynmt.training - Example #3
2025-05-27 12:10:17,753 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-27 12:10:17,753 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-27 12:10:17,753 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', '<unk>', '@', 'pan@@', '<unk>', '@', 'di', 'in', 'inver@@', '<unk>', '@', 'no', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-27 12:10:17,753 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-27 12:10:17,753 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-27 12:10:17,753 - INFO - joeynmt.training - 	Hypothesis: Si es<unk> @ pan<unk> @ di in inver<unk> @ no e contr<unk> @ atti in estate .
2025-05-27 12:10:17,753 - INFO - joeynmt.training - Example #4
2025-05-27 12:10:17,753 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-27 12:10:17,753 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-27 12:10:17,753 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'di@@', '<unk>', '@', 'ap@@', '<unk>', '@', 'os@@', '<unk>', '@', 'itiva', 'che', 'vi', 'mostrerò', 'essere', 'un', 'passo', 'avanti', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-27 12:10:17,753 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-27 12:10:17,753 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-27 12:10:17,753 - INFO - joeynmt.training - 	Hypothesis: La prossima di<unk> @ ap<unk> @ os<unk> @ itiva che vi mostrerò essere un passo avanti di quello che è successo negli ultimi 25 anni .
2025-05-27 12:25:49,186 - INFO - joeynmt.training - Epoch  10, Step:    48100, Batch Loss:     1.355697, Batch Acc: 0.639725, Tokens per Sec:       80, Lr: 0.000300
2025-05-27 12:26:17,022 - INFO - joeynmt.training - Epoch  10, Step:    48200, Batch Loss:     1.389879, Batch Acc: 0.641876, Tokens per Sec:     2680, Lr: 0.000300
2025-05-27 12:44:09,162 - INFO - joeynmt.training - Epoch  10, Step:    48300, Batch Loss:     1.377461, Batch Acc: 0.635861, Tokens per Sec:       69, Lr: 0.000300
2025-05-27 12:44:37,589 - INFO - joeynmt.training - Epoch  10, Step:    48400, Batch Loss:     1.257411, Batch Acc: 0.636770, Tokens per Sec:     2575, Lr: 0.000300
2025-05-27 12:51:43,335 - INFO - joeynmt.training - Epoch  10, Step:    48500, Batch Loss:     1.330502, Batch Acc: 0.638542, Tokens per Sec:      180, Lr: 0.000300
2025-05-27 12:51:43,335 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 13:24:21,927 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.30, ppl:   3.67, acc:   0.67, generation: 1958.5844[sec], evaluation: 0.0000[sec]
2025-05-27 13:24:21,929 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-27 13:24:22,040 - INFO - joeynmt.helpers - delete models/bpe_8k/46000.ckpt
2025-05-27 13:24:22,046 - INFO - joeynmt.training - Example #0
2025-05-27 13:24:22,046 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-27 13:24:22,046 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-27 13:24:22,046 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'due', 'di@@', '<unk>', '@', 'ap@@', '<unk>', '@', 'os@@', '<unk>', '@', 'sa', 'così', 'che', 'il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'molti', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'del', '4@@', '<unk>', '@', '8', 'stati', 'le', 'dimensioni', 'del', '4@@', '<unk>', '@', '8', 'stati', ',', 'ha', 'ri@@', '<unk>', '@', 'mp@@', '<unk>', '@', 'i@@', '<unk>', '@', 'l@@', '<unk>', '@', 'ica', 'del', '40', '%', '.', '</s>']
2025-05-27 13:24:22,047 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-27 13:24:22,047 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-27 13:24:22,047 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso ho mostrato queste due di<unk> @ ap<unk> @ os<unk> @ sa così che il ghiaccio ar<unk> @ c<unk> @ ico , che per molti degli ultimi tre milioni di anni è stata la dimensione del 4<unk> @ 8 stati le dimensioni del 4<unk> @ 8 stati , ha ri<unk> @ mp<unk> @ i<unk> @ l<unk> @ ica del 40 % .
2025-05-27 13:24:22,047 - INFO - joeynmt.training - Example #1
2025-05-27 13:24:22,047 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-27 13:24:22,047 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-27 13:24:22,047 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', '<unk>', '@', 'sce', 'la', 'seri@@', '<unk>', '@', 'età', 'di', 'questo', 'problema', 'perché', 'non', 'mostra', 'la', 'poll@@', '<unk>', '@', 'o', 'del', 'ghiaccio', '.', '</s>']
2025-05-27 13:24:22,047 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-27 13:24:22,047 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-27 13:24:22,047 - INFO - joeynmt.training - 	Hypothesis: Ma questo capi<unk> @ sce la seri<unk> @ età di questo problema perché non mostra la poll<unk> @ o del ghiaccio .
2025-05-27 13:24:22,047 - INFO - joeynmt.training - Example #2
2025-05-27 13:24:22,047 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-27 13:24:22,047 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-27 13:24:22,047 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'climatico', '.', '</s>']
2025-05-27 13:24:22,047 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-27 13:24:22,047 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-27 13:24:22,047 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio ar<unk> @ c<unk> @ ico è , in un senso , il cuore del sistema climatico .
2025-05-27 13:24:22,047 - INFO - joeynmt.training - Example #3
2025-05-27 13:24:22,047 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-27 13:24:22,047 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-27 13:24:22,048 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', '<unk>', '@', 'pan@@', '<unk>', '@', 'di', 'in', 'inver@@', '<unk>', '@', 'no', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-27 13:24:22,048 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-27 13:24:22,048 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-27 13:24:22,048 - INFO - joeynmt.training - 	Hypothesis: Si es<unk> @ pan<unk> @ di in inver<unk> @ no e contr<unk> @ atti in estate .
2025-05-27 13:24:22,048 - INFO - joeynmt.training - Example #4
2025-05-27 13:24:22,048 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-27 13:24:22,048 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-27 13:24:22,048 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'di@@', '<unk>', '@', 'ap@@', '<unk>', '@', 'os@@', '<unk>', '@', 'itiva', 'che', 'vi', 'mostrerò', 'un', 'rapi@@', '<unk>', '@', 'do', 'di', 'ciò', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-27 13:24:22,048 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-27 13:24:22,048 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-27 13:24:22,048 - INFO - joeynmt.training - 	Hypothesis: La prossima di<unk> @ ap<unk> @ os<unk> @ itiva che vi mostrerò un rapi<unk> @ do di ciò che è successo negli ultimi 25 anni .
2025-05-27 13:26:11,380 - INFO - joeynmt.training - Epoch  10, Step:    48600, Batch Loss:     1.278946, Batch Acc: 0.641295, Tokens per Sec:      646, Lr: 0.000300
2025-05-27 13:26:38,393 - INFO - joeynmt.training - Epoch  10, Step:    48700, Batch Loss:     1.352636, Batch Acc: 0.645087, Tokens per Sec:     2753, Lr: 0.000300
2025-05-27 13:29:54,464 - INFO - joeynmt.training - Epoch  10, Step:    48800, Batch Loss:     1.423334, Batch Acc: 0.639365, Tokens per Sec:      372, Lr: 0.000300
2025-05-27 13:30:22,652 - INFO - joeynmt.training - Epoch  10, Step:    48900, Batch Loss:     1.400417, Batch Acc: 0.642224, Tokens per Sec:     2614, Lr: 0.000300
2025-05-27 13:44:21,543 - INFO - joeynmt.training - Epoch  10, Step:    49000, Batch Loss:     1.594851, Batch Acc: 0.643109, Tokens per Sec:       89, Lr: 0.000300
2025-05-27 13:44:21,543 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 14:19:40,975 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.31, ppl:   3.70, acc:   0.67, generation: 2119.4248[sec], evaluation: 0.0000[sec]
2025-05-27 14:19:41,097 - INFO - joeynmt.helpers - delete models/bpe_8k/44500.ckpt
2025-05-27 14:19:41,110 - INFO - joeynmt.training - Example #0
2025-05-27 14:19:41,110 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-27 14:19:41,110 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-27 14:19:41,110 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'due', 'di@@', '<unk>', '@', 'ap@@', '<unk>', '@', 'os@@', '<unk>', '@', 'sa', 'così', 'che', 'il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'la', 'maggior', 'parte', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'del', '4@@', '<unk>', '@', '8', 'stati', 'la', 'dimensione', 'del', '4@@', '<unk>', '@', '8', 'stati', 'la', 'dimensione', 'del', '40', '%', '.', '</s>']
2025-05-27 14:19:41,110 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-27 14:19:41,110 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-27 14:19:41,110 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso ho mostrato queste due di<unk> @ ap<unk> @ os<unk> @ sa così che il ghiaccio ar<unk> @ c<unk> @ ico , che per la maggior parte degli ultimi tre milioni di anni è stata la dimensione del 4<unk> @ 8 stati la dimensione del 4<unk> @ 8 stati la dimensione del 40 % .
2025-05-27 14:19:41,110 - INFO - joeynmt.training - Example #1
2025-05-27 14:19:41,110 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-27 14:19:41,110 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-27 14:19:41,110 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', '<unk>', '@', 'sce', 'la', 'seri@@', '<unk>', '@', 'età', 'di', 'questo', 'problema', 'perché', 'non', 'mostra', 'il', 'poll@@', '<unk>', '@', 'o', 'del', 'ghiaccio', '.', '</s>']
2025-05-27 14:19:41,110 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-27 14:19:41,110 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-27 14:19:41,110 - INFO - joeynmt.training - 	Hypothesis: Ma questo capi<unk> @ sce la seri<unk> @ età di questo problema perché non mostra il poll<unk> @ o del ghiaccio .
2025-05-27 14:19:41,110 - INFO - joeynmt.training - Example #2
2025-05-27 14:19:41,111 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-27 14:19:41,111 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-27 14:19:41,111 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'climatico', '.', '</s>']
2025-05-27 14:19:41,111 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-27 14:19:41,111 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-27 14:19:41,111 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio ar<unk> @ c<unk> @ ico è , in un senso , il cuore del sistema climatico .
2025-05-27 14:19:41,111 - INFO - joeynmt.training - Example #3
2025-05-27 14:19:41,111 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-27 14:19:41,111 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-27 14:19:41,111 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'espan@@', '<unk>', '@', 'de', 'in', 'inver@@', '<unk>', '@', 'no', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-27 14:19:41,111 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-27 14:19:41,111 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-27 14:19:41,111 - INFO - joeynmt.training - 	Hypothesis: Si espan<unk> @ de in inver<unk> @ no e contr<unk> @ atti in estate .
2025-05-27 14:19:41,111 - INFO - joeynmt.training - Example #4
2025-05-27 14:19:41,111 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-27 14:19:41,111 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-27 14:19:41,111 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'di@@', '<unk>', '@', 'ap@@', '<unk>', '@', 'os@@', '<unk>', '@', 'itiva', 'vi', 'vi', 'mostrerò', 'un', 'rapi@@', '<unk>', '@', 'do', 'di', 'cosa', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-27 14:19:41,111 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-27 14:19:41,111 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-27 14:19:41,111 - INFO - joeynmt.training - 	Hypothesis: La prossima di<unk> @ ap<unk> @ os<unk> @ itiva vi vi mostrerò un rapi<unk> @ do di cosa è successo negli ultimi 25 anni .
2025-05-27 14:36:50,193 - INFO - joeynmt.training - Epoch  10, Step:    49100, Batch Loss:     1.303121, Batch Acc: 0.638608, Tokens per Sec:       72, Lr: 0.000300
2025-05-27 14:54:23,217 - INFO - joeynmt.training - Epoch  10, Step:    49200, Batch Loss:     1.273450, Batch Acc: 0.639869, Tokens per Sec:       70, Lr: 0.000300
2025-05-27 14:54:50,478 - INFO - joeynmt.training - Epoch  10, Step:    49300, Batch Loss:     1.548328, Batch Acc: 0.644262, Tokens per Sec:     2707, Lr: 0.000300
2025-05-27 15:11:53,911 - INFO - joeynmt.training - Epoch  10, Step:    49400, Batch Loss:     1.298336, Batch Acc: 0.639844, Tokens per Sec:       74, Lr: 0.000300
2025-05-27 15:12:22,227 - INFO - joeynmt.training - Epoch  10, Step:    49500, Batch Loss:     1.429743, Batch Acc: 0.642786, Tokens per Sec:     2643, Lr: 0.000300
2025-05-27 15:12:22,227 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 15:36:08,152 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.30, ppl:   3.69, acc:   0.67, generation: 1425.9179[sec], evaluation: 0.0000[sec]
2025-05-27 15:36:08,291 - INFO - joeynmt.helpers - delete models/bpe_8k/46500.ckpt
2025-05-27 15:36:08,310 - INFO - joeynmt.training - Example #0
2025-05-27 15:36:08,311 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'ast', 'year', 'I', 'showed', 'these', 'two', 'sli@@', '@@@', '@', 'des', 'so', 'that', 'demonstr@@', '@@@', '@', 'ate', 'that', 'the', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'size', 'of', 'the', 'lower', '4@@', '@@@', '@', '8', 'states', ',', 'has', 'sh@@', '@@@', '@', 'run@@', '@@@', '@', 'k', 'by', '40', 'percent', '.']
2025-05-27 15:36:08,311 - DEBUG - joeynmt.training - 	Tokenized reference:  ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itive', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimensioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'contin@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'stret@@', '@@@', '@', 'ta', 'del', '40', '%', '.']
2025-05-27 15:36:08,311 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["L'", 'anno', 'scorso', 'ho', 'mostrato', 'queste', 'due', 'slide', ',', 'così', 'che', 'dimostr@@', '<unk>', '@', 'ano', 'che', 'il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'molti', 'degli', 'ultimi', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimensione', 'dei', '4@@', '<unk>', '@', '8', 'stati', ',', 'ha', 'ri@@', '<unk>', '@', 'dotto', 'la', 'dimensione', 'dei', '4@@', '<unk>', '@', '8', 'stati', ',', 'ha', 'ri@@', '<unk>', '@', 'dotto', 'il', '40', '%', '.', '</s>']
2025-05-27 15:36:08,311 - INFO - joeynmt.training - 	Source:     L@@ ast year I showed these two sli@@ des so that demonstr@@ ate that the ar@@ c@@ tic ice ca@@ p , which for most of the last three million years has been the size of the lower 4@@ 8 states , has sh@@ run@@ k by 40 percent .
2025-05-27 15:36:08,311 - INFO - joeynmt.training - 	Reference:  L' anno scorso ho mostrato queste di@@ ap@@ os@@ itive per dimostr@@ are che la cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimensioni dei 4@@ 8 Stati Uniti contin@@ ent@@ ali , si è ri@@ stret@@ ta del 40 % .
2025-05-27 15:36:08,311 - INFO - joeynmt.training - 	Hypothesis: L' anno scorso ho mostrato queste due slide , così che dimostr<unk> @ ano che il ghiaccio ar<unk> @ c<unk> @ ico , che per molti degli ultimi tre milioni di anni è stata la dimensione dei 4<unk> @ 8 stati , ha ri<unk> @ dotto la dimensione dei 4<unk> @ 8 stati , ha ri<unk> @ dotto il 40 % .
2025-05-27 15:36:08,311 - INFO - joeynmt.training - Example #1
2025-05-27 15:36:08,311 - DEBUG - joeynmt.training - 	Tokenized source:     ['But', 'this', 'underst@@', '@@@', '@', 'ates', 'the', 'seri@@', '@@@', '@', 'ousness', 'of', 'this', 'particular', 'problem', 'because', 'it', 'doesn', "'t", 'show', 'the', 'th@@', '@@@', '@', 'ick@@', '@@@', '@', 'ness', 'of', 'the', 'ice', '.']
2025-05-27 15:36:08,311 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Tuttavia', 'questo', 'sotto@@', '@@@', '@', 'val@@', '@@@', '@', 'uta', 'la', 'grav@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghiaccio', '.']
2025-05-27 15:36:08,311 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'capi@@', '<unk>', '@', 'sce', 'la', 'seri@@', '<unk>', '@', 'età', 'di', 'questo', 'problema', 'perché', 'non', 'mostra', 'la', 'poll@@', '<unk>', '@', 'o', 'del', 'ghiaccio', '.', '</s>']
2025-05-27 15:36:08,311 - INFO - joeynmt.training - 	Source:     But this underst@@ ates the seri@@ ousness of this particular problem because it doesn 't show the th@@ ick@@ ness of the ice .
2025-05-27 15:36:08,311 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sotto@@ val@@ uta la grav@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghiaccio .
2025-05-27 15:36:08,311 - INFO - joeynmt.training - 	Hypothesis: Ma questo capi<unk> @ sce la seri<unk> @ età di questo problema perché non mostra la poll<unk> @ o del ghiaccio .
2025-05-27 15:36:08,311 - INFO - joeynmt.training - Example #2
2025-05-27 15:36:08,311 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'ar@@', '@@@', '@', 'c@@', '@@@', '@', 'tic', 'ice', 'ca@@', '@@@', '@', 'p', 'is', ',', 'in', 'a', 'sense', ',', 'the', 'be@@', '@@@', '@', 'ating', 'heart', 'of', 'the', 'global', 'climate', 'system', '.']
2025-05-27 15:36:08,311 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'ot@@', '@@@', '@', 'ta', 'gl@@', '@@@', '@', 'aci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'climatico', 'globale', '.']
2025-05-27 15:36:08,311 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'ghiaccio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ico', 'è', ',', 'in', 'un', 'senso', ',', 'il', 'cuore', 'del', 'sistema', 'climatico', '.', '</s>']
2025-05-27 15:36:08,311 - INFO - joeynmt.training - 	Source:     The ar@@ c@@ tic ice ca@@ p is , in a sense , the be@@ ating heart of the global climate system .
2025-05-27 15:36:08,311 - INFO - joeynmt.training - 	Reference:  La cal@@ ot@@ ta gl@@ aci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema climatico globale .
2025-05-27 15:36:08,311 - INFO - joeynmt.training - 	Hypothesis: Il ghiaccio ar<unk> @ c<unk> @ ico è , in un senso , il cuore del sistema climatico .
2025-05-27 15:36:08,311 - INFO - joeynmt.training - Example #3
2025-05-27 15:36:08,312 - DEBUG - joeynmt.training - 	Tokenized source:     ['It', 'ex@@', '@@@', '@', 'pan@@', '@@@', '@', 'ds', 'in', 'win@@', '@@@', '@', 'ter', 'and', 'contr@@', '@@@', '@', 'acts', 'in', 'summer', '.']
2025-05-27 15:36:08,312 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'espan@@', '@@@', '@', 'de', "d'", 'inver@@', '@@@', '@', 'no', 'e', 'si', 'r@@', '@@@', '@', 'iti@@', '@@@', '@', 'ra', "d'", 'estate', '.']
2025-05-27 15:36:08,312 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'es@@', '<unk>', '@', 'ag@@', '<unk>', '@', 'ge', 'in', 'inver@@', '<unk>', '@', 'no', 'e', 'contr@@', '<unk>', '@', 'atti', 'in', 'estate', '.', '</s>']
2025-05-27 15:36:08,312 - INFO - joeynmt.training - 	Source:     It ex@@ pan@@ ds in win@@ ter and contr@@ acts in summer .
2025-05-27 15:36:08,312 - INFO - joeynmt.training - 	Reference:  Si espan@@ de d' inver@@ no e si r@@ iti@@ ra d' estate .
2025-05-27 15:36:08,312 - INFO - joeynmt.training - 	Hypothesis: Si es<unk> @ ag<unk> @ ge in inver<unk> @ no e contr<unk> @ atti in estate .
2025-05-27 15:36:08,312 - INFO - joeynmt.training - Example #4
2025-05-27 15:36:08,312 - DEBUG - joeynmt.training - 	Tokenized source:     ['The', 'next', 'slide', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', '@@@', '@', 'd', 'f@@', '@@@', '@', 'ast@@', '@@@', '@', '-@@', '@@@', '@', 'forward', 'of', 'what', "'s", 'happened', 'over', 'the', 'last', '25', 'years', '.']
2025-05-27 15:36:08,312 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossima', 'di@@', '@@@', '@', 'ap@@', '@@@', '@', 'os@@', '@@@', '@', 'itiva', 'sarà', 'una', 'rapi@@', '@@@', '@', 'da', 'car@@', '@@@', '@', 're@@', '@@@', '@', 'll@@', '@@@', '@', 'ata', 'sugli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'imenti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-27 15:36:08,312 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossima', 'di@@', '<unk>', '@', 'ap@@', '<unk>', '@', 'os@@', '<unk>', '@', 'itiva', 'che', 'vi', 'mostro', 'un', 'rapi@@', '<unk>', '@', 'do', 'di', 'ciò', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-27 15:36:08,312 - INFO - joeynmt.training - 	Source:     The next slide I show you will be a rapi@@ d f@@ ast@@ -@@ forward of what 's happened over the last 25 years .
2025-05-27 15:36:08,312 - INFO - joeynmt.training - 	Reference:  La prossima di@@ ap@@ os@@ itiva sarà una rapi@@ da car@@ re@@ ll@@ ata sugli av@@ ven@@ imenti degli ultimi 25 anni .
2025-05-27 15:36:08,312 - INFO - joeynmt.training - 	Hypothesis: La prossima di<unk> @ ap<unk> @ os<unk> @ itiva che vi mostro un rapi<unk> @ do di ciò che è successo negli ultimi 25 anni .
2025-05-27 15:36:36,454 - INFO - joeynmt.training - Epoch  10, Step:    49600, Batch Loss:     1.322048, Batch Acc: 0.638108, Tokens per Sec:     2615, Lr: 0.000300
2025-05-27 15:52:32,112 - INFO - joeynmt.training - Epoch  10, Step:    49700, Batch Loss:     1.347021, Batch Acc: 0.645216, Tokens per Sec:       77, Lr: 0.000300
2025-05-27 16:07:31,669 - INFO - joeynmt.training - Epoch  10, Step:    49800, Batch Loss:     1.323270, Batch Acc: 0.635262, Tokens per Sec:       82, Lr: 0.000300
2025-05-27 16:08:00,576 - INFO - joeynmt.training - Epoch  10, Step:    49900, Batch Loss:     1.302035, Batch Acc: 0.638657, Tokens per Sec:     2605, Lr: 0.000300
2025-05-27 16:08:04,219 - INFO - joeynmt.training - Epoch  10: total training loss 6832.92
2025-05-27 16:08:04,219 - INFO - joeynmt.training - Training ended after  10 epochs.
2025-05-27 16:08:04,219 - INFO - joeynmt.training - Best validation result (greedy) at step    48500:   3.67 ppl.
2025-05-27 16:08:04,230 - INFO - joeynmt.model - Building an encoder-decoder model...
2025-05-27 16:08:04,284 - INFO - joeynmt.model - Enc-dec model built.
2025-05-27 16:08:04,332 - INFO - joeynmt.helpers - Load model from /Users/liuxduan/Desktop/mt-exercise-4-master/models/bpe_8k/48500.ckpt.
2025-05-27 16:08:04,335 - INFO - joeynmt.prediction - Model(
	encoder=TransformerEncoder(num_layers=4, num_heads=2, alpha=1.0, layer_norm="pre", activation=ReLU()),
	decoder=TransformerDecoder(num_layers=1, num_heads=2, alpha=1.0, layer_norm="post", activation=ReLU()),
	src_embed=Embeddings(embedding_dim=256, vocab_size=7973),
	trg_embed=Embeddings(embedding_dim=256, vocab_size=7973),
	loss_function=None)
2025-05-27 16:08:04,337 - INFO - joeynmt.prediction - Decoding on dev set...
2025-05-27 16:08:04,337 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 16:10:05,135 - INFO - joeynmt.prediction - Evaluation result (beam search) , generation: 120.7901[sec], evaluation: 0.0000[sec]
2025-05-27 16:10:05,138 - INFO - joeynmt.prediction - Translations saved to: /Users/liuxduan/Desktop/mt-exercise-4-master/models/bpe_8k/00048500.hyps.dev.
2025-05-27 16:10:05,138 - INFO - joeynmt.prediction - Decoding on test set...
2025-05-27 16:10:05,138 - INFO - joeynmt.prediction - Predicting 1566 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-27 16:12:35,375 - INFO - joeynmt.prediction - Evaluation result (beam search) , generation: 150.2265[sec], evaluation: 0.0000[sec]
2025-05-27 16:12:35,378 - INFO - joeynmt.prediction - Translations saved to: /Users/liuxduan/Desktop/mt-exercise-4-master/models/bpe_8k/00048500.hyps.test.
